<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" >
<title>bigdata&#174;</title>
<!-- $Id$ -->
</head>
<body>

  <p>

	  <em>bigdata&#174;</em> is a scale-out data and computing
	  fabric designed for commodity hardware.  The bigdata
	  architecture provides named scale-out indices that are
	  transparently and dynamically key-range partitioned and
	  distributed across a cluster or grid of commodity server
	  platforms.  The scale-out indices are B+Trees and remain
	  balanced under insert and removal operations.  Keys and
	  values for btrees are variable length byte[]s (the keys are
	  interpreted as unsigned byte[]s).

	  Atomic "row" operations are supported for very high
	  concurrency.  However, full transactions are also available
	  for applications needing less concurrency but requiring
	  atomic operations that read or write on more than one row,
	  index partition, or index.

	  Writes are absorbed on mutable btree instances in append
	  only "journals" of ~200M capacity.  On overflow, data in a
	  journal is evicted onto read-optimized, immutable "index
	  segments".  The metadata service manages the index
	  definitions, the index partitions, and the assignment of
	  index partitions to data services.  A data service
	  encapsulates a journal and zero or more index partitions
	  assigned to that data service, including the logic to handle
	  overflow. 

</p><p>

          A deployment is made up of a transaction service, a load
	  balancer service, and a metadata service (with failover
	  redundancy) and many data service instances.  bigdata can
	  provide data redundancy internally (by pipelining writes
	  bound for a partition across primary, secondary, ... data
	  service instances for that partition) or it can be deployed
	  over NAS/SAN.  bigdata itself is 100% Java and requires a
	  JDK 1.6.  There are additional dependencies for the
	  installer (un*x) and for collecting performance counters
	  from the OS (<a
	  href="http://pagesperso-orange.fr/sebastien.godard/">sysstat</a>).
	  
  </p>
  
  <h2>Architecture</h2>
  
  <p>
  
  	  The bigdata SOA defines three essential services and some
  	  additional services.  The essential services are the
  	  metadata service (provides a locator service for index
  	  partitions on data services), the data service (provides
  	  read, write, and concurrency control for index partitions),
  	  and the transaction service (provides consistent timestamps
  	  for commits, facilitates the release of data associated with
  	  older commit points, read locks, and transactions).  Full
  	  transactions are NOT required, so you can use bigdata as a
  	  scale-out row store.  The load balancer service guides the
  	  dynamic redistribution of data across a bigdata federation.
  	  There are also client services, which are containers for
  	  executing distributed jobs.
  
  </p>
  
  <p>
  
  	  While other service fabric architectures are contemplated,
  	  bigdata services today use JINI 2. to advertise themselves
  	  and do service discovery.  This means that you must be
  	  running a JINI registrar in order for services to be able to
  	  register themselves or discover other services.  The JINI
  	  integration is bundled and installed automatically by
  	  default. 
  
  </p>

  <p>

          Zookeeper handles master election, configuration management
	  and global synchronous locks.  Zookeeper was developed by
	  Yahoo! as a distributed lock and configuration management
	  service and is now an Apache subproject (part of
	  Hadoop). Among other things, it gets master election
	  protocols right.  Zookeeper is bundled and installed
	  automatically by default.

  </p>
  
  <p>
  
  	  The main building blocks for the bigdata architecture are
  	  the journal (both an append-only persistence store and a recently
  	  introduced read/write store with the ability to recycle historical
  	  commit points), the mutable
  	  B+Tree (used to absorb writes), and the read-optimized
  	  immutable B+Tree (aka the index segment).  Highly efficient
  	  bulk index builds are used to transfer data absorbed by a
  	  mutable B+Tree on a journal into index segment files.  Each
  	  for index segment contains data for a single partition of a
  	  scale-out index.  In order to read from an index partition,
  	  a consistent view is created by dynamically fusing data for
  	  that index partition, including any recent writes on the
  	  current journal, any historical writes that are in the
  	  process of being transferred onto index segments, and any
  	  historical index segments that also contain data for that
  	  view. Periodically, index segments are merged together, at
  	  which point deleted tuples are purged from the view.

</p>

<p>

	  Bigdata periodically releases data associated with older
	  commit points, freeing up disk resources.  The transaction
	  service is configured with a minimum release age in
	  milliseconds.  This can be ZERO (0L) milliseconds, in which
	  case historical views may be released if there are no read
	  locks for that commit point.  The minimum release age can
	  also be hours or days if you want to keep historical states
	  around for a while.  When a data service overflows, it will
	  consult the transaction service to determine the effective
	  release time and release any old journals or index segments
	  no longer required to maintain views GT that release time.
</p><p>	  
	  An immortal or temporal database can be realized by
	  specifying Long#MAX_VALUE for the minimum release age.  In
	  this case, the old journals and index segments will all be
	  retained and you can query any historical commit point of
	  the database at any time.
  
  </p><p>
  	  
  	  An detailed architecture whitepaper for bigdata is posted only and
  	  linked from our <a href="http://www.bigdata.com/blog">blog</a>.
  	  
  </p>
  
  <h2>Sparse row store</h2>
  
  <p>
  
 	  The <em>SparseRowStore</em> provides a column flexible row
 	  store similar to Google's bigtable or HBase, including very
 	  high read/write concurrency and ACID operations on logical
 	  "rows".  Internally, a "global row store" instance is used
 	  to maintain metadata on relations declared within a bigdata
 	  federation.  You can use this instance to store your own
 	  data, or you can create your own named row store instances.
 	  However, there is no REST api for the row store at this time
 	  (trivial to be sure, but not something that we have gotten
 	  around to yet).

</p><p>

          In fact, it is trivial to realize bigtable semantics with
 	  bigdata - all you need to do is exercise a specific protocol
 	  when forming the keys for your scale-out indices and then
 	  you simply choose to NOT use transactions.  A bigtable style
 	  key-value is formed as:

  </p>
  
  <pre>
  
      [columnFamily][primaryKey][columnName][timestamp} : [value]
  
  </pre>
  
  <p>
 	  
 	  By placing the column family identifier up front, all data
 	  in the same column family will be clustered together by the
 	  index.  The next component is the "row" identifier, what you
 	  would think of as the primary key in a relational table.
 	  The column name comes next - only column names for non-null
 	  columns are written into the index.  Finally, there is a
 	  timestamp column that is used either to record a timestamp
 	  specified by the application or a datum write time.  The
 	  value associated with the key is simply the datum for that
 	  column in that row.  The use of nul byte separators makes it
 	  possible to parse the key, which is required for various
 	  operations including index partition splits and filtering
 	  key scans based on column names or timestamps.  See the
 	  <em>KeyBuilder</em> class in
 	  <code>com.bigdata.btree.keys</code> for utilities that may
 	  be used to construct keys for variety of data types.
  
  </p>
  
  <h2>Map/reduce, Asynchronous Write API, and Query</h2>
  
  <p>
  
  	  Google's map/reduce architecture has received a lot of
  	  attention, along with its bigtable architecture.  Map/reduce
  	  provides a means to transparently decompose processing
  	  across a cluster.  The "map" process examines a series of
  	  key-value pairs, emitting a set of intermediate key-value
  	  pairs for each input.  Those intermediate key-values are
  	  then hashed (module R) onto R reduce processes.  The inputs
  	  for the reduce processes are pre-sorted.  The reduce process
  	  then runs some arbitrary operation on the sorted data, such
  	  as computing an inverted index file or loading the data into
  	  a scale-out index.
  
  </p>

  <p>

          bigdata&#174; supports an <em>asynchronous index write
          API</em>, which delivers extremely high throughput for
          scattered writes.  While map/reduce is tremendously
          effective when there is good locality in the data, it is not
          the right tool for processing ordered data.  Instead, you
          execute a master job, which spawns client(s) running in
          <em>client service</em>(s) associated with the bigdata
          federation.  Those clients process data, writing onto
          blocking buffers.  The writes are automatically split and
          buffered for each key-range shard.  This maximizes the chunk
          size for ordered writes and provides a tremendous throughput
          boost.  Bigdata can work well in combination with
          map/reduce.  The basic paradigm is you use map/reduce jobs
          to generate data, which is then bulk loaded into a bigdata
          federation using bigdata jobs and the asynchronous write
          API.

  </p><p>

          bigdata&#174; has built in support for distributed rule
          execution.  This can be used for high-level query or for
          materializing derived views, including maintaining the RDFS+
          closure of a semantic web database.  The implementation is
          highly efficient and propagates binding sets to the data
          service for each key-range shard touched by the query, so
          the JOINs happen right up against the data.  Unlike using
          map/reduce for join processing, bigdata query processing is
          very low latency.  Distributed query execution can be
          substantially faster than local query execution, even for
          low-latency queries.

  </p>

  <h2>Standalone Journals</h2>
  
  <p>
  
  	  While bigdata&#174; is targeted at scale-out federations,
  	  it can also be deployed as simple persistence store using
  	  just the <code>com.bigdata.journal.Journal</code> API.

  </p><p>

	  The read-write (RWStore) version of the journal can scale up to 50 billion 
	  triples or quads and is
	  capable of reclaiming storage by releasing historical commit points, aging
	  them out of the backing file in a manner very similar to how the scale-out
	  database releases historical commit points.  The read/write store is good
	  for standalone database instances, especially when the data have a lot of
	  skew and when the new data are arriving continually while older data should
	  be periodically released (for example, in a monitoring application where
	  historical events may be released after 30 days).

  </p><p>
	  
	  The read/write store is
	  also used in the scale-out architecture for the transaction manager and the
	  aggregated performance counters.  However, the data services use a WORM
	  store to buffer writes, asynchronously migrate the buffered writes onto
	  read-optimized B+Tree files using index segments builds and compacting.
	  One an index partition (aka shard) reaches ~ 200MB on the disk (dynamic
	  sharding).  Index partitions are moved from time to time to load balance
	  the cluster.
  
  </p>
  
  <h2>Status</h2>
  
  <p>

      bigdata&#174; is a petabyte scale database architecture.  It has been
      tested on clusters of up to 16 nodes.  We have loaded data sets of 10B+
      rows, at rates of over 300,000 rows per second.  Overall, 100s of billions
      of rows have been put down safely on disk.

   </p>
   
      <ul>

	  <li>Hadoop integration points, including a REST API for the sparse row
	  store and scanners for HDFS files making easier to target bigdata 
	  distributed jobs from Hadoop map/reduce jobs.</li>

	  <li>Online backup and point in time recovery.</li>

	  <li>Full distributed read/write transaction support (read-only transaction
	  support is done, but we still have some work to do on the commit protocol
	  for read-write transactions).</li>

	  <li>OODBMS.  We will be introducing an OODBM layer based on the
	  Generic Object Model shortly.  This will be layered over the RDF
	  database and will have bindings and clients for Java, JSON, and
	  other client environments.</li>

	  </ul>

  <h2>Getting Started</h2>

   <p>

         See the wiki for <a
         href="http://bigdata.wiki.sourceforge.net/GettingStarted">Getting
         Started</a> and our <a
         href="http://www.bigdata.com/bigdata/blog/">blog</a> for
         what's new.  The javadoc is <a
         href="http://www.bigdata.com/bigdata/docs/api/">online</a>
         and you can also build it with the ant script.  If you have a
         question, please post it on the blog or the forum.

   </p>

<h2>Getting Involved</h2>

<p>

         bigdata&#174; is an open source project.  Contributors and
         contributions are welcome.  Like most open source project,
         contributions must be submitted under a contributor
         agreement, which must be signed by someone with the
         appropriate authority.  This is necessary to ensure that the
         code base remains open.

   </p><p>

	 If you want to help out, please check out what is going on
	 our <a href="http://www.bigdata.com/bigdata/blog/">blog</a>
	 and on the <a
	 href="https://sourceforge.net/projects/bigdata/">main project
	 site</a>.  Post your questions and we will help you figure
	 out where you can contribute or how to create that new
	 feature that you need.

   </p>
  
  <h2>Licenses and Services</h2>
  
  <p>
  
  	  bigdata&#174; is distributed under GPL(v2).  SYSTAP, LLC
  	  offers commercial licenses for customers who either want the
  	  value add (warranty, technical support, additional
  	  regression testing), who want to redistribute bigdata with
  	  their own commercial products, or who are not "comfortable"
  	  with the GPL license.  For inquiries or further information,
  	  please write <a
  	  href="mailto:licenses@bigdata.com">licenses@bigdata.com</a>.
  
  </p><p>

          Please let us know if you need specific feature development
  	  or help in applying bigdata&#174; to your problem.  We are
  	  especially interested in working directly with people who
  	  are trying to handle massive data, especially for the
  	  semantic web.  Please <a
  	  href="http://www.systap.com/contact.htm">contact us</a>
  	  directly.
  
  </p>

  <h2>Related links</h2>

  <dl>

  <dt>CouchDB</dt>
  <dd>http://couchdb.org/CouchDB/CouchDBWeb.nsf/Home?OpenForm</dd>

  <dt>bigtable</dt>
  <dd>http://labs.google.com/papers/bigtable.html, http://www.techcrunch.com/2008/04/04/source-google-to-launch-bigtable-as-web-service/</dd>

  <dt>map/reduce</dt>
  <dd>http://labs.google.com/papers/mapreduce.html</dd>

  <dt>Hadoop</dt>
  <dd>http://lucene.apache.org/hadoop/</dd>

  <dt>Zookeeper</dt>
  <dd>http://hadoop.apache.org/zookeeper/</dd>

  <dt>Jini/River</dt>
  <dd>http://www.jini.org/wiki/Main_Page, http://incubator.apache.org/river/RIVER/index.html</dd>

  <dt>Pig</dt>
  <dd>http://research.yahoo.com/node/90</dd>

  <dt>Sawzall</dt>
  <dd>http://labs.google.com/papers/sawzall.html</dd>

  <dt>Boxwood</dt>
  <dd>http://research.microsoft.com/research/sv/Boxwood/</dd>

  <dt>Blue Cloud</dt>
  <dd>http://www.techcrunch.com/2007/11/15/ibms-blue-cloud-is-web-computng-by-another-name/</dd>

  <dt>SimpleDB</dt>
  <dd>http://www.techcrunch.com/2007/12/14/amazon-takes-on-oracle-and-ibm-with-simple-db-beta/</dd>

  <dt>mg4j</dt>
  <dd>http://mg4j.dsi.unimi.it/</dd>

  </dl>

</body>
</html>
