RunningQuery:

  - TestJiniFederatedQueryEngine.

  - Reconcile IElementFilter, FilterConstructor, and stackable
    striterators for access paths and raw iterators.  We should be
    able to use any of the striterator patterns!

    There are a lot of partially duplicated classes which need to be
    cleared up.  This can be tested on the individual striterator /
    filter and also on the local and remote access paths.  

    Note: Some filters can be sent with a remote iterator request (for
    example, the advancer), but most are applied after we convert from
    tuples into elements.

    IFilterConstructor only works with ITupleIterator.

    IElementFilter only filters. It can not resolve, transform, chunk,
    etc. It is currently sent with the iterator to the B+Tree.  See
    AccessPath#iterator().

  - Break the DistinctSPOIterator into a Distinct(Element)Filter and a
    StripContextFilter.

  - AbstractTuple#getObject() should cache the materialized object
    since some patterns used by an AccessPath may request the object
    multiple times.  Of course, caching implies that we must clear the
    reference when advancing the iterator.  Check TupleFilter and
    friends since there might need to be more than once class which
    caches the object.  Clear the reference when the iterator is
    exhausted.

  - Adapt the code to use the cost models and decision trees and write
    Unit tests of the default and named graph access path patterns.

  - Implement subquery support and subquery operators {Union, Steps,
    Star}. (We can't test Star until we have the mutation API in
    place.)

  - PipelineType {Vectored,OneShot}.

        A vectored operator processes its inputs in chunks, producing
        output chunks each time it runs.

	An one shot operator runs exactly once for a given query and
	must wait for all of its inputs to become available before it
	can begin.  For example, SORT is a one shot operator.

  - Mutation {Insert, Delete}.  Concurrency control.  Scale-out
    mutation operators must go through the ConcurrencyManager in order
    to respect the isolation levels imposed within AbstractTask.  For
    standalone, we can using either UnisolatedReadWriteIndex or the
    ConcurrencyManager as appropriate (but how can we tell which is
    appropriate!?!).

    Note: AccessPath currently does not allow inserts (just
    deletes). Hopefully it can be extended to allow INSERT and UPDATE
    so we can use the same abstraction for mutation operators.

  - Mutation {Create, Destroy}.  This gets into resource management,
    so defer for the moment but tackle in the context of RDFS closure
    using STAR.

  - MemoryType {Chunked,Blocked}.  

	Blocked operators need to inherit some interface which
	declares annotations for the #of blocks allocated to an
	operator.  How those blocks are used is very much operator
	specific.  For example, an external merge sort can make
	different use of its buffers than some other kind of
	algorithm.
	
	OneShot operators may be allocated some #of blocks which they
	can use to buffer their inputs.  When those blocks are
	exhausted, then they will have to start dumping inputs to the
	disk. However, that is operator specific and not yet defined.
    
    /**
     * Chunked means that the operator handles variable sized chunks of
     * intermediate results where the results are managed on the Java heap.
     * Chunked operators are useful for low-latency queries where the latency to
     * the first result should be low. Chunked operators generally have low
     * latency to the first query result because the data flows the operators in
     * smaller chunks with high overall parallelism in the query.
     * <p>
     * Chunk sizes of ~ 100 appear to work well for low latency queries and
     * strike a balance several factors, including: latency, heap churn, RMI
     * overhead (in scale-out), and IO vectoring. Chunks may be automatically
     * combined by the pipeline. In scale-out, chunks may be migrated onto the C
     * heap for transfer across network boundaries and transparently inflated
     * into Java objects by {@link #Chunked} operators.
     * <p>
     * Java GC can handle high object creation rates very nicely as long as the
     * objects have short life cycles. To avoid causing Java GC problems, the
     * chunk size should be kept moderate such that chunked operators do not
     * create large collections of long lived objects.
     */
    Chunked,

    /**
     * Blocked means that the operator natively manages some number of fixed
     * capacity {@link ByteBuffer}s. Such buffers are allocated on the C heap
     * using {@link ByteBuffer#allocateDirect(int)}. A population of such
     * buffers are managed by the {@link DirectBufferPool}. Direct buffers
     * provide fast transfer between disk and network subsystems and may be used
     * to share data with other devices, including GPUs.
     */
    Blocked,

Note: Many of the maxParallel annotations related to thread
consumption will go away with Java7 and async file IO.  Other
annotations, such as the #of 1M buffers to allocate to an operator,
need to be introduced to handle high volume queries.

Note: UNION, STEPS, and STAR(transitive closure) are all evaluated on
the query controller.  Do these together.  However, I need to handle
Insert/Delete before I can do STAR since it is predicated on the
mutation count reporting.  Also, make sure that we CAN do unisolated
access path reads, just in case.

----

SPARQL named graph query patterns.  

Standalone: 

  Named graph queries use an expander pattern.  See
  NamedGraphSolutionExpander. 

  The following special cases exist:

  - The named graph data set is empty (no graphs were identified which
    are known to the database), in which case an empty access path is
    used.

  - The named graph data set includes a single graph which is known to
    the database.  C is bound and we use the normal access path (this
    is done by special case logic in NamedGraphSolutionExpander and
    should be done by the query rewrite instead).

  - The named graph data set includes all graphs.  C is left unbound
    and the unmodified access path is used.

  - The named graph data set includes more than a threshold number of
    graphs.  The context position is left unbound and an IN filter is
    applied to retrict the access path to the desired graphs.  See
    NamedGraphsFilteredAccessPath.

    FIXME The threshold for this case is 200, which is WAY too
	  low. 
	  
	  For standalone the decision should be based on whether more
	  leaves would be read (the tuple range count may be used as a
	  proxy for this) by issuing individual subqueries for the
	  specific as bound predicates or by reading with C unbound.

	  For scale-out, the decision is different since we can use
	  multi-block iterators on the index segments and do fewer
	  disk seeks.

  - The named graph data set includes more than one graph but less
    than some threshold #of graphs.  Parallel subtasks are evaluated
    for each graph in the data set and write on a shared
    BlockingBuffer.  See NamedGraphsParallelEvaluationAccessPath. 

    @todo This is equivelant to an in-memory join, but does not
    scale-out.  Replace this with a DataSetJoin rather than the
    expander pattern since that will work for both standalone and
    scale-out.

Scale-out:

  As per above, except:

  - When the #of named graphs is moderate, the data set graphs are
    joined with the source binding sets (using DataSetJoin) to produce
    a cross product in which each source binding set is replicated for
    each distinct data set graph.  Those binding sets are then fed
    into a second join which reads on the access path for the
    appropriate statement index.

  - When the #of named graphs is large we need to do something special
    to avoid sending huge graph sets around with the query.

----

SPARQL default graph query patterns.  

Note: Default graph queries require us to apply a distinct {s,p,o}
filter to each default graph access path.  The default graph queries
uses an expander pattern.  See DefaultGraphSolutionExpander and its
inner classes.

The following special cases exist:

  - The default graph data set is empty (no graphs were identified
    which are known to the database), in which case an empty access
    path is used.

  - The default graph data set includes a single graph which is known
    to the database.  C is bound and we impose a filter which strips
    off the context position.  Because C takes on only one value, a
    distinct filter is not required.  This means that scale-out can
    use normal pipeline joins.

    See StripContextAccessPath.

  - ___C index: We know that C is strictly ascending in index order
    within each triple.  Use an advancer pattern or ignore quads until
    the data changes to a new triple.  Apply a filter to strip off the
    context position.

    The same optimization works in scale-out using shard-wise pipeline
    joins if the ___C index was created with the constraint that the
    all quads for a given triple are on the same shard.

  - SCALEOUT and VERY HIGH VOLUME: Use a distributed external merge
    sort to impose distinct and do operator at a time processing.

  - SCAN and FILTER: The default graph data set includes all graphs OR
    the cost of scanning with C unbound is less than the cost of
    subqueries with C bound (for scale-out, subquery cost must be
    estimated for a remote access path).  C is left unbound and we
    impose a distinct SPO filter which strips off the context
    position.  Unless all graphs are being merged, we also apply an IN
    filter.

    SCALEOUT: The join evaluation context is ANY, uses a remote access
              path, and the access path should be configured to move a
              lot of data efficiently over the remote range iterator.

              - It is possible to partition the IN filter based on the
	        shard on which it will be applied (split the ordered
	        list of contexts based on the contexts found in a
	        given shard).

    See MergeAllGraphsAccessPath.

  - SUBQUERY: Parallel subtasks are evaluated for each graph in the
    data set and write on a shared BlockingBuffer.  The BlockingBuffer
    is wrapped with an SPORelation.distinctSPOIterator().

    SCALEOUT: Mark the join evaluation context as ANY and mark the
	      access path as remote.

              Tune the capacity for the remote access path iterator.
	      When the remote access path will be selective, the
	      capacity should be small and we will pay a high price if
	      there are a lot of nested subqueries.  When the remote
	      access path is less selective the capacity should be
	      larger to reduce the #of RMI requests made per access
	      path.

	      Note: The way the code is written, the access path will
	      do RMI for the range count before issuing the RMI
	      iterator request.  Look at ways to optimize this.

    See DefaultGraphParallelEvaluationAccessPath.

- @todo Lazily create the hash map for the distinctSPOIterator when we
  observe the 2nd distinct SPO value.

---
UNION(ops)[maxParallel(default all)]

Executes each of the operands in the union as subqueries.  Each
subquery is run as a separate RunningQuery but is linked to the parent
query in which the UNION is being evaluated.  The subqueries do not
receive bindings from the parent and may be executed independently.

Note: In order to avoid materializing all of the intemediate results
on the query controller, the target for the subqueries SHOULD be
overriden to be whatever operator is the parent of the UNION.

---
STEPS(ops)[maxParallel(default 1)]

The operands are executed as independent subqueries. 

@todo It should be possible to write the results for each step onto a
named query local resource so they can be reused in subsequent steps.

@todo This operator is really no different from UNION.  UNION defaults
to running all in parallel while STEPS defaults to running them
sequentially.  In addition, there is an assumption for UNION that the
operands return binding sets and an assumption for STEPS that they are
mutation operators.  However, what makes the UNION work is that the
operands target the UNION's parent.

---

STAR(op) [maxItr(default all)]

Evaluate the operand until its mutation count remains unchanged from
one round to the next.  The operand must write on a resource.  The
fixed point is determined by examining BOPStats.mutationCount.

Do with INSERT/REMOVE since all involve mutation.

---
INSERT(op,pred) : insert elements into an index.
DELETE(op,pred) : remove elements from an index.

The access path mutation operators construct elements from the source
binding sets and the asBBound predicates.  For each element so
constructed, they insert/ remove the corresponding element into/from
the access path.  These operators update a mutation counter IFF the
access path was modified for the constructed element.  STAR relies on
the mutation operator to detect a fixed point.

The mutation access paths need to use the appropriate concurrency
control to ensure the constraint on the mutable B+Tree is respected.
This is either the UnisolatedReadWriteIndex or the LockManager /
ConcurrencyManager.

The basic mutation operators write on an access path and may be
combined using STEPS in order to update all of the indices associated
with a relation.

  - For incremental TM, we also need to construct an element for the just index
    from the rule and assert it onto that index.
    
  - For the lexicon, we also need to write on the full text index.
  
  - For SIDs mode, we also need to capture the logic to ground the statements by
    binding the SIDs.

  - triggers could be integrated here.  perhaps events backed by a queue which
    could be either restart safe or query local?

----
Parallel distributed closure : TBD.  Review notes posted on trak.

----
Lexicon joins - 

====
Features:

 - operator-at-once evaluation. The operator is triggered once its possible
   triggers are done.  This is just an application of the same utility method
   which we use to decide when a query is done.

 - ISimpleSplitHandler to enforce constraint on the SPOC index such
   that an SPO prefix is never split.  This has to be IV aware since
   we are now using variable length keys in the statement indices.

 - query and connection local resources: creating, destroying and
   using resources.  references to query local resources permit reuse
   of intermediate results across different.
   
   CREATE FOO AS TEMP GRAPH ON LOCAL TEMP STORE SPO ONLY SHARED LEXICON
 
 - subquery evaluation (linked parent to child). a subquery may be cancelled
   by a slice without cancelling the parent.  cancelling the parent terminates
   all subqueries.  whenever a query or subquery is terminated, we need to go
   through its operator and query life cycle tear down methods (unit tests).
 
 - "thick" resources which can be sent along with the query or access either by
   RMI or copied to the node where the query is running on demand.  (This could
   be just alternative access path instantiations which are selected by the query
   optimizer or defaulted based on the amount of data to be moved to/from the
   node if not specified.)

 - The Predicate could have fromRevision/toRevision annotations which would be
   used for fast computation of the delta between two historical commit points.

   
 * FIXME Unit tests for non-distinct {@link IElementFilter}s on an
 * {@link IPredicate}, unit tests for distinct element filter on an
 * {@link IPredicate} which is capable of distributed operations. Do not use
 * distinct where not required (SPOC, only one graph, etc).
 * <p>
 * It seems like the right way to approach this is by unifying the stackable CTC
 * striterator pattern with the chunked iterator pattern and passing the query
 * engine (or the bop context) into the iterator construction process (or simply
 * requesting that the query engine construct the iterator stack).
 * <p>
 * In terms of harmonization, it is difficult to say which way would work
 * better. In the short term we could simply allow both and mask the differences
 * in how we construct the filters, but the conversion to/from striterators and
 * chunked iterators seems to waste a bit of effort.
 * <p>
 * The trickiest part of all of this is to allow a distributed filter pattern
 * where the filter gets created on a set of nodes identified by the operator
 * and the elements move among those nodes using the query engine's buffers.
 * <p>
 * To actually implement the distributed distinct filter we need to stack the
 * following:
 * 
 * <pre>
 * - ITupleIterator
 * - Resolve ITuple to Element (e.g., SPOC).
 * - Layer on optional IElementFilter associated with the IPredicate.
 * - Layer on SameVariableConstraint iff required (done by AccessPath) 
 * - Resolve SPO to SPO, stripping off the context position.
 * - Chunk SPOs (SPO[], IKeyOrder), where the key order is from the access path.
 * - Filter SPO[] using DHT constructed on specified nodes of the cluster.
 *   The SPO[] chunks should be packaged into NIO buffers and shipped to those
 *   nodes.  The results should be shipped back as a bit vectors packaged into
 *   a NIO buffers.
 * - Dechunk SPO[] to SPO since that is the current expectation for the filter
 *   stack.
 * - The result then gets wrapped as a {@link IChunkedOrderedIterator} by
 *   the AccessPath using a {@link ChunkedArrayIterator}.
 * </pre>
 * 
 * This stack is a bit complex(!). But it is certainly easy enough to generate
 * the necessary bits programmatically.
 * 
 * FIXME Handling the {@link Union} of binding sets. Consider whether the chunk
 * combiner logic from the {@link DistributedJoinTask} could be reused.
 * 
 * FIXME INSERT and DELETE which will construct elements using
 * {@link IRelation#newElement(java.util.List, IBindingSet)} from a binding set
 * and then use {@link IMutableRelation#insert(IChunkedOrderedIterator)} and
 * {@link IMutableRelation#delete(IChunkedOrderedIterator)}. For s/o, we first
 * need to move the bits into the right places so it makes sense to unpack the
 * processing of the loop over the elements and move the data around, writing on
 * each index as necessary. There could be eventually consistent approaches to
 * this as well. For justifications we need to update some additional indices,
 * in which case we are stuck going through {@link IRelation} rather than
 * routing data directly or using the {@link IAsynchronousWriteBufferFactory}.
 * For example, we could handle routing and writing in s/o as follows:
 * 
 * <pre>
 * INSERT(relation,bindingSets) 
 * 
 * expands to
 * 
 * SEQUENCE(
 * SELECT(s,p,o), // drop bindings that we do not need
 * PARALLEL(
 *   INSERT_INDEX(spo), // construct (s,p,o) elements and insert
 *   INSERT_INDEX(pos), // construct (p,o,s) elements and insert
 *   INSERT_INDEX(osp), // construct (o,s,p) elements and insert
 * ))
 * 
 * </pre>
 * 
 * The output of the SELECT operator would be automatically mapped against the
 * shards on which the next operators need to write. Since there is a nested
 * PARALLEL operator, the mapping will be against the shards of each of the
 * given indices. (A simpler operator would invoke
 * {@link SPORelation#insert(IChunkedOrderedIterator)}. Handling justifications
 * requires that we also formulate the justification chain from the pattern of
 * variable bindings in the rule).
 * 
 * FIXME Handle {@link Program}s. There are three flavors, which should probably
 * be broken into three operators: sequence(ops), set(ops), and closure(op). The
 * 'set' version would be parallelized, or at least have an annotation for
 * parallel evaluation. These things belong in the same broad category as the
 * join graph since they are operators which control the evaluation of other
 * operators (the current pipeline join also has that characteristic which it
 * uses to do the nested index subqueries).
 * 
 * FIXME SPARQL to BOP translation
 * <p>
 * The initial pass should translate from {@link IRule} to {@link BOp}s so we
 * can immediately begin running SPARQL queries against the {@link QueryEngine}.
 * A second pass should explore a rules base translation from the openrdf SPARQL
 * operator tree into {@link BOp}s, perhaps using an embedded {@link Prolog}
 * engine. What follows is a partial list of special considerations for that
 * translation:
 * <ul>
 * <li>Distinct can be trivially enforced for default graph queries against the
 * SPOC index.</li>
 * <li>Local distinct should wait until there is more than one tuple from the
 * index since a single tuple does not need to be made distinct using a hash
 * map.</li>
 * <li>Low volume distributed queries should use solution modifiers which
 * evaluate on the query controller node rather than using distributed sort,
 * distinct, slice, or aggregation operators.</li>
 * <li></li>
 * <li></li>
 * <li></li>
 * <li>High volume queries should use special operators (different
 * implementations of joins, use an external merge sort, etc).</li>
 * </ul>
 * 
 * FIXME SPARQL Coverage: Add native support for all SPARQL operators. A lot of
 * this can be picked up from Sesame. Some things, such as isIRI() can be done
 * natively against the {@link IV}. Likewise, there is already a set of
 * comparison methods for {@link IV}s which are inlined values. Add support for
 * <ul>
 * <li></li>
 * <li></li>
 * <li></li>
 * <li></li>
 * <li></li>
 * <li></li>
 * </ul>

------------------------------------------------------------

Some problems:

- Should Appender simply pass the other iterator as m_state to the FilterBase?

- Mapper uses non-Serializable "Method".  We should provide defered reflection for the method.

- Merger uses non-Serializable "Iterator".

- Sorter and Resolver should use an annotation pattern so we can create instances from Prolog of the operator.  I do not think we can do this when the operator is abstract.  Maybe we can have two "Resolver" classes, two "Sorter" classes, etc.  The simple one can be used for inline programming.  The other one will extend BOpBase and implement IFilter and will be used for query.

- CONTRACTOR should be able to break an iterator into many chunks, not just one.  Maybe the API should return an Iterator from an Iterator in which the chunkiness is changed (from element to element[])?

Here is what I have not done yet:

- Striterator unit tests.

- Bop-erator tests.

- MGC: All of the Filterators need to use deferred prefetch.  Prefetch
  during the constructor causes problems when we are stacking filters
  using FilterBase.  (This is also true for the ITuple filters).

- BT: Write unit tests at the IPredicate/AccessPath level to use
  stackable filters (for the LOCAL and REMOTE access paths).  Write
  unit tests for correct range counts with and without local/remote
  filters.  Write unit tests for caching of those range counts.

- Write BOp versions for TupleResolver, ...

- done. IPredicate#getConstraint() must go (issues remain with
  BackchainAccessPath).

- The (SPO|Magic)Predicate contructors must be touched up.  

  - They assume an IElementFilter (IFilterTest). However, the filters
    are now specified with IFilter. 

  - Those constructor will DROP the local and remote filters unless
    the logic is modified.

  - When used remotely, IElementFilter must do ITuple.getObject() and
    MUST be wrapped using ElementFilter to be "Tuple" aware (problem
    with Tuple reference reuse inside the ITupleIterators).

  - Reconcile IElementFilter and implementations.  IElementFilter was
    transparently wrapped by AccessPath to resolve the ITuple to the
    element before applying the filter.  If IElementFilter is used in
    contexts other than the index then all implementations must be
    modified to conditionally resolve the element while filtering.

  - IPredicates created from a Relation MUST use the timestamp of that
    relation NOT READ_COMMITTED.  Remove the default timestamp
    annotation.  Make this required where it is used so we can track
    all of this down.

MikeP:

  - BackchainAccessPath.  Are the index local or access path filters
    ever used here?  If they are then this code needs to be modified.

  - The timestamp on which the backchain access path will read MUST be
    passed through to the IPredicate constructors.

  - Leave C as an anonymous variable when it will not be used, not
    null.

  - Replace NamedGraph and DefaultGraph access paths per the decision
    tree and cost model.

  - Set the TIMESTAMP on the predicate.

  - Additional SPOPredicate and Predicate constructor cleanup.

Reconcile the com.bigdata.striterators package.  Can we get rid of it?
Incrementally?
