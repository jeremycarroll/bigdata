/**

Copyright (C) SYSTAP, LLC 2006-2007.  All rights reserved.

Contact:
     SYSTAP, LLC
     4501 Tower Road
     Greensboro, NC 27410
     licenses@bigdata.com

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; version 2 of the License.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with this program; if not, write to the Free Software
Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
*/
/*
 * Created on Feb 15, 2007
 */

package com.bigdata.journal;

import java.io.File;
import java.io.IOException;
import java.nio.ByteBuffer;
import java.util.UUID;
import java.util.concurrent.TimeUnit;

import org.apache.log4j.Logger;

import com.bigdata.counters.CounterSet;
import com.bigdata.io.DirectBufferPool;
import com.bigdata.mdi.AbstractResourceMetadata;
import com.bigdata.mdi.IResourceMetadata;
import com.bigdata.rawstore.AbstractRawWormStore;
import com.bigdata.rawstore.Bytes;
import com.bigdata.rawstore.IMRMW;
import com.bigdata.rawstore.IMROW;
import com.bigdata.rawstore.IRawStore;
import com.bigdata.rawstore.WormAddressManager;
import com.bigdata.util.ChecksumUtility;

/**
 * A non-restart-safe store for temporary data that buffers data in memory until
 * a {@link #maximumInMemoryExtent} has been reached and then converts to a
 * disk-based store with a maximum capacity determined by the configuration of
 * the {@link WormAddressManager}. On conversion to a disk-backed store, the
 * disk file is created using the temporary file mechansism and is marked for
 * eventual deletion no later than when the JVM exits and as soon as the store
 * is {@link #close() closed}.
 * 
 * @author <a href="mailto:thompsonbry@users.sourceforge.net">Bryan Thompson</a>
 * @version $Id$
 * 
 * @todo add configuration properties.
 * 
 * FIXME Use the {@link DiskOnlyStrategy} here but add an option for lazy
 * creation of the file on the disk, disable forcing of writes or force on
 * commit, and mark the file as "temporary". This will let us write in memory
 * until the write cache overflows and then it will start putting down the data
 * on the disk. This has all of the advantages of the current approach (low
 * latency on startup), plus we get MRMW for the temp store.
 */
public class TemporaryRawStore extends AbstractRawWormStore implements IRawStore, IMROW {

    protected static final Logger log = Logger.getLogger(TemporaryRawStore.class);
    
    protected final static int DEFAULT_INITIAL_IN_MEMORY_EXTENT = Bytes.megabyte32 * 1;

    protected final static int DEFAULT_MAXIMUM_IN_MEMORY_EXTENT = Bytes.megabyte32 * 10;
    
    /**
     * The initial size of the in-memory buffer. This buffer will grow as
     * necessary until {@link #maximumInMemoryExtent} at which point the store
     * will convert over to a disk-based mechanism.
     */
    public final long initialInMemoryExtent;
    
    /**
     * The maximum capacity of the in-memory buffer.
     */
    public final long maximumInMemoryExtent;
    
    /**
     * Whether or not a direct buffer was used for the in-memory store (not
     * recommended).
     */
    public final boolean useDirectBuffers;

    private boolean open = true;
    
    private IBufferStrategy buf;
    
    /**
     * When non-<code>null</code> this is a direct {@link ByteBuffer}
     * allocated using the {@link DirectBufferPool} during
     * {@link #overflowToDisk()} and handed off to the {@link DiskOnlyStrategy}
     * for use as its write cache. When non-<code>null</code> this buffer is
     * {@link DirectBufferPool#release(ByteBuffer)}ed back to the
     * {@link DirectBufferPool} in {@link #finalize()} to avoid a native memory
     * leak.
     */
    private ByteBuffer writeCache = null;
    
    /**
     * Store identifier.
     */
    private UUID uuid = UUID.randomUUID();
    
    /**
     * Note: this timestamp is NOT generated by a centralized time server. 
     */
    private long createTime = System.currentTimeMillis();

    /**
     * Note: Temporary stores do not have persistent resource descriptions.
     */
    public IResourceMetadata getResourceMetadata() {
        
        final File file = buf.getFile();
        
        final String fileStr = file == null ? "" : file.toString();
        
        return new AbstractResourceMetadata(fileStr, buf.getExtent(), uuid, createTime) {
        
            private static final long serialVersionUID = 1L;

            public boolean isJournal() {
                return false;
            }
        
            public boolean isIndexSegment() {
                return false;
            }
        
        };
        
    }
    
    public IBufferStrategy getBufferStrategy() {
        
        return buf;
        
    }

    /**
     * Create a {@link TemporaryRawStore} with an initial in-memory capacity of
     * 10M that will grow up to 100M before converting into a disk-based store
     * backed by a temporary file. These defaults are appropriate for a
     * relatively small number of processes that will write a lot of data. If
     * you have a lot of processes then you need to be more conservative with
     * RAM in the initial allocation and switch over to disk sooner. For
     * example, transactions use smaller defaults in order to support a large
     * #of concurrent transactions without a large memory burden.
     * 
     * @todo the memory growth strategy does not respect the in-memory maximum
     *       without parameterizing and overriding the #of bytes to extent the
     *       store on overflow for {@link TransientBufferStrategy}
     */
    public TemporaryRawStore() {

        this(WormAddressManager.SCALE_UP_OFFSET_BITS);
        
    }
    
    public TemporaryRawStore(int offsetBits) {

        this(offsetBits,
                DEFAULT_INITIAL_IN_MEMORY_EXTENT,
                DEFAULT_MAXIMUM_IN_MEMORY_EXTENT,
                false // useDirectBuffers (NO!)
                );
        
    }

    /**
     * Create a {@link TemporaryRawStore} with the specified configuration.
     * 
     * @param offsetBits
     *            This determines the capacity of the store file and the maximum
     *            length of a record.  The value is passed through to
     *            {@link WormAddressManager#WormAddressManager(int)}.
     * @param initialInMemoryExtent
     *            The initial size of the in-memory buffer. This buffer will
     *            grow as necessary until <i>maximumInMemoryExtent</i> at which
     *            point the store will convert over to a disk-based mechanism.
     * @param maximumInMemoryExtent
     *            The maximum capacity of the in-memory buffer. The actual
     *            maximum may differ slightly based on the buffer growth policy.
     * @param useDirectBuffers
     *            Whether or not the in-memory buffer will be direct. The use of
     *            a direct buffer here is NOT recommended.
     */
    public TemporaryRawStore(int offsetBits, long initialInMemoryExtent,
            long maximumInMemoryExtent, boolean useDirectBuffers) {
        
        super(offsetBits);
        
        buf = new TransientBufferStrategy(offsetBits, initialInMemoryExtent,
                maximumInMemoryExtent, useDirectBuffers);
        
        this.initialInMemoryExtent = initialInMemoryExtent;
        
        this.maximumInMemoryExtent = maximumInMemoryExtent;
        
        this.useDirectBuffers = useDirectBuffers;
        
    }

    /**
     * Closes the store if it gets GCd.
     */
    protected void finalize() throws Throwable {
        
        try {
            
            if(open) {
                
                log.warn("Closing temp store.");
                
                close();
                
            }
            
            if (writeCache != null) {

                DirectBufferPool.INSTANCE.release(writeCache);
                
                writeCache = null;
                
            }
            
        } catch (Throwable t) {
            
            t.printStackTrace(System.err);
            
        }
        
        super.finalize();
        
    }
    
    public File getFile() {
        
        if(!open) throw new IllegalStateException();
        
        return buf.getFile();
        
    }
    
    /**
     * Close the store and delete the associated file, if any.
     */
    public void close() {

        if(!open) throw new IllegalStateException();
        
        open = false;
        
        buf.closeAndDelete();
        
        buf = null;
        
    }

    /**
     * Note: This operation is a NOP since {@link #close()} always deletes the
     * backing file and {@link #deleteResources()} requires that the store is closed as a
     * pre-condition.
     */
    public void deleteResources() {
        
        if(open) throw new IllegalStateException();
        
        /*
         * NOP
         */
        
    }
    
    public void closeAndDelete() {
    
        // Close already deletes the backing file.
        close();
        
    }

    public void force(boolean metadata) {
        
        if(!open) throw new IllegalStateException();
        
        buf.force(metadata);
        
    }

    public long size() {
        
        return buf.size();
        
    }
    
    public boolean isOpen() {
        
        return open;
        
    }

    public boolean isReadOnly() {
        
        if(!open) throw new IllegalStateException();
        
        return false;
        
    }
    
    /**
     * Always returns <code>false</code> since the store will be deleted as soon
     * as it is closed.
     */
    public boolean isStable() {
    
        if(!open) throw new IllegalStateException();
        
        return false;
        
    }
    
    public boolean isFullyBuffered() {
        
        if(!open) throw new IllegalStateException();
        
        return buf.isFullyBuffered(); 
        
    }

    /**
     * FIXME readers and writers are being serialized since in order to ensure
     * that {@link #overflowToDisk()} is invoked during a time when no reader
     * and no other writer is trying to access the store. Examine ways to
     * increase the concurrency here. Both the transient and the disk-only modes
     * are fully {@link IMRMW}. It is only the transition that requires us to
     * serialize access.  Also note that the write task can wait when it needs
     * to {@link #overflowToDisk()} since readers can continue to read against
     * the transient buffer (as long as the write task eventually runs).
     */
    synchronized public ByteBuffer read(long addr) {

        if(!open) throw new IllegalStateException();

        return buf.read(addr);
        
    }

    /**
     * FIXME This method is <code>synchronized</code> so that overflow of the
     * buffer from memory to disk will be atomic. While this provides both
     * {@link IMROW} and {@link IMRMW} guarentees for the
     * {@link TemporaryRawStore} it does so at the expense of serializing calls
     * to {@link #write(ByteBuffer)}.
     * <p>
     * Explore options for greater concurrency here in support of concurrent
     * tasks executing against the same transaction or map/reduce jobs writing
     * on a temporary store from multiple map tasks.
     */
    synchronized public long write(ByteBuffer data) {
        
        if(!open) throw new IllegalStateException();
        
        try {
            
            /*
             * Note: this operation will transparently extend the in-memory
             * buffer as necessary up to the specified maximum capacity.
             */
            return buf.write(data);
            
        } catch(OverflowException ex) {
        
            if(buf instanceof TransientBufferStrategy) {

                overflowToDisk();
                
                return buf.write(data);
                
            } else {
                
                throw ex;
                
            }
            
        }
        
    }

    /**
     * Spills the in-memory buffer onto the disk and replaces the
     * {@link TransientBufferStrategy} with a {@link DiskOnlyStrategy}.
     * <p>
     * Note: The {@link TemporaryRawStore} transitions from a transient store
     * (fully buffered, so interrupts can not cause the channel to be closed) to
     * a disk-only store (already knows how to handle interrupts).
     * <p>
     * The only place where interrupts could be a problem is during a transition
     * from transient to disk-only, i.e., in this method. However, we already
     * force readers and writers to synchronize for an overflow event so there
     * will only be the one writer running when this method is invoked.
     */
    private void overflowToDisk() {

        log.info("TemporaryRawStore: overflow to disk; nbytes="
                + buf.getNextOffset());
        
        TransientBufferStrategy tmp = (TransientBufferStrategy)buf;
        
        File file;
        
        try {

            file = File.createTempFile("bigdata", ".tmpStore");
            
        } catch (IOException ex) {
            
            throw new RuntimeException(ex);
            
        }
        
        /*
         * Set the initial extent to be large enough for the root blocks plus
         * twice the data in the in-memory buffer.
         */
        final long initialExtent = FileMetadata.headerSize0 + tmp.getUserExtent() * 2;
        
        final long maximumDiskExtent = Bytes.gigabyte32 * 2;
        
        final boolean create = false;
        
        final boolean isEmptyFile = true;
        
        final boolean deleteOnExit = true;
        
        final boolean readOnly = false;
        
        // Do not force writes since the store is not restart safe.
        final ForceEnum forceWrites = ForceEnum.No;

        // The store is never pre-existing so we do not have a checksum to validate.
        final boolean validateChecksum = false;
        
        // the local system time.
        final long createTime = System.currentTimeMillis();
        
        // We still need an object to compute the checksum to be stored in the root blocks.
        final ChecksumUtility checker = new ChecksumUtility();
        
        /*
         * Create a unique store file and setup the root blocks. The file will
         * be pre-extended to the requested initialExtent.
         * 
         * @todo There is a bug in the release of large temporary direct
         * ByteBuffers. For regular journals that overflow, the write cache is
         * allocated once and handed off from journal to journal. However, there
         * is no such opportunity for temporary stores. Also, the overflow
         * operation itself will cause a "temporary" direct buffer to be
         * allocated and thereby create a memory leak.
         * 
         * The current workaround uses a JVM-wide pool of direct ByteBuffers.
         * During overflow, the data is transferred from the heap ByteBuffer
         * backing the TransientBufferStrategy onto a direct ByteBuffer, and
         * from there onto disk using a sequence of writes until all data has
         * been transferred. At that point the direct ByteBuffer is either
         * released back to the pool or a reference is saved on the temporary
         * raw store and the direct byte buffer is handed off to the disk only
         * strategy for use as a write cache. (In fact, the operations must be
         * performed in a slightly different sequence since we need to hand the
         * write cache buffer to the disk only strategy before we transfer the
         * data onto the disk).
         * 
         * @see http://bugs.sun.com/bugdatabase/view_bug.do;jsessionid=8fab76d1d4479fffffffffa5abfb09c719a30?bug_id=6210541
         * 
         * @todo use a read cache when converting to a DiskOnlyStrategy?
         */
        
        try {

            writeCache = DirectBufferPool.INSTANCE.acquire(2000, TimeUnit.MILLISECONDS);
            
        } catch(Exception ex) {
        
            throw new RuntimeException("Could not allocate buffer: " + ex, ex);
        
        }
        
        FileMetadata fileMetadata = new FileMetadata(file, BufferMode.Disk,
                useDirectBuffers, initialExtent, maximumDiskExtent, create,
                isEmptyFile, deleteOnExit, readOnly, forceWrites,
                getOffsetBits(), 0/* readCacheCapacity */, 0/*readCacheMaxRecordSize*/,
                writeCache, validateChecksum, createTime, checker);
        
        // Open the disk-based store file.
        DiskOnlyStrategy diskBuf = new DiskOnlyStrategy(Bytes.gigabyte * 2,
                fileMetadata);

        try {

            /*
             * Transfer the data from the in-memory buffer to the disk.  The
             * write begins immediately after the file header.
             */
            
//            // setup the transfer source.
//            ByteBuffer b = tmp.getBuffer();
//            b.limit((int)tmp.nextOffset);
//            b.position(0);
//            
//            diskBuf.writeOnDisk(b, 0L);
//            
////            // write the data on the channel.
////            diskBuf.getChannel().write(b,diskBuf.getHeaderSize());

            /*
             * This transfers the data from the heap buffer backing the
             * TemporaryRawStore onto a direct byte buffer that will also be
             * used for the write cache by the DiskOnlyStrategy in a sequence of
             * operations, each operation transferring at most one "write cache"
             * worth of data onto the disk.
             * 
             * Note: We go through this hassle with the [writeCache] because it
             * is a _direct_ ByteBuffer and Java therefore WILL NOT allocate a
             * "temporary" direct buffer for these operations.
             */
            
            // the heap buffer for the TransientBufferStrategy.
            final ByteBuffer data = tmp.getBuffer();

            // should be a heap buffer.
            assert data.hasArray();

            // setup to transfer all bytes from [0:tmp.nextOffset].
            data.limit((int)tmp.nextOffset);
            data.position(0);

            // #of bytes to transfer.
            final int nbytes = data.remaining();

            // position the file channel after the root blocks.
            diskBuf.getChannel().position(diskBuf.getHeaderSize());

            int count = 0;
            int nwrites = 0;

            while (data.remaining() > 0) {

                // reset the direct buffer used to manage the transfers.
                writeCache.clear();

                // #of bytes to copy from the heap buffer to the direct buffer.
                final int nxfer = Math.min(data.remaining(), writeCache
                        .capacity());

                // copy from the heap buffer to the direct buffer.
                writeCache.put(data.array(), data.arrayOffset() + count, nxfer);
                
                // advance by the #of bytes copied from the heap buffer.
                data.position(data.position() + nxfer);
                
                // prepare the direct buffer for writing.
                writeCache.flip();

                // write the direct buffer on the disk, updating the channel
                // position.
                count += diskBuf.getChannel().write(writeCache);

                nwrites++;

            }

            if (count != nbytes) {

                throw new RuntimeException("Expecting to write " + nbytes
                        + " bytes, but wrote " + count + " bytes in " + nwrites);

            }

            /*
             * Reset the write cache to make sure that it is empty before the
             * DiskOnlyStrategy begins to use this buffer.
             */
            
            writeCache.clear();

            /*
             * Increment the offset - this is where the next write will be made
             * on the backing file.
             */

            diskBuf.nextOffset += tmp.nextOffset;
            
        } catch(Throwable t) {
            
            try {

                diskBuf.close();
                
                if (writeCache != null) {

                    DirectBufferPool.INSTANCE.release(writeCache);
                    
                    writeCache = null;
                    
                }
                
            } catch (Throwable ex2) {

                log.warn(ex2, ex2);
                
            }
            
            throw new RuntimeException(t);
            
        }
        
        this.buf = diskBuf;
        
    }
    
    /**
     * The maximum length of a record that may be written on the store.
     */
    final public int getMaxRecordSize() {

        return ((AbstractRawWormStore) buf).getAddressManger()
                .getMaxByteCount();

    }

    synchronized public CounterSet getCounters() {

        if (root == null) {

            root = new CounterSet();

        }

        return root;

    }
    private CounterSet root;

}
