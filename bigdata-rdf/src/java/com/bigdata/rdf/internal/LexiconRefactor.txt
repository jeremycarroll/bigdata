These are my notes on the lexicon refactor.

       #5. Order preserving coding in the statement indices together
           with less data stored in the lexicon and exploiting that
           information in FILTERs (range queries, value tests, etc).
           This should be a lexicon option in case people want to have
           exact lexical values preserved.  The data type would be
           preserved so an xsd:int and an xsd:long would not compare
           as EQ unless they were coerced to the same data type during
           query (such coercion would require running against the
           value range for each source data type convertable to the
           target data type).

	   There are complex issues involving xsd:decimal (BigDecimal)
	   and xsd:integer (BigInteger).  These have very large
	   ranges.  Making them into inline values would require a
	   variable length component for the "termId", which in turn
	   makes it more complex to form and decode the statement
	   index keys.

	   Long literals should be supported as transparently
	   compressed blobs to get the data out of the index.  This
	   raises the question of just where to store those
	   blobs. This is the same issue for BFS or key-blob stores.
	   Basically, we need to dump them into the local file system
	   in a directory structure parallel to the journals and
	   segments.  If we move a shard, we also need to move the
	   blobs.  Note that there are relatively low limits on the
	   #of files/subdirectories in a directory for many operating
	   systems.

	   - done. Verify key component decode with variable length
             keys.

	   - Add smart separator key selection for the ID2TERM index?

	   - Note: We can not inline plain text or language code
             literals unless the ASCII option was specified.  Ditto
             for URIs.  This is because we can not decode Unicode sort
             keys.  An extensibility mechanism has been defined to
             handle this using "enumerations".

	   - Use raw records and an IOverflowHandler to move large
             literals out of the ID2TERM index.  Likewise, we should
             probably not register large literals in the TERM2ID
             index.  However, they could still be indexed, e.g., free
             text indexing or XML literal indexing. [One problem with
             not entering them into the TERM2ID index is that we can
             get duplicate entries.  Perhaps we should enter the MD5
             signature of the long literal, which is 128 bytes, as the
             key.  Those keys should all be in their own prefix space
             for long literals.]

	   - Take notes on the changes to the physical schema in order
             to support a port of data.  However, the easiest way to
             do a port is to not break the existing schema and allow
             people to export/import into a different triple store
             relation in the same database.  Even easier is export /
             import into a new database instance.

	   - Generate an unsigned byte[] coding of BigInteger (done)
             and BigDecimal (todo).

=========================

There are several wrinkles to proper handling of datatype literals,
some of which only apply when we choose to inline those literals into
the statement indices.  In general, this would result in statement
indices with variable length keys and the SPO class would have to be
modified to correctly decode datatype literals when they were inlined
within the key.  

The biggest single initial win will be not having to indirect through
the lexicon to resolve xsd:int, xsd:float, xsd:double, etc.  This will
have a huge impact for aggregations.  A secondary win, and something
which can be phased in over time, is the ability to do range queries
against datatype values.  For example, 50 < AGE < 60.  Today, the
filter is applied after the solutions have been identified.  Once we
inline numeric datatypes into the statement indices (and modulo issues
surrounding casts and filters) these queries could also be evaluated
against the desired key-range of the statement indices.  This
optimization can be approached incrementally because the existing
approach (filtering the solutions) will continue to work correctly.

Issues arise with:

- datatypes which have long representations (xsd:integer,
  xsd:decimal). It appears that we can handle both xsd:integer and
  xsd:decimal inline.  Therefore it would be reasonable to either
  represent all literals having numeric values in a single value space
  -or- represent all values for each each numeric datatype within its
  own region by using a prefix to separate the keys accordinging to
  the numeric data type.  The advantage of the former is we can filter
  by data type in a single scan across all numeric values.  The
  advantage of the latter is that we can represent the values more
  tersely since we do not need to convert everything to xsd:decimal
  before encoding it as an unsigned byte[] key component.  The second
  approach requires us to query the union of the numeric datatype key
  spaces when a cast is used in the query but will be much faster if
  no cast is used.  [We are still looking into the xsd:decimal mapping
  onto keys, but this appears to be simpler than mapping BigDecimal
  since xsd:decimal DOES NOT maintain any information about the
  precision of the value.  Representing the precision would require us
  to carry the original scale in the value associated with the tuple
  in the statement index while the key coded the value space.]

- geospatial datatypes.  these need to be mapped onto a spatial index.
  the statement would just use the termId of the literal representing
  the point, rectangle, or generalized shape of the extent.  Since we
  are assigning a termId in the lexicon, we can add geospatial indices
  later without changing anything in the existing indices.

- datatypes which correspond to durations.  I am open to suggestions
  here.  This might be handled in a manner similar to spatial data
  types or by using temporal reasoners.  Either way, we would store
  the termId in the statement index.  We would have to recognize when
  a query was addressing a one-dimensional data type for which we
  lacked a key range index scan and handle it as we handle all queries
  today (by resolving the lexical values from the id2term index and
  then filtering for GT, LT, or EQ).

- Infinately repeating intervals, e.g., the 5th day of each month.  We
  can't really code such things into a spatial index since it would
  require an infinite number of bands on the index.  I am thinking
  that temporal reasoners are the only real solution here.

- dateTime. There is no XML schema datatype which corresponds to an
  unambiguous timeOnTimeline value.  Something similar can be derived
  from xsd:dateTime.  In order to also preserve the timezone we would
  have to encode that in the value associated with the statement
  tuple.  [Alternatively, we could encode all spatial and/or temporal
  information in spatial indices, in which case the original Literal
  is stored in the id2term index, however the dateTime still needs to
  be unambiguous.]

- Long and very long literals.  these should be taken out of the
  id2term index and put into raw record blobs which are then flowed
  into the index segments for scale-out (long literals, e.g., up to a
  few mb) or an addressible file system abstraction (very long
  literals).  regardless of how we handle the literal, the statement
  index would use a termId value for the literal.  Since some literals
  can not reasonably be represented inline in the statement indices,
  none of the literals can be represented inline and we can not
  perform key-range scans on the statement indices for plain or
  language code literals.

- xml literals. per long and very long literals.  one would expect to
  do additional indexing on the XML blobs at some point, but that is
  beyond where we are today.

- extensible data types.  I think that there are easy and hard cases
  in this category.  Some interesting examples might be custom
  indexing for genomics data.  For some kinds of extensibility we
  would probably register additional indices (geospatial).

This will require that Literal#equals(Object) be implemented such that
representations of the same point in the value space are equals
without regard to their lexical form.  Comparison operators must
likewise respect the total ordering of the value spaces within each of
the data type literals (or across all of them if we choose to
interleave the value spaces for all numeric types, which I am inclined
NOT to do).  Literal#stringValue() might be a lazy method to avoid
overhead if we wrap an inline literal from a statement index as a
Literal.

One thing that we should do is mark which datatypes are inline and
assign unique codes for those data types (reserving either one or two
bytes for that information and using it to create partitions in the
key space for the inlined values).  This will provide a declarative
mechanism for the query processor so that it can understand when a
filter can be mapped onto a key-range scan of a statement index.  This
could be part of the state serialized into the global row store. An
open question is whether and how to indicate relationships among the
inline datatypes.

Bryan

    ----------

At this time, literals can only appear in the Object position of an
RDF statement.  This limits the scope of the proposed change to the
ISPO#o() method, the SPO#o public field, and the ISPO#get(int slot)
method.  However, these changes will still cause a lot of distruption
as we break the assumption that the statement index keys consist
solely of term identifiers.

Open questions:

- How to represent a statement key component which is not a term
  identifier?  Maybe we want to inline only those values which can fit
  into a long and use the high bits to differentiate, interleave all
  the numeric values except for BigInteger and BigDecimal since their
  representations are too long.  We can immediately benefit from
  inlining [but this requires an additional byte per object] IF we
  restrict our goals to fast resolution of numeric literals rather
  than range-based query of statement indices.  If we also disable
  BigInteger and BigDecimal when values are being inlined key-range
  queries on OSP(C).

  We need to create an interface / object for the internal
  representation of the RDF Value, e.g., RDFValueInternal or VI.  This
  has been a long termId up to now.  With this generalization, it can
  still be a termId (URIs, literals), the termId could be more than 64
  bits (if more partitions could be created), it can directly encode a
  boolean, char, short, int, long, BigInteger, etc.  It could even
  encode short literals directly.  We need an isTermId() method.  When
  false, the RDF Value is expressed inline.  When it is inline (not a
  termId), asValue() will skin the object as a BigdataValue.

  - Move the flag bits into a high byte (in addition to the long
    termId).  This will cluster URIs and Literals in their own parts
    of the lexicon.  It will also increase the maximum #of URIs, plain
    or language code literals, XML literals, SIDs, etc. allowed in the
    lexicon significantly.

  - Add an inline bit?  When inlining is enabled, some values will
    always be inlined (boolean, char, short, int, float, long, double,
    etc).  However, an inline bit would let us differentiate this at
    runtime based solely on the VI state.  This could be useful for
    inlining short plain text or language code literals.
    
      - Except for the inability to decode a Unicode sort key, we
        could then support a mode which inlined everything [we could
        do this anyway if we wanted to tolerate the redundancy of the
        data in the statement tuple value or if we restricted
        ourselves to ASCII].

  - Backward compatible mode where flags are in the low bits of the
    termId and we extract them to the flags byte in decode?  Or just
    break with the past?

  - Inline blank nodes? (16 bytes).

  - Inlining all values for a datatype makes it possible to translate
    LT/GT style filters into OSP(C) key-range queries when casts are
    not used.

  - Inlining all numeric datatypes makes it possible to OSP(C) queries
    when casts are used. Otherwise, we can not translate a LT / GT
    style FILTERs into a key-range query.

  - Handle better coding of URIs (scheme, reversed dns components,
    path component compression)?

  - Join processing must handle IConstant<X> differently for term
    identifiers and inline literal values.

SPOKeyOrder#encodeKey() : These methods should determine the #of key components based on isQuads()/getKeyArity()
SPOKeyOrder#decodeKey()

LexiconKeyBuilder - Review this class and then drop it.  It was an
		    alternative proposal for native datatype handling.

ISPO#o()
ISPO#get(int index)

SPO#o()
SPO(s,p,o)
SPO(s,p,o,c)
SPO(s,p,o,type)
SPO(s,p,o,c,type)
SPO(s,p,o) w/ IConstant<Long> arguments.
SPO(IPredicate<ISPO>) handling of o.
SPO(BigdataStatement) handling of o.
SPO#hashCode() The o hash component is based on the decoded point in the value space?
SPO#compareTo(ISPO) Must order the o within the value space and also put the value spaces into some order.
SPO#equals(ISPO) Uses o == foo.o(), but must test 

ITermIdCodes - Must specify inline and long literal flag bits.  Might
	       also specify how the datatype code is represented.
	       Update the javadoc since no longer the low bits of the
	       term identifier.  The TERM2ID index write procedure
	       should continue to return 64-bit long integers since
	       the caller can decorate them with the appropriate flags
	       (is this also true for long literals which get written
	       onto raw records?)

ITermIndexCodes - This partitions the TERM2ID index using a one byte
		  prefix.  This probably does not need to be changed.

KVOTermIdComparator - No change.  This is only used for operations on
		      the lexicon so it will only see term identifiers
		      rather than inline values.

LexiconRelation - RDF values which can be inlined should be handled
		  differently. Also, we need to pass around
		  collections of TermId or InternalValue objects, not
		  Longs.

TermIdEncoder - drop setFields() since the RDF Value type flags are
	        now in a header byte.
		
		? Modify to use the sign bit from the partition local
		counter and/or the partition identifier?  Right now we
		do not roll those counters around to negative values,
		but doing so would allow us to have 4x as many term
		identifiers in the lexicon (2^32 vs 2^30).

TermAdvancer - How do we form the successor for the TermAdvancer?  Is
	       this a fixed length bit string successor or do we
	       decode the Statement and then get the successor() of
	       the appropriate RDF Value (the s,p,o, or c).


IRawTripleStore
#addTerm(Value):long
#getTerm(long id):Value - this is fine as far as it goes, but it is a special case now.
#getTermId(Value):long - should probably return an InternalValue.  Look at callers.
#getAccessPath(...) - Redirect callers to SPORelation#getAccessPath()? Or just refactor to use InternalValue rather than long?
#toString(long,...) - these use be changed to use InternalValues.

AbstractTripleStore#
isURI(), isLiteral(), isBNode(), isSID() - these static methods could
		      be colocated on InternalValue with isTermId() etc.

... Finish this survey of places where the code would have to change.
    Especially, locate all code which encodes or decodes the keys for
    the statement indices.

BigdataStatement
SPOTupleSerializer#statement2Key(s,p,o)

Various SPO comparators.

SPOPredicate

...

BaseAxioms

========================================


The current xsd:integer coding uses 2 bytes (less the signum bit) for
the run length of the encoded BigInteger value.  This is pragmatic
since that would be a key component which is 32k long, and that is
getting to the point where you do not want to be putting those things
into the keys of the index.  I will add some code to catch very large
xsd:integer and xsd:decimal values and refuse to generate keys for
them.

I think that we should handle very long literals specially.  Probably
we should put them into raw journal records and store them under their
MD5 hash code (that is 128 bits and collisions are exceedingly rare)
in the TERM2ID index.  This change would not effect the termIds as
such, but rather than keys for the TERM2ID index (we would add a
prefix code for very long literals there) and the values for the
ID2TERM index (we would have a bit flag indicating that the value was
a blog reference which needed to be dereferenced by reading on an
appropriate record).  This change could handle literals of up to
several MB in size.  Of course, this also presents a challenge for
Literal#equals(Literal) for your clients.

I have already mapped out support for this kind of blob reference for
both standalone and scale-out, so there is nothing extra to do there.
Just a little bit of work in how we write on and read from the TERM2ID
and ID2TERM indices respectively.  I am pretty sure that this change
would not break either the code or the data.

============================================================

Note: appendUnsigned(byte v) already exists.  However, the rest of
these methods are new and ALL of them need unit tests.  Also, consider
passing in a data type with more bits to avoid problems with handling
unsigned values in a signed data type.

    final public KeyBuilder appendUnsigned(final long v) {

        // performance tweak
        if (len + 8 > buf.length) ensureCapacity(len+8);
        // ensureFree(1);

        // big-endian.
        buf[len++] = (byte)(v >>> 56);
        buf[len++] = (byte)(v >>> 48);
        buf[len++] = (byte)(v >>> 40);
        buf[len++] = (byte)(v >>> 32);
        buf[len++] = (byte)(v >>> 24);
        buf[len++] = (byte)(v >>> 16);
        buf[len++] = (byte)(v >>>  8);
        buf[len++] = (byte)(v >>>  0);

        return this;
        
    }

    final public KeyBuilder appendUnsigned(final int v) {

        // performance tweak
        if (len + 4 > buf.length) ensureCapacity(len+4);
        // ensureFree(1);

        // big-endian
        buf[len++] = (byte)(v >>> 24);
        buf[len++] = (byte)(v >>> 16);
        buf[len++] = (byte)(v >>>  8);
        buf[len++] = (byte)(v >>>  0);

        return this;
        
    }

    final public KeyBuilder appendUnsigned(final short v) {

        // performance tweak
        if (len + 2 > buf.length) ensureCapacity(len+2);
        // ensureFree(1);

        // big-endian
        buf[len++] = (byte)(v >>>  8);
        buf[len++] = (byte)(v >>>  0);
        
        return this;
        
    }
