nciOncology.owl, embedded federation.

INFO : 31844   Main Thread com.bigdata.rdf.rio.BasicRioLoader.loadRdf(BasicRioLoader.java:194): parse complete: elapsed=28187ms, toldTriples=464841, tps=16491

INFO : 882875   Main Thread com.bigdata.rdf.store.DataLoader.loadData(DataLoader.java:517): Loaded 1 resources: 464841 stmts added in 28.265 secs, rate= 528, commitLatency=0ms
rule    	ms	#entms	entms/ms
RuleFastClosure13	15	0	0
RuleOwlEquivalentProperty	16	0	0
RuleRdfs02	5890	395806	67
RuleRdfs03	3297	395806	120
RuleRdfs08	31	41618	1342
RuleRdfs09	12109	41724	3
RuleRdfs10	110	41618	378
RuleRdfs11	720126	5324314	7
totals: elapsed=741594, nadded=376849, numComputed=6241034, added/sec=508, computed/sec=8415

Note: this appears to be incremental TM rather than database at once closure.

========================================

Modified to use database at once closure.

nciOncology.owl, embedded federation.

INFO : 36188   Main Thread com.bigdata.rdf.store.DataLoader.loadData2(DataLoader.java:628): 464841 stmts added in 32.109 secs, rate= 14476, commitLatency=0ms

rule    	ms	#entms	entms/ms
RuleOwlEquivalentProperty	157	0	0
RuleRdf01	110	43	0
RuleRdfs02	8859	395958	44
RuleRdfs03	7125	395958	55
RuleRdfs08	250	41631	166
RuleRdfs09	11406	41759	3
RuleRdfs10	219	41631	190
RuleRdfs11	240719	3951672	16
totals: elapsed=268845, nadded=3951672, numComputed=4868778, added/sec=14698, computed/sec=18109

Computed closure in 301500ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1249


============================================================


nciOncology.owl, no closure.

ids: #entries(est)=289871
SPO: #entries(est)=464993
POS: #entries(est)=464993
OSP: #entries(est)=464993
just: #entries(est)=0

!!!Note: be careful to choose the line that reports after the commit on the store!!!

local, unisolated:

run 1: Loaded 1 resources: 464841 stmts added in 23.656 secs, rate= 19650, commitLatency=172ms
run 2: Loaded 1 resources: 464841 stmts added in 24.094 secs, rate= 19292, commitLatency=156ms
run 3: Loaded 1 resources: 464841 stmts added in 24.328 secs, rate= 19107, commitLatency=235ms (after refactor for procedures)
(Computed closure in 141047ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=2671)

local, isolated:

run 1: Loaded 1 resources: 464841 stmts added in 26.735 secs, rate= 17386, commitLatency=438ms
run 2: Loaded 1 resources: 464841 stmts added in 25.719 secs, rate= 18073, commitLatency=297ms

embedded data service:
run 1: Loaded 1 resources: 464841 stmts added in 27.532 secs, rate= 16883, commitLatency=0ms (SPOArrayIterator)
(Computed closure in 375953ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1002)
run 2: Loaded 1 resources: 464841 stmts added in 27.016 secs, rate= 17206, commitLatency=0ms (SPOIterator - no mem cap)
(Computed closure in 482453ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=781)
ren 3: Loaded 1 resources: 464841 stmts added in 27.485 secs, rate= 16912, commitLatency=0ms (SPOIterator - no mem cap)
(Computed closure in 436266ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=863)

embedded federation:

run 1: Loaded 1 resources: 464841 stmts added in 32.313 secs, rate= 14385, commitLatency=31ms

jini federation:

run 1: Loaded 1 resources: 464841 stmts added in 57.204 secs, rate= 8126, commitLatency=16ms
run 2: Loaded 1 resources: 464841 stmts added in 49.172 secs, rate= 9453, commitLatency=16ms

(done) Report more data about the scale-out indices, including the #of
partitions, where each partition is located, and the size on disk on
the partition (the btrees on the journal are conflated so the journal
space needs to be factored out but we can report the #of entries on
the journal and maybe even the bytes written on the journal by the
btree).  Call out the time spent on each index - we need better
counters to report that correctly, or even counters on the data
service.

The embedded federation has a substantial drop in performance when
compared to the local store using isolated indices (the data services
always use isolated indices so that is the point for comparison), but
the big drop is the jini federation - presumably that cost is entirely
attributable to the serialization overhead for RPCs.

Examine in more depth why the embedded federation is slower.  Try a
run on a larger data set and see if this is related to start up costs.

Thesaurus.owl: #terms=586945, #stmts=1,047,647

local, unisolated  : Loaded 1 resources: 1086012 stmts added in  59.609 secs, rate= 18218, commitLatency=312ms
                   : Loaded 1 resources: 1086012 stmts added in  57.765 secs, rate= 18800, commitLatency=328ms
                   : Loaded 1 resources: 1086012 stmts added in  58.313 secs, rate= 18623, commitLatency=312ms
		   : Loaded 1 resources: 1086012 stmts added in  58.687 secs, rate= 18505, commitLatency=312ms (keybuilder refactor)
local,   isolated  : Loaded 1 resources: 1086012 stmts added in  64.562 secs, rate= 16821, commitLatency=156ms
embedded federation: Loaded 1 resources: 1086012 stmts added in  76.969 secs, rate= 14109, commitLatency=31ms
                   : Loaded 1 resources: 1086012 stmts added in  76.938 secs, rate= 14115, commitLatency=16ms
jini federation    : Loaded 1 resources: 1086012 stmts added in 103.734 secs, rate= 10469, commitLatency=0ms
                   : Loaded 1 resources: 1086012 stmts added in 103.859 secs, rate= 10456, commitLatency=31ms

Results for a variety of serialization/compression approaches for the
various Procedures (IndexWriteProc, JustificationWriteProc, etc), but
NOT for serialization changes to the ResultSet (which is really only
used during inference).  In all cases these results are obtained for
the jini federation since that is the only case where we are forced to
serialize the data in a Procedure or a ResultSet for RPC.

NoCompression.  This serializes each key and value as a full length
byte[].

   Loaded 1 resources: 1086012 stmts added in 107.922 secs, rate= 10062, commitLatency=0ms
   Loaded 1 resources: 1086012 stmts added in 105.531 secs, rate= 10290, commitLatency=16ms

NoCompression, but writing on a DataOutputBuffer and then copying the
results to the output stream (see if this case improves if we reuse
the buffer for each request or using a thread-local variable):

   Loaded 1 resources: 1086012 stmts added in 149.484 secs, rate= 7265, commitLatency=16ms

BTreeCompression.  This uses prefix compression on the keys and simple
serialization of the values.

   Loaded 1 resources: 1086012 stmts added in 103.203 secs, rate= 10523, commitLatency=16ms

FastRDFCompression

   Loaded 1 resources: 1086012 stmts added in 102.109 secs, rate= 10635, commitLatency=16ms
   Loaded 1 resources: 1086012 stmts added in  99.75  secs, rate= 10887, commitLatency=16ms (NIO)
   Loaded 1 resources: 1086012 stmts added in  99.313 secs, rate= 10935, commitLatency=15ms (NIO)

The "FastRDF" approach is probably as good as I can make it for the
statement indices.  It performs only marginally better than the no
compression approach.

Perhaps the additional overhead is a mixture of:

 - de-serialization to support RPC;
 - the mechanisms of RPC (client, server, protocol, network)
 - the added burden on the heap

NIO for the RPC protocol appears to help a bit, but it runs out of
memory in the test suite (this shows up as an NPE in ByteBuffer).


Concurrent load rates:

Explore interaction of the group commit policy.  If we check point vs
commit vs do not wait around then how does that effect the
throughput!!!

Note: smaller buffer sizes (1000 statements) makes the total run much
slower.  Try this with more threads, but we will probably have to wait
on the group commit so that won't help with the current policy.

Note: larger buffer sizes will cap out since there is only so much
data in the LUBM files.

U10

embedded data service:

Finished: #loaded=189 files in 96015 ms, #stmts=1272577, rate=13253.0
(#threads=3, largestPoolSize=3, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 73797 ms, #stmts=1272577, rate=17244.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 85625 ms, #stmts=1272577, rate=14862.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 78750 ms, #stmts=1272577, rate=16159.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 73203 ms, #stmts=1272577, rate=17384.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 63172 ms, #stmts=1272577, rate=20144.0
(#threads=20, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=189, #ok=189, #err=0)

All done: #loaded=189 files in 59734 ms, #stmts=1272577, rate=21304.0
(#threads=20, class=LocalTripleStoreWithEmbeddedDataService,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=189,
#ok=189, #err=0)

embedded federation:

Finished: #loaded=189 files in 191828 ms, #stmts=1272577, rate=6633.0
(#threads=1, largestPoolSize=1, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 122343 ms, #stmts=1272577, rate=10401.0
(#threads=3, largestPoolSize=3, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 282140 ms, #stmts=1272577, rate=4510.0
(#threads=3, largestPoolSize=3, bufferCapacity=1000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 90860 ms, #stmts=1272577, rate=14005.0
(#threads=10, largestPoolSize=10, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 85735 ms, #stmts=1272577, rate=14843.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 88453 ms, #stmts=1272577, rate=14387.0
(#threads=20, largestPoolSize=20, bufferCapacity=20000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 87359 ms, #stmts=1272577, rate=14567.0
(#threads=30, largestPoolSize=30, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 105203 ms, #stmts=1272577, rate=12096.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

All done: #loaded=189 files in 106109 ms, #stmts=1272577, rate=11993.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

   disk: 1,230,029,630 {osp,spo,terms} + 51,870,457 {ids,pos}

Alternative index allocation: 

   Note: This case appears to be much more efficient in term and
   space, at least for the embedded federation:

   disk: 80,506,107 {terms,spo} + 90,515,091 {ids,pos,osp}

   All done: #loaded=189 files in 88016 ms, #stmts=1272577,
   rate=14458.0 (#threads=20, class=ScaleOutTripleStore,
   largestPoolSize=20, bufferCapacity=100000, autoFlush=false,
   #done=189, #ok=189, #err=0)

jini federation:

Finished: #loaded=189 files in 392078 ms, #stmts=1272578, rate=3245.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 371297 ms, #stmts=1272582, rate=3427.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 82328 ms, #stmts=1272577, rate=15457.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

    Note: This is an extremely odd result.  It was obtained by running
    immediately after the previous jini federation run.  Overall, jini
    seems very sensitive to initial conditions.  Perhaps this is
    related to memory limits on the laptop platform?  Often the jini
    run appears to be very nearly single threaded.

All done: #loaded=189 files in 241672 ms, #terms=314871,
#stmts=1272577, rate=5265.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=189,
#ok=189, #err=0)

server1: All done: #loaded=190 files in 74049 ms, #terms=314871,
#stmts=1272577, rate=17185.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=190,
#ok=189, #err=1)

server1: All done: #loaded=190 files in 76956 ms, #terms=314871,
#stmts=1272577, rate=16536.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=190,
#ok=189, #err=1)

   disk: 90,926,328 {terms,spo} + 90,926,328 {ids,pos,osp}

server1: All done: #loaded=2008 files in 739904 ms, #terms=3301736,
#stmts=13405383, rate=18117.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false,
#done=2008, #ok=2007, #err=1) (U100 is 13M triples)

   disk: 1,110,058,584 {terms,spo} + 1,071,640,537 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         140.096 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         112.644 s (pause 15.700 s)

server1: #loaded=20022 files in 11419382 ms, #terms=32885169,
#stmts=133573856, rate=11697.0 (#threads=20,
class=ScaleOutTripleStore, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=20022, #ok=20020, #err=2)

   disk: 14,110,887,061 {terms,spo} + 12,039,810,264 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         1319.038 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         693.036 s (pause 103.692 s)

server1: All done: #loaded=20022 files in 11633794 ms, #terms=32885169,
#stmts=133573856, rate=11481.0 (#threads=20,
class=ScaleOutTripleStore, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=20022, #ok=20020, #err=2)

   disk: 14,093,839,353 {terms,spo} + 12,038,914,891 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         1279.318 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         666.388 s (pause 100.395 s)

*** Jini tuning:

    - Server runs:

      - Since jini is so fast on the server, try to get asynchronous
        writes to disk working in DiskOnlyStrategy - it might have a
        big impact since we loose all concurrency when a write to disk
        occurs.

      - Try U10000 reading the data from NAS with 2 clients, 10
        threads each and 2 servers.  See if scale-out holds as we
        increase the data size.  The point of comparison is the 1B run
        that we did on server2 (single host, non-scale-out
        architecture, non-concurrent load).

	- I am not seeing the 2nd data service on the current U10000
          run.  That is super weird. 

        - Make sure that yum-updatesd does not run on the servers.  It
          absorbs an entire CPU for quite a while.

        - Try running a thread that reads the file names and building
          up a (blocking) queue of tasks to be started.  Reading all
          the filenames and creating LoadTasks for each requires too
          much startup time and too much heap!

	- Make sure that each process writes on its own nohup_xxx.out
          file.  Hum - there is no way to do that.  Try creating a
          command group using (...) and running that group with nohup
          - probably won't work either.  How about run each in its own
          subdirectory?  Could work, but need to fiddle with the jini
          config and CLASSPATH.

      - Try with multiple index partitions, ideally dynamically
        determined!

      - Run single client with the metadata service (its lightly
        loaded) and one data service on one server and the other data
        service on another server.

	- Move the client and  metadata server to server3, running the
          data services on server1 and server2.

        - Have a client on each machine connect to the same federation
          (using hash(filename) MOD 2) to select the files to be
          loaded (distributed clients doing a concurrent batch load).
          The source data files will have to reside on NAS or a NSF
          mount or be pre-allocated to the different servers.

      - try TestTripleStoreLoadRateLocalConcurrent on the server to
        get a sense of the performance comparison between jini and an
        embedded data service when both use concurrent data load.

      - we do not appear to be memory capped on the server on U10.  On
        U100 we are using 60% of the RAM on the server (2.2G).

      - examine performance logs to see IO, CPU, etc. rates over time.

      - try larger loads (U100, U1000)

    - Tune indices

      - The ids index should benefit from value compression since the
        values are the serialized terms.  This will require custom
        code to break the values into symbols and then use huffman
        encoding.  Alternatively, simply treat each value as a symbol
        and code from that (assuming that value reuse is common - if
        not then at least URIs can be broken down into common
        symbols).

      - The terms (term:id) index is on the order of 5x larger than
        the ids (id:term) index.  Presumably this is because updates
        are distributed more or less randomly across the terms index
        as new terms become defined but are strictly append only for
        the ids index since new ids are always larger than old ids.
	
         - A larger branching factor may benefit the ids index.

	 - A compacting merge of the terms index should greatly reduce
           its size.

	 - Nearly ALL _read_ time between the SPO and TERMS index is
           reading the TERMS index (99%).

	 - Nearly ALL _write_ time between the SPO and the TERMS index
           is writing the SPO index (99%).  Statements are more likely
           to be distinct than terms, so it makes sense that we write
           on the statement index more often.  However, note that this
           is true even though the TERMS index is 3x larger than the
           SPO index.

    - BTree

      - bug sometimes demonstrated by com.bigdata.service.StressTestConcurrent_stressTest1.  The parent
        of the split is being marked as "clean" when the code assumes that it should be dirty. This sort
        of thing tends to involve touches driving evictions resulting in a node asynchronously being made
        persistent (and hence not dirty).  Verify that there are no concurrent readers executing by mistake
        against the live index (e.g., read_committed reads must not read on the live index). This could be
        related to the change in the group commit policy as well since that changes when we record a commit
        point for an index.
      
		Caused by: java.lang.AssertionError
			at com.bigdata.btree.Node.insertChild(Node.java:1515)
			at com.bigdata.btree.Leaf.split(Leaf.java:665)
			at com.bigdata.btree.Leaf.insert(Leaf.java:472)
			at com.bigdata.btree.Node.insert(Node.java:637)
			at com.bigdata.btree.AbstractBTree.insert(AbstractBTree.java:995)
			at com.bigdata.btree.AbstractBTree.insert(AbstractBTree.java:940)
			at com.bigdata.btree.AbstractBTree.rangeCopy(AbstractBTree.java:1477)
			at com.bigdata.resources.SplitIndexPartitionTask$UpdateSplitIndexPartition.doTask(SplitIndexPartitionTask.java:534)
			at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.call(AbstractTask.java:1035)
			... 9 more
			
	 - done. Going to work converting values to byte[]s throughout the
	   api, exposing versionCounter and deletion marker metadata
	   via the ITuple and IEntryIterator interface and moving
	   their data into the leaf, copying data into/out of leafs
	   (rather than by reference), fused views that process
	   version counters (never implemented before and required for
	   compacting merges of isolatable indices), and better
	   alignment of the various serialization apis.  This will
	   also set the stage for a compacting btree node or leaf data
	   structure which should help with long-term GC problems.

	 - done. Update UnisolatedBTree, IsolatedBTree.

	 - done. Update the fused views for these classes.  There is a
           single unifying model: the IsolatedBTree should be a fused
           view of the read-only historical state of an index
           partition and the btree on which the write set is isolated.
           Right now it is stated just in terms of the UnisolatedBTree
           (the historical state of the index as of the transaction
           start) and is not generalized to an index partition view.
           The rangeIterator for the fused view will examine the
           version metadata.

		   I can keep the simple btree api and extend the semantics on
		   the abstract btree, node and leaf directly and then roll
		   them into the rest of the system incrementally.
	
		   This will also make it easy to convert the values stored
		   under the keys to byte[]s since they are no longer going to
		   be wrapped by Value objects.
	
		   That will make it easier to align the serialization?

     - Support copy in/out of keys and vals in lookup(),
           insert(), remove(), and rangeIterator so that we can (a) be
           more efficient in handling keys and vals by copying; (b)
           handle keys and vals that are byte aligned or bit aligned
           in the node or leaf; (c) reduce GC by converting to a
           compacting record for the node/leaf; and (d) expose the
           version counter and deletion marker for fused views of
           indices with isolation.

	 - done. change version counters to long (commit timestamps).

    - Tune serialization:

	 - Test use of FastRDFValueCompression to write the values for
	   the statement indices.

	 - Look at how to do prefix coding using the mg4j and fast
           classes.  That could be used as the default key
           serialization and only overridden in special cases like RDF
           where we can do even better with less effort.
           
	   (old summary, revisit now since significant code changes): the "fast" rdf serializers definately write (and
	   read) less data resulting in a smaller store and less IO.
	   However, they are also without a doubt SLOWER.

	   It is hard to tell to what extend the throughput drop is
	   due to the work required to compute the compressed codes
	   and to what extent it is due to the additional allocation
	   and copying that we need to do to align the IDataSerializer
	   API with the IKeySerializer API, with the IValueSerializer
	   API, and with the use of the ImmutableKeyBuffer class in
	   the BTree.

	   Those API misalignments need to be reduced before we can
	   figure out whether the "fast" rdf serialization is faster
	   overall.  (There is SOME evidence that the API misalignment
	   is causing problems since only using the "fast" value
	   serializer is slower than using the byte[] value serializer
	   and there is very little "computation" to be done -
	   especially when compared to the "fast" key serializer which
	   computes a code for each long term identifier used in the
	   leaf.)

	   Also, note that most of the time is on the SPO and JUST
	   indices during closure (I would expect that) and on the
	   statement indices (rather than the lexicon) during load
	   (which is NOT what I would have expected for load).

	   I should also try some other data sets, with more memory
	   free on the machine, and on a server platform with more
	   CPUs and memory.

	   nciOncology:

	   fast 1 : Loaded 1 resources: 464841 stmts added in 31.922 secs, rate= 14561, commitLatency=203ms; bytesWritten=?
	   fast 1 : Computed closure in 289703ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1300; bytesWritten=?

	   fast 2 : Loaded 1 resources: 464841 stmts added in 26.094 secs, rate= 17814, commitLatency=172ms, bytesWritten=95,703,361, bytesRead=42,603,290
	   fast 2 : Computed closure in 135313ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=2785, bytesWritten=581,338,696, bytesRead=742,823,674

	   fast 3 : Loaded 1 resources: 464841 stmts added in 26.265 secs, rate= 17698, commitLatency=172ms
	   fast 3 : Computed closure in 136875ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=2753

	   kbs  1 : Loaded 1 resources: 464841 stmts added in 28.828 secs, rate= 16124, commitLatency=250ms, bytesWritten=109,833,180
	   kbs  1 : Computed closure in 124860ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=3018; bytesWritten=??? (not reported)

	   kbs  2 : Loaded 1 resources: 464841 stmts added in 23.594 secs, rate= 19701, commitLatency=250ms, bytesWritten=109,829,813, bytesRead=44,865,353
	   kbs  2 : Computed closure in 123500ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=3051, bytesWritten=653,911,378, bytesRead=862,234,149


Loaded 1 resources: 464841 stmts added in 25.218 secs, rate= 18432, commitLatency=391ms
Computed closure in 125296ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=3007

    - Distributed file repository

         - handle overflow of blocks to the index segments during MOVE

	 - provide streaming socket api on data service for reading
           blocks (low level in the DiskOnlyStrategy - if in the write cache
           then return directly else return buffered input stream reading on
           the disk file and interrupt if journal is closed).

	 - range delete

	 - logical row scan for headers of documents in a key range.

    - Map/Reduce demo jobs.

      - Download, prepare, extract.

    - Full text indexing

    - Quad store.

    - Metadata index
    
      - Merge (evict view onto index segment with history policy IFF
        sparse row store, since that is the only data model with the
        necessary timestamps on the data - version counters are NOT
        sufficient for this purpose).

      - Overflow (generate index segment from btree and update view defn).

      - Snap (generate new view by creating a new btree and making the
        old one "immutable").

      - Split (generate N partitions from one partition)

      - Join (generate 1 partition from 2 partitions).

      - Move (move partition to another data service).

    - Tune network IO

      - AddTerms and AddIds

      - Have the keys and values converted to a compressed byte[]
        before serialization (or deserialize the a compressed byte[]).

      - Modify the procedure logic to abstract a 'next key/val'
        iterator using a shared buffer for de-compression in order to
        minimize heap churn on the data server.

      - huffman encoding is appropriate for network IO, but hu-tucker
        is not required since we have to decompress keys to get them
        inserted into the btree.

      - tokenization needs to be specified for RDF Value types for the
        purposes of compression.  In fact, we are guarenteed that
        values a NOT duplicated in a given batch so tokenization needs
        to uncover common symbols.  This is easy for URIs but less so
        for literals and impossible for BNodes (which do not really
        need to be in the lexicon anyway).

      - Make sure that ResultSet, batch operations, and remote
        procedures all benefit from these tuning steps.  These should
        all be implemented as procedures (at least the batch btree api
        and submit) for simplicity of the API and the serialization
        support mechanisms.

    - Try jini federation using only the terms index to assign
      consistent term identifiers, bulk loading into local SPO-only
      indices, and then range partitioning the indices into global
      SPO, POS, and OSP orders and bulk loading the global statement
      indices.  The data loader should be concurrent and a filter
      should be applied such that each "host" loads only the files
      that hash MOD N to that host.  (note that only AddTerms and
      AddIds go across the network API in this case.)

    - Make it easy to summarize data service or index partition
      properties (report named properties by data service and/or index
      partition and aggregate).  Probably an XML syntax or a
      generalized property set model will benefit in the long term for
      telemetry.

    - The temp triple store supports concurrent read only, so it is
      not appropriate for a concurrent bulk loader.

    Other tuning and feature completeness

    - Inference has to be snell across the data service api.  So does
      high-level query.  At least inference is slow due to a large #of
      small join results.  Parallel sub-query is probably the way to
      beat that.  After tuning, compare to the purely local
      unconcurrent line.

      - Consider batching a set of rangeQueries together in a single
        operation vs parallel submits.  

    - quad store

    - 3+1 or 4+1 data models.  It IS possible to do statement
      identifiers using bnodes on a quad store, but we lose the
      opportunity to use the context position for virtual graph
      partition (often used to group one or more data sources
      together), which is also a key feature for federation

    - Sesame 2 TCL (integration tests)

**  Maven 2.x build and subversion for bigdata.


Short term tasks:

   - Compute average response time and throughput as 1/average
     response time. and measure on the client as well so that we can
     compute the RMI overhead,
   
   - full text indexing for kb

   - SemTech08 submission?

   - close out index resources (and journals) in resource manager so
     that we do not always grow the memory demand.

   - test counter reporting under linux.

   - stress testing with index partition moves, not just split/join.

   - correctness testing for scale-out.

   - streaming io for block read/write.  note that asynch IO for the
     disk only strategy will not have an impact if we are doing
     commits whose size fits within a single write cache (10M by
     default).

   - write performance test drivers and run on cluster.

      - rdf concurrent data load.

      - rdf concurrent query (rdf lubm is not designed to test with
        concurrent loads).

      - bigdata file system workload

   - ClassCastException during BuildIndexSegmentTask$AtomicUpdate.
     This was being thrown due to a pre-condition assumptions.  The
     assumption was that the source view would include at least two
     sources - the B+Tree on the new journal and at least a B+Tree on
     the old journal as well.  I am going to assume for now that there
     are valid pre-conditions where those assumptions do not hold and
     rely on a ground truth correctness suite to validate the overflow
     processing.

============================================================

KeyAfterPartition problem:

DEBUG: 40673  0cfe918c-8677-4878-917e-3a8610e15725   pool-1-thread-248 com.bigdata.btree.BatchInsert.apply(BatchInsert.java:180): keys: data(n=10)={
data[0]=[128, 0, 5, 127],
data[1]=[128, 0, 5, 128],
data[2]=[128, 0, 5, 217],
data[3]=[128, 0, 6, 40],
data[4]=[128, 0, 6, 112],
data[5]=[128, 0, 6, 150],
data[6]=[128, 0, 6, 231],
data[7]=[128, 0, 6, 254],
data[8]=[128, 0, 7, 35],
data[9]=[128, 0, 7, 62]}

============================================================

@7:42PM Sunday - for a lot of different negative array indices.
WARN : 40000  4293f5be-63ea-42a8-b4be-31a5026b122f   pool-1-thread-12 com.bigdata.counters.CounterSet$MyHandler.endElement(CounterSet.java:1197): Could not set counter value: path=/dpp2-wrkst13.dpp2.org/service/32c7601d-79d3-4a8e-b3b3-2a29e9174ef8/Concurrency Manager/Tasks/Unisolated, name=Average Task Latency : java.lang.ArrayIndexOutOfBoundsException: -4
java.lang.ArrayIndexOutOfBoundsException: -4
	at com.bigdata.counters.History.getAverage(History.java:383)
	at com.bigdata.counters.History.getAverage(History.java:329)
	at com.bigdata.counters.History.add(History.java:582)
	at com.bigdata.counters.HistoryInstrument.add(HistoryInstrument.java:111)
	at com.bigdata.counters.HistoryInstrument.setValue(HistoryInstrument.java:117)
	at com.bigdata.counters.Counter.setValue(Counter.java:128)
	at com.bigdata.counters.CounterSet$MyHandler.endElement(CounterSet.java:1188)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.endElement(AbstractSAXParser.java:633)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanEndElement(XMLNSDocumentScannerImpl.java:719)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(XMLDocumentFragmentScannerImpl.java:1685)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:368)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:834)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:764)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:148)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1242)
	at javax.xml.parsers.SAXParser.parse(SAXParser.java:375)
	at javax.xml.parsers.SAXParser.parse(SAXParser.java:176)
	at com.bigdata.counters.CounterSet.readXML(CounterSet.java:1002)
	at com.bigdata.service.LoadBalancerService.notify(LoadBalancerService.java:1244)
	at com.bigdata.service.DataService$ReportTask.reportPerformanceCounters(DataService.java:959)
	at com.bigdata.service.DataService$ReportTask.run(DataService.java:906)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:417)


============================================================

- ERROR: 39296     pool-1-thread-142 com.bigdata.resources.PostProcessOldJournalTask.call(PostProcessOldJournalTask.java:1436): java.lang.AssertionError: ndone=8, but #used=9
java.lang.AssertionError: ndone=8, but #used=9
	at com.bigdata.resources.PostProcessOldJournalTask.chooseTasks(PostProcessOldJournalTask.java:1299)
	at com.bigdata.resources.PostProcessOldJournalTask.call(PostProcessOldJournalTask.java:1344)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
	at java.lang.Thread.run()V(Unknown Source)
