nciOncology.owl, embedded federation.

INFO : 31844   Main Thread com.bigdata.rdf.rio.BasicRioLoader.loadRdf(BasicRioLoader.java:194): parse complete: elapsed=28187ms, toldTriples=464841, tps=16491

INFO : 882875   Main Thread com.bigdata.rdf.store.DataLoader.loadData(DataLoader.java:517): Loaded 1 resources: 464841 stmts added in 28.265 secs, rate= 528, commitLatency=0ms
rule    	ms	#entms	entms/ms
RuleFastClosure13	15	0	0
RuleOwlEquivalentProperty	16	0	0
RuleRdfs02	5890	395806	67
RuleRdfs03	3297	395806	120
RuleRdfs08	31	41618	1342
RuleRdfs09	12109	41724	3
RuleRdfs10	110	41618	378
RuleRdfs11	720126	5324314	7
totals: elapsed=741594, nadded=376849, numComputed=6241034, added/sec=508, computed/sec=8415

Note: this appears to be incremental TM rather than database at once closure.

========================================

Modified to use database at once closure.

nciOncology.owl, embedded federation.

INFO : 36188   Main Thread com.bigdata.rdf.store.DataLoader.loadData2(DataLoader.java:628): 464841 stmts added in 32.109 secs, rate= 14476, commitLatency=0ms

rule    	ms	#entms	entms/ms
RuleOwlEquivalentProperty	157	0	0
RuleRdf01	110	43	0
RuleRdfs02	8859	395958	44
RuleRdfs03	7125	395958	55
RuleRdfs08	250	41631	166
RuleRdfs09	11406	41759	3
RuleRdfs10	219	41631	190
RuleRdfs11	240719	3951672	16
totals: elapsed=268845, nadded=3951672, numComputed=4868778, added/sec=14698, computed/sec=18109

Computed closure in 301500ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1249


============================================================


nciOncology.owl, no closure.

ids: #entries(est)=289871
SPO: #entries(est)=464993
POS: #entries(est)=464993
OSP: #entries(est)=464993
just: #entries(est)=0

!!!Note: be careful to choose the line that reports after the commit on the store!!!

local, unisolated:

run 1: Loaded 1 resources: 464841 stmts added in 23.656 secs, rate= 19650, commitLatency=172ms
run 2: Loaded 1 resources: 464841 stmts added in 24.094 secs, rate= 19292, commitLatency=156ms
run 3: Loaded 1 resources: 464841 stmts added in 24.328 secs, rate= 19107, commitLatency=235ms (after refactor for procedures)
(Computed closure in 141047ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=2671)

local, isolated:

run 1: Loaded 1 resources: 464841 stmts added in 26.735 secs, rate= 17386, commitLatency=438ms
run 2: Loaded 1 resources: 464841 stmts added in 25.719 secs, rate= 18073, commitLatency=297ms

embedded data service:
run 1: Loaded 1 resources: 464841 stmts added in 27.532 secs, rate= 16883, commitLatency=0ms (SPOArrayIterator)
(Computed closure in 375953ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1002)
run 2: Loaded 1 resources: 464841 stmts added in 27.016 secs, rate= 17206, commitLatency=0ms (SPOIterator - no mem cap)
(Computed closure in 482453ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=781)
ren 3: Loaded 1 resources: 464841 stmts added in 27.485 secs, rate= 16912, commitLatency=0ms (SPOIterator - no mem cap)
(Computed closure in 436266ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=863)

embedded federation:

run 1: Loaded 1 resources: 464841 stmts added in 32.313 secs, rate= 14385, commitLatency=31ms

jini federation:

run 1: Loaded 1 resources: 464841 stmts added in 57.204 secs, rate= 8126, commitLatency=16ms
run 2: Loaded 1 resources: 464841 stmts added in 49.172 secs, rate= 9453, commitLatency=16ms

(done) Report more data about the scale-out indices, including the #of
partitions, where each partition is located, and the size on disk on
the partition (the btrees on the journal are conflated so the journal
space needs to be factored out but we can report the #of entries on
the journal and maybe even the bytes written on the journal by the
btree).  Call out the time spent on each index - we need better
counters to report that correctly, or even counters on the data
service.

The embedded federation has a substantial drop in performance when
compared to the local store using isolated indices (the data services
always use isolated indices so that is the point for comparison), but
the big drop is the jini federation - presumably that cost is entirely
attributable to the serialization overhead for RPCs.

Examine in more depth why the embedded federation is slower.  Try a
run on a larger data set and see if this is related to start up costs.

Thesaurus.owl: #terms=586945, #stmts=1,047,647

local, unisolated  : Loaded 1 resources: 1086012 stmts added in  59.609 secs, rate= 18218, commitLatency=312ms
                   : Loaded 1 resources: 1086012 stmts added in  57.765 secs, rate= 18800, commitLatency=328ms
                   : Loaded 1 resources: 1086012 stmts added in  58.313 secs, rate= 18623, commitLatency=312ms
		   : Loaded 1 resources: 1086012 stmts added in  58.687 secs, rate= 18505, commitLatency=312ms (keybuilder refactor)
local,   isolated  : Loaded 1 resources: 1086012 stmts added in  64.562 secs, rate= 16821, commitLatency=156ms
embedded federation: Loaded 1 resources: 1086012 stmts added in  76.969 secs, rate= 14109, commitLatency=31ms
                   : Loaded 1 resources: 1086012 stmts added in  76.938 secs, rate= 14115, commitLatency=16ms
jini federation    : Loaded 1 resources: 1086012 stmts added in 103.734 secs, rate= 10469, commitLatency=0ms
                   : Loaded 1 resources: 1086012 stmts added in 103.859 secs, rate= 10456, commitLatency=31ms

Results for a variety of serialization/compression approaches for the
various Procedures (IndexWriteProc, JustificationWriteProc, etc), but
NOT for serialization changes to the ResultSet (which is really only
used during inference).  In all cases these results are obtained for
the jini federation since that is the only case where we are forced to
serialize the data in a Procedure or a ResultSet for RPC.

NoCompression.  This serializes each key and value as a full length
byte[].

   Loaded 1 resources: 1086012 stmts added in 107.922 secs, rate= 10062, commitLatency=0ms
   Loaded 1 resources: 1086012 stmts added in 105.531 secs, rate= 10290, commitLatency=16ms

NoCompression, but writing on a DataOutputBuffer and then copying the
results to the output stream (see if this case improves if we reuse
the buffer for each request or using a thread-local variable):

   Loaded 1 resources: 1086012 stmts added in 149.484 secs, rate= 7265, commitLatency=16ms

BTreeCompression.  This uses prefix compression on the keys and simple
serialization of the values.

   Loaded 1 resources: 1086012 stmts added in 103.203 secs, rate= 10523, commitLatency=16ms

FastRDFCompression

   Loaded 1 resources: 1086012 stmts added in 102.109 secs, rate= 10635, commitLatency=16ms
   Loaded 1 resources: 1086012 stmts added in  99.75  secs, rate= 10887, commitLatency=16ms (NIO)
   Loaded 1 resources: 1086012 stmts added in  99.313 secs, rate= 10935, commitLatency=15ms (NIO)

The "FastRDF" approach is probably as good as I can make it for the
statement indices.  It performs only marginally better than the no
compression approach.

Perhaps the additional overhead is a mixture of:

 - de-serialization to support RPC;
 - the mechanisms of RPC (client, server, protocol, network)
 - the added burden on the heap

NIO for the RPC protocol appears to help a bit, but it runs out of
memory in the test suite (this shows up as an NPE in ByteBuffer).


Concurrent load rates:

Explore interaction of the group commit policy.  If we check point vs
commit vs do not wait around then how does that effect the
throughput!!!

Note: smaller buffer sizes (1000 statements) makes the total run much
slower.  Try this with more threads, but we will probably have to wait
on the group commit so that won't help with the current policy.

Note: larger buffer sizes will cap out since there is only so much
data in the LUBM files.

U10

embedded data service:

Finished: #loaded=189 files in 96015 ms, #stmts=1272577, rate=13253.0
(#threads=3, largestPoolSize=3, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 73797 ms, #stmts=1272577, rate=17244.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 85625 ms, #stmts=1272577, rate=14862.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 78750 ms, #stmts=1272577, rate=16159.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 73203 ms, #stmts=1272577, rate=17384.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 63172 ms, #stmts=1272577, rate=20144.0
(#threads=20, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=189, #ok=189, #err=0)

All done: #loaded=189 files in 59734 ms, #stmts=1272577, rate=21304.0
(#threads=20, class=LocalTripleStoreWithEmbeddedDataService,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=189,
#ok=189, #err=0)

embedded federation:

Finished: #loaded=189 files in 191828 ms, #stmts=1272577, rate=6633.0
(#threads=1, largestPoolSize=1, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 122343 ms, #stmts=1272577, rate=10401.0
(#threads=3, largestPoolSize=3, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 282140 ms, #stmts=1272577, rate=4510.0
(#threads=3, largestPoolSize=3, bufferCapacity=1000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 90860 ms, #stmts=1272577, rate=14005.0
(#threads=10, largestPoolSize=10, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 85735 ms, #stmts=1272577, rate=14843.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 88453 ms, #stmts=1272577, rate=14387.0
(#threads=20, largestPoolSize=20, bufferCapacity=20000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 87359 ms, #stmts=1272577, rate=14567.0
(#threads=30, largestPoolSize=30, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 105203 ms, #stmts=1272577, rate=12096.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

All done: #loaded=189 files in 106109 ms, #stmts=1272577, rate=11993.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

   disk: 1,230,029,630 {osp,spo,terms} + 51,870,457 {ids,pos}

Alternative index allocation: 

   Note: This case appears to be much more efficient in term and
   space, at least for the embedded federation:

   disk: 80,506,107 {terms,spo} + 90,515,091 {ids,pos,osp}

   All done: #loaded=189 files in 88016 ms, #stmts=1272577,
   rate=14458.0 (#threads=20, class=ScaleOutTripleStore,
   largestPoolSize=20, bufferCapacity=100000, autoFlush=false,
   #done=189, #ok=189, #err=0)

jini federation:

Finished: #loaded=189 files in 392078 ms, #stmts=1272578, rate=3245.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 371297 ms, #stmts=1272582, rate=3427.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 82328 ms, #stmts=1272577, rate=15457.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

    Note: This is an extremely odd result.  It was obtained by running
    immediately after the previous jini federation run.  Overall, jini
    seems very sensitive to initial conditions.  Perhaps this is
    related to memory limits on the laptop platform?  Often the jini
    run appears to be very nearly single threaded.

All done: #loaded=189 files in 241672 ms, #terms=314871,
#stmts=1272577, rate=5265.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=189,
#ok=189, #err=0)

server1: All done: #loaded=190 files in 74049 ms, #terms=314871,
#stmts=1272577, rate=17185.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=190,
#ok=189, #err=1)

server1: All done: #loaded=190 files in 76956 ms, #terms=314871,
#stmts=1272577, rate=16536.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=190,
#ok=189, #err=1)

   disk: 90,926,328 {terms,spo} + 90,926,328 {ids,pos,osp}

server1: All done: #loaded=2008 files in 739904 ms, #terms=3301736,
#stmts=13405383, rate=18117.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false,
#done=2008, #ok=2007, #err=1) (U100 is 13M triples)

   disk: 1,110,058,584 {terms,spo} + 1,071,640,537 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         140.096 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         112.644 s (pause 15.700 s)

server1: #loaded=20022 files in 11419382 ms, #terms=32885169,
#stmts=133573856, rate=11697.0 (#threads=20,
class=ScaleOutTripleStore, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=20022, #ok=20020, #err=2)

   disk: 14,110,887,061 {terms,spo} + 12,039,810,264 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         1319.038 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         693.036 s (pause 103.692 s)

server1: All done: #loaded=20022 files in 11633794 ms, #terms=32885169,
#stmts=133573856, rate=11481.0 (#threads=20,
class=ScaleOutTripleStore, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=20022, #ok=20020, #err=2)

   disk: 14,093,839,353 {terms,spo} + 12,038,914,891 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         1279.318 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         666.388 s (pause 100.395 s)

*** Jini tuning:

    - Server runs:

      - Since jini is so fast on the server, try to get asynchronous
        writes to disk working in DiskOnlyStrategy - it might have a
        big impact since we loose all concurrency when a write to disk
        occurs (however, disk writes will always be at group commits
        if the write cache is large enough so this might have NO
        impact).

      - Try U10000 reading the data from NAS with 2 clients, 10
        threads each and 2 servers.  See if scale-out holds as we
        increase the data size.  The point of comparison is the 1B run
        that we did on server2 (single host, non-scale-out
        architecture, non-concurrent load).

	- I am not seeing the 2nd data service on the current U10000
          run.  That is super weird. 

        - Make sure that yum-updatesd does not run on the servers.  It
          absorbs an entire CPU for quite a while.

	- Make sure that each process writes on its own nohup_xxx.out
          file.  Hum - there is no way to do that.  Try creating a
          command group using (...) and running that group with nohup
          - probably won't work either.  How about run each in its own
          subdirectory?  Could work, but need to fiddle with the jini
          config and CLASSPATH.

      - Try with dynamically determined index partitions.

        Start with either single or double host placement of the
        initial index partitions.

	Watch the load balancer and see how host utilization and
        service response time change as the run progresses, for
        different #of client threads, and as index splits occur, and
        as index moves occur.

	Delay start of some data services, either on each host or on
        one of the hosts and then see how the load changes once we
        start additional data services.

	Observe the metadata service response time and verify that it
        does not become a bottleneck since the current implementation
        is NOT caching.

      - Run single client with the metadata service (its lightly
        loaded) and one data service on one server and the other data
        service on another server.

	- Move the client and metadata server to server3, running the
          data services on server1 and server2.

        - Have a client on each machine connect to the same federation
          (using hash(filename) MOD 2) to select the files to be
          loaded (distributed clients doing a concurrent batch load).
          The source data files will have to reside on NAS or a NSF
          mount or be pre-allocated to the different servers.

      - try TestTripleStoreLoadRateLocalConcurrent on the server to
        get a sense of the performance comparison between jini and an
        embedded data service when both use concurrent data load.

      - we do not appear to be memory capped on the server on U10.  On
        U100 we are using 60% of the RAM on the server (2.2G).

      - examine performance logs to see IO, CPU, etc. rates over time.

      - try larger loads (U100, U1000)

    - Tune indices

      - The ids index should benefit from value compression since the
        values are the serialized terms.  This will require custom
        code to break the values into symbols and then use huffman
        encoding.  Alternatively, simply treat each value as a symbol
        and code from that (assuming that value reuse is common - if
        not then at least URIs can be broken down into common
        symbols).

	Done. Do not store bnodes in the id:term index.

      - The terms (term:id) index is on the order of 5x larger than
        the ids (id:term) index.  Presumably this is because updates
        are distributed more or less randomly across the terms index
        as new terms become defined but are strictly append only for
        the ids index since new ids are always larger than old ids.
	
         - A larger branching factor may benefit the ids index.

	 - A compacting merge of the terms index should greatly reduce
           its size.

	 - Nearly ALL _read_ time between the SPO and TERMS index is
           reading the TERMS index (99%).

	 - Nearly ALL _write_ time between the SPO and the TERMS index
           is writing the SPO index (99%).  Statements are more likely
           to be distinct than terms, so it makes sense that we write
           on the statement index more often.  However, note that this
           is true even though the TERMS index is 3x larger than the
           SPO index.

    - BTree

      - bug sometimes demonstrated by com.bigdata.service.StressTestConcurrent_stressTest1.  The parent
        of the split is being marked as "clean" when the code assumes that it should be dirty. This sort
        of thing tends to involve touches driving evictions resulting in a node asynchronously being made
        persistent (and hence not dirty).  Verify that there are no concurrent readers executing by mistake
        against the live index (e.g., read_committed reads must not read on the live index). This could be
        related to the change in the group commit policy as well since that changes when we record a commit
        point for an index.
      
		Caused by: java.lang.AssertionError
			at com.bigdata.btree.Node.insertChild(Node.java:1515)
			at com.bigdata.btree.Leaf.split(Leaf.java:665)
			at com.bigdata.btree.Leaf.insert(Leaf.java:472)
			at com.bigdata.btree.Node.insert(Node.java:637)
			at com.bigdata.btree.AbstractBTree.insert(AbstractBTree.java:995)
			at com.bigdata.btree.AbstractBTree.insert(AbstractBTree.java:940)
			at com.bigdata.btree.AbstractBTree.rangeCopy(AbstractBTree.java:1477)
			at com.bigdata.resources.SplitIndexPartitionTask$UpdateSplitIndexPartition.doTask(SplitIndexPartitionTask.java:534)
			at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.call(AbstractTask.java:1035)
			... 9 more
			
     - Support copy in/out of keys and vals in lookup(), insert(),
       remove(), and rangeIterator so that we can (a) be more
       efficient in handling keys and vals by copying; (b) handle keys
       and vals that are byte aligned or bit aligned in the node or
       leaf; (c) reduce GC by converting to a compacting record for
       the node/leaf; and (d) expose the version counter and deletion
       marker for fused views of indices with isolation.

     - Turn off sendVals for rangeIterators if we recognize the value
       serialized as the NoDataSerializer?

     - Change checksums to be at the store/record level.  Interpret
       the record length as having 2 additional bytes for read/write
       of the checksum.  Put it at the end of the record.
       Enable/disable at the store level.

       Add an option for read-back validation of writes?
       
       Add an option for a fully synchronized raw store interface on
       the Journal?

    - Distributed file repository

         - handle overflow of blocks to the index segments during MOVE

	 - provide streaming socket api on data service for reading
           blocks (low level in the DiskOnlyStrategy - if in the write cache
           then return directly else return buffered input stream reading on
           the disk file and interrupt if journal is closed).

	 - range delete

	 - logical row scan for headers of documents in a key range.

    - Map/Reduce demo jobs.

      - Download, prepare, extract.

    - Tune network IO

      - Modify the procedure logic to abstract a 'next key/val'
        iterator using a shared buffer for de-compression in order to
        minimize heap churn on the data server.

      - huffman encoding is appropriate for network IO, but hu-tucker
        is not required since we have to decompress keys to get them
        inserted into the btree.

      - tokenization needs to be specified for RDF Value types for the
        purposes of compression.  In fact, we are guarenteed that
        values are NOT duplicated in a given batch so tokenization
        needs to uncover common symbols.  This is easy for URIs but
        less so for literals and impossible for BNodes (which do not
        really need to be in the lexicon anyway).

    - Try jini federation using only the terms index to assign
      consistent term identifiers, bulk loading into local SPO-only
      indices, and then range partitioning the indices into global
      SPO, POS, and OSP orders and bulk loading the global statement
      indices.  The data loader should be concurrent and a filter
      should be applied such that each "host" loads only the files
      that hash MOD N to that host.  (note that only AddTerms and
      AddIds go across the network API in this case.)

    - The temp triple store supports concurrent read only but not
      concurrent write, so it is not appropriate for a concurrent bulk
      loader.

    - An extended transaction model can be used for truth maintenance.
      The focus store is built up within isolated indices (that do not
      actually correspond to persistent indices, they only exist on
      the per-tx per-dataservice TemporaryStore).  The application can
      simply combine sets of assertions or retractions within a single
      transaction.  Either the application or an extension of the
      transaction manager MUST serialize the commits.  Within the
      commit processing, first do retractions then do assertions.

      - Provide for transaction local indices.  The index is dropped
        when the tx completes.

      - Provide for registration of a global index within a
        transaction, but the transaction will fail if the index
        already exists when it commits.

Short term tasks:

   - (*) Builds and releases.
   
      - Change over to subversion so that the edit trail does not get
        lost (complex process).

      - Maven 2.x build
      
         - Start doing snapshot releases.

	 - Start periodic project documentation builds, perhaps on SF.
           Publish on the www.bigdata.com site.

         - Change the dependency to dsiutils.  I tried to do this with
           dsiutils-1.0.4 and ran into problems with
           (de-)serialization when compared to the lgpl-utils versions
           of the same classes.  Try this again and pay close
           attention to the lgpl-utils versions of the classes now
           located in dsiutils and see if I can isolated the problem.
           The problem was demonstrated by the bigdata-rdf test suites
           for both the temp and local triple stores but not for the
           bigdata test suites.

	 - Done. Update the Sesame 2.x dependency.

	 - Put all properties into the com.bigdata namespace.

   - Counters

     - Done. Work the counter path, name, and date(s) into the table
       which shows the counters history values so that it can all get
       copied easily into a worksheet.

     - Done. #commit is not being encoded property and shows up as a
       URL anchor and not as part of the PATH parameter.

     - Done. Counter XML MUST persist the HISTORY in the XML so that
       the log files can be useful for post-mortem.
       
     - Done. Write a final log file ('-final.xml') when the LBS
       terminates.

     - Done. This is now a configuration property.  The load balancer
       is not writing its counters into the correct location (logDir).
       The directory needs to be relative to the service directory, so
       a method needs to expose that directory to the service.

     - Done. (Not quite sure what the problem was here, but I made a
       few changes and it appears to be fixed.)  The concurrent data
       loader was failing to halt once it started the flush tasks.

     - Done.  (Modified to accept samples out of timestamp order and
       to record the #of and total of samples falling within a given
       period.)  Loosing some samples through reporting w/in the same
       period.  Round up to the next period if this period is filled.
       An alternative is to sum the samples in the period and report
       their average by also tracking the #of samples in the period!

     - Done. When writing the path in the table rows, only write the
       path from the selected root.

     - Done. Problem with double-decoding of URL in NanoHTTP.

     - Done. (Can be a bit odd when also using a regex filter.) Add
       depth query parameter to limit the #of levels resolved from the
       path.

     - Done. (Currently using engineering notation, should be query
       parameter).  Set to 6 digits precision, not {3,6} after the
       decimal.  Or right justify decimal value with fixed N digits
       after the decimal (could be query param).

     - Done. (Also added the timestamp itself.) When converting to
       minutes, hours, and days in httpd make sure to have a few
       digits after the decimal -- otherwise false boundaries.

     - Done. (uses wildcards before and after and ORs together.) The
       filter needs to accept regex characters or prefix and post-fix
       with ".*".  Since things are quoted, right now nothing is
       actually matched.

     - Done. Since the log files provide post-mortem, there should be
       a way to view the files through the same httpd tool - a mode
       where it reads a single named counter XML file and then lets
       you browse it.  This will make it easy to find interesting
       views.

     - Done. The IndexManager should report the #of index views open
       concurrently.  Either sample once per second and take a moving
       average track the total number and compute the instanteous
       average per minute.
 
     - Done (reports the #of stores in the weak value
       cache). Likewise, the StoreManager should report the #of open
       journals and index segments.

     - Done. Anything with "%" or "percent" in the name should be
       formatted as a percentage in [0.00:1.00].

     - Add UI elements to set the filter(s), depth, decimalFormat,
       etc.  These should be a FORM with a GET action.

     - The services should be translated (or annotated) by the
       hostname and service info.  This will make the information more
       interesting than just staring at UUIDs.

     - I should be able to bookmark interesting counter sets, but the
       service name needs to be there or the bookmarks will depend
       only on filters, such as all unisolated counters.  An xpath
       might be useful here so that the filter reads down to a service
       name or type and then back up to reveal the service.

     - May be loosing some samples by running multiple typeperf's at
       once.  Explore.  If true, then trying combining all w/in same
       JVM using reference counter for process or identifying one
       process in the JVM which will have responsibility for those
       counters.

     - Cache-Control (data are good up to 60 seconds after the
       counters were last updated) and auto-update of page/view?

     - Done.  The data service should report its configuration
       properties under "Info".

       This should be done for the other services as well.  Refactor
       the code in DataService, moving it into the counters package.

       Servers should add their Jini configuration information as
       well.  This probably has to be done explicitly for the
       configuration items of interest.

     - Add histograms of response time by task.  E.g., AddIds,
       AddTerms, IndexWriteProx, etc.

     - (**) Compute average response time and throughput as 1/average
       response time.  And measure on the client as well so that we
       can compute the RMI overhead.  Do this as a refactor of the
       task that takes samples from a queue.

     - Done. Add counters for #of index partition split, move, and
       join operations (OverflowManager).

       Done. Also report #of errors during asynchronous overflow
       processing.

       There should also be a counter of the #of index partitions
       moved onto a data service.  However there is no place in the
       code to easily note this on the target data service since the
       move is made atomic by an action on the metadata service.

     - Add a parameterized moving average computation to History with
       a default weight.  This could be written into the UI tables for
       easy of plotting.

     - Add counter to the write service that reports the #of tasks
       which have their locks and are actually doing their work
       concurrently (LockManager defines such a counter but we need
       its moving average not the instantaneous value).  This is the
       real concurrency of the tasks.  The #of active tasks in the
       write service is a red herring since those tasks could be
       waiting for their locks.

       Done, but report the averageQueueLength for both tasks holding
       their locks and for all active tasks in the pool since both are
       interesting data. For example, the averageQueueLength below is
       .25 but the activeTaskSetSize is 20.  For this example this was
       because there were two indices and many operations were forced
       to be sequential.  Even though the commit group size was large
       (committedTaskCount/groupCommitCount) =~ 20, the parallelism
       was poor (average of .25 tasks executing in parallel!)

       averageQueueLength=0.25841155522560005
       (activeCountAverage=0.217179869184,queueSizeAverage=0.04123168604160004),
       nsamples=20

       WriteExecutorService{ paused=false, nrunning=8,
       concurrentTaskCount=1, activeTaskSetSize=20, nwrites=12,
       groupCommitFlag=true, abortFlag=false,
       lockHeldByCurrentThread=false, lockHoldCount=0,
       lockQueueLength=0, activeCount=20, queueSize=0, poolSize=200,
       largestPoolSize=200, maxPoolSize=200, maxRunning=20,
       maxCommitLatency=187, maxLatencyUntilCommit=125,
       groupCommitCount=109, abortCount=0, failedTaskCount=0,
       successTaskCount=2152, committedTaskCount=2140,
       overflowCount=0}

     - (****) I think that nearly all of these issues a problems with
       the Sar, and to some extend the Pidstat, collectors.

       The saved history is filling up the hours and days with zero
       for some odd reason when you bring it up again under the
       post-mortem mode.  E.g., CPU/% IO Wait.  This only happens for
       the host-wide statistics reported by Sar.  The PidStat
       statistics and the service specific statistics are fine.
	    
       Sar is the worst - lost samples and bad timestamps.

       Parsing errors for sysstat utilities from time to time.  They
       are mostly for the SarCpuUtilization utility.  "grep -i
       exception nohup.out" will find them.

       I am seeing a "ignoring sample WAY out of timestamp order"
       warning.
	    
       PidStat appears to report Ok, but it does not aggregate to
       hours or days correctly.  Or maybe I lost the history when
       rolling over at midnight?  No.  It was a run from ~noon to
       ~6pm. Other counters have all of those hours.

     - Done. The per-process counters for linux are not being reported
       under "service" but instead directly under the service UUID.

     - The per-host physical disk counters for linux are not being
       reported at all. In fact, I need to write the Sar or vmstat
       utility to collect these data.

     - For some reason a query for the hostname on host3 does not
       return the counters in the browser.  Response is fast both
       beneath that level at at the root.  Maybe the problem is in the
       /host/CPU and /host/Info counter sets - those appear to hang
       while /host/service is fine.... that does not seem to pay out
       either.

       (**) turn up the log level for the server and service and see
       what's up with this.  I can direct the log to another file to
       make this easy to see.

     - Make it possible to have more than 60 minutes in the buffer but
       still overflow after 60 minutes onto the hours.  This will
       allow a longer reachback at a given level of aggregation.

     - syslogd integration so that I see ERROR and FATAL messages for
       the hosts in the federation.

   - LocalTripleStoreWithEmbeddedDataService

     - Benchmark with owl:sameAs backchainer.

     - (*****) Optimized JOIN that assumes that all indices are local
       within the data service and reads locally on both access paths.
       This would probably be implemented as a AbstractTask and it
       would need to declare access to the indices being used for the
       left and right hand sides of the join.

     - test small and large document sets with and without incremental
       closure:

       -server -Xmx500m -DtestClass=com.bigdata.rdf.store.TestLocalTripleStore -Ddocuments.directory=../rdf-data/metrics/smallDocuments -Ddocuments.ontology=../rdf-data/metrics/metricsOntology_v1.9.rdfs -Dfile=C:/smallDocuments.jnl -DdataLoader.commit=None -DdataLoader.closure=None

   - ScaleOutTripleStore

     - (*****) Optimized JOIN for the scale-out triple store.  It
       needs to block up a set of right hand tuples that will be
       joined against data on a given data service and then send those
       tuples to that data service, recieving the results in
       return. It will also have to handle stale locators if the join
       is running in an read-committed mode, but not if it is
       transactional or a historical read.

       Inference is slow due to a large #of small join results.
       Parallel sub-query is probably the way to beat that.  After
       tuning, compare to the purely local unconcurrent line.

       Consider batching a set of rangeQueries together in a single
       operation vs parallel submits.

     - The distinct term scan (really, the prefix scan) needs to be
       optimized for the data service and the federation use cases.
       This also effects the sparse row store.  Also, the sparse row
       store needs to use an extended split handler that always
       chooses a split point which is on a logical row boundary.

     - (*) Need ability to request a rangeIterator that reads in
       reverse key order and the ability to visit the prior or next
       key.  This requirement arises in particular for the bigdata
       repository which is currently using the ILinearList API for the
       AtomicAppend.  This will also help us to replace the
       requirement for the ILinearList API in the metadata index.
       
       The change needs to occur at several levels and should include
       a prior() method on ITupleIterator and the ability to acquire
       an ITupleIterator for ITuple returned from lookup(), insert(),
       etc. so that it can be turned into an iterator for prior/next visitation.

       This is a good time to do efficient prior/next leaf operations
       for the IndexSegment and to make that more efficient for the
       BTree as well.

     - Done. (get() and find() were running as unisolated tasks.)  I
       am seeing a lot of tasks in the concurrency manager for the
       metadata service, but few commits on the live journal.  What
       the heck is the being reported for the metadata service?

   - (*****) The consistentRead option used by the
     PartitionedIndexRangeIterator needs to use the most recent commit
     time for the federation.

     Currently it uses the lastCommitTime for the first index
     partition which it scans.  However, it is quite possible that
     there have been writes on other index partitions since which
     would not be reflected in BTree#lastCommitTime.  The
     consistentRead would therefore reflect an older history rather
     than the most recent writes when it moved onto the other index
     partitions.

     In order to fix this the data services MUST discover and use a
     centralized timeservice.  This can be essentially a stripped down
     centralized transaction manager.  The data services will obtain
     their commitTime timestamps from this centralized time service
     and MUST also notify the centralized timeservice once they have
     successfully committed.  The PartitionedRangeIterator will then
     query the centralized time service for the last commit time for
     the federation as a whole and use that as the time for a
     consistentRead operation.

     As an alternative, the data services could periodically publish
     their commit times.  I need to see how much of a bottleneck it is
     for clients calling nextTimestamp().

   - OverflowManager

      - Could optionally convert from  a fully-buffered to a disk-only
        store  in  order to  reduce  the  memory  footprint for  fully
        buffered  stores,  but in  that  case  this conversion  should
        happen once asynchronous overflow handling was complete.

   - StoreManager / IndexManager

      - Modify  LRU  to  purge  entries  older than  a  specified  age
        (including an asych  daemon thread to make sure  that they get
        purged even if the LRU is not being touched).  Do this for the
        index segment cache in the IndexManager as well.

      - (*) Better concurrency for openIndex, openStore, getJournal,
        and getIndexOnStore

      - Modify WeakValueCache to use ConcurrentHashMap and support an
        atomic putIfAbsent operation.  This will reduce the latency
        imposed when we need to re-open an index segment from a store.

      - (***) StoreManager#getEarliestDependencyTimestamp() is not
        implemented.  Old store files will not be released as a
        result unless you explicitly set the release time.

   - LockManager

       - Use a WeakValueCache to purge unused resources.  The size of
         its internal map from resource name to resource queue will
         grow without bound on a data service as index partitions are
         split and moved around.  There are notes on this issue in the
         LockManager class.

   - DiskOnlyStrategy
   
     - Lazy creation of the backing file.

     - (***) An LRU read cache for records.

       This could be a big win for the DiskOnlyStrategy.  Either make
       this its own layer that can be interposed between the journal
       and the DiskOnlyStrategy or add directly to the
       DiskOnlyStrategy since a read cache is not required for the
       fully buffered modes. Regardless, allow configuration of the
       cache size.

       Also, efficient nextLeaf could improve read performance by
       reducing node reads.

     - CounterSets

       - Add counters designed to give insight into whether the write
         cache tends to full up completely or only partly before the
         next group command and the #of bytes that tend to be written.
         What I want to understand is whether the cache is too large
         and whether an asynchronous of the cache to the disk would be
         a benefit.

	 Note that writes which would exceed the remaining cache size
         cause the existing cache to be flushed while writes that
         exceed the cache capacity are written directly to the disk -
         the cache itself is always dense in terms of the bytes
         written on the address space.

   - Done. Full text indexing for KB.

       - Done. Analyzers are not thread-safe.

       - Try out an mg4j integration for an alternative text indexer
         and search.

   - Consider thread pool size defaults, especially for the temporary
     stores such as the temp triple store.  What is a good policy?

   - *** Batch API for extractor, allowing runs directly against the
     KB.  Form a single prefix-scan query from a sort of all simple
     terms in the document and then piece together the phrases from
     the result.

   - ** Change tx timestamps to negative and use positive timestamps for
     historical reads.  Changes to AbstractTask, ITx,
     ITransactionManager, StoreFileManager, IsolationEnum, and the
     post-processing tasks.  This will greatly simplify thinking about
     historical read operations since they will simply use the actual
     commit time while transactions will use a free (-timestamp) value
     selected by the transaction manager.

   - Modify to have an observable event or callback that assigns the
     service UUID and that indicates when the resource manager is
     running and refactor the LDS and DS startup logic to use that to
     configure the reporting of counters and an optional httpd service
     (at least for the LDS). The relevant Jini method is
     ServiceIDNotify().  For the moment I have disabled the httpd for
     the LDS.

   - Consider dropping the BasicRioLoader, PresortRioLoader, etc.  All
     of the benefit is in the use of the StatementBuffer.  These
     loaders just obscure the RIO mechanism and make them harder to
     configure.

     The DataLoader might be a utility class.

     The ConcurrentDataLoader is certainly a useful utility class.

   - Quad store.

    - ** Sesame 2 TCK (integration tests)

         (temp fix) You should add the following URL as a maven
	 repository to your maven settings.xml file:

	    http://repo.aduna-software.org/maven2/releases/

    - ***** Two database modes: named-graph mode (quads with all 6
      indices) and provenance mode (3+1 where the context position
      holding a bnode with a 1:1 relationship to the triple and
      therefore serving as a statement identifier and is stored as the
      value associated to the triple in the index; The source
      extension for RDF/XML needs to be supported such that the given
      BNode or URI for the source is correlated to the use of that
      same Resource elsewhere in the same RDF/XML document - we need
      to extend RIO for this).

      1. 3+1

	 - Done. value serializer must be different when using sids

	 - Note: any partition of the term:id index may be used to
           assign term identifiers for bnodes since the bnode ID is
           only required to be distinct, but not stable.

	 - Done. TMStatementBuffer needs to recursively wipe out
           statements using a statement identifier.  this should be
           part of truth maintenance.  when we get the original set of
           explicit statements to be removed we collect their
           statement identifiers and then collect all statements using
           those statement identifiers in either the subject or object
           position and add them to the original set of statements to
           be removed.

	   Done. AccessPath#removeAll() we also need to collect the
	   set of statements using a statement identifier and delete
	   them as well.  this will wind up being a double test for
	   the original set of statements if TM is being used, but it
	   is required when TM is not in use.

	   Done. Do not generate statement identifiers if the SPO is
	   marked as an inference (an optimization).
	   
	   Done. Modify the procedure that actually writes on the
	   statement index to write a ZERO (0L) statement identifier
	   if the statement is (in fact at the time that we examine
	   the statement index) an inference or an axiom.  If an
	   explicit statement is later asserted for the same SPO, then
	   we need to overwrite that 0L with the assigned statement
	   identifier.

	   Done. override ISPOIterator#close() when returning an
	   iterator backed by a temporary store.

	   Done. infinite loop test case fails when sids NOT used. I'm
	   not sure how the test was succeeding before, but the
	   problem was failing to verify that a statement was in the
	   database before adding it to the focusStore.

	   Done. TestTripleStore#removeStatements() has problem with
	   sids.  The problem is that the sid is not getting placed
	   onto the SPO by the unit test, but it highlights the fact
	   that with sids you need to either have the SID on hand
	   before calling removeStatements() or I need to modify the
	   code to resolve the sids as a first step when I compute
	   their fixed point (at which point I could also discard any
	   statements that were not actually in the database).
	   (addStatements already resolves sids so as to always make
	   them consistent).

	   Done. write unit tests for TM cases when using statement
	   identifiers to make metadata statements.

	 - Work through an import scenario from an application that is
           using URIs generated from the {s,p,o} to represent the
           statement identifier.

	 - Survey all of the ways in which reportStatement/3 gets
           called in RDFXMLParser and decide whether or not "context"
           (the variable set based on bigdata:sid) is always correct
           (either "" or the sid) or if there are some uses, such as
           reification, where "context" should be ignored and update
           the calls to reportStatement/3 or reportStatement/4 as
           appropriate.
      
      2. Done. RDF/XML w/ statement identifiers in/out.

         - @todo reduce to only an "explicit" flag rather than
           {explicit, axiom, inferred} since we can not differentiate
           between constructed statements and inferences.

      3. High level query for reading variable bindings out.

	 - Verify a CONSTRUCT query.

	 - Done. Verify a SELECT query using statement identifiers.

      4. (****) JOIN optimization.

	 - (*) direct term scan.

	 - Map the first triple pattern over its index, and for each
           mapped key-range of the index collect intermediate results
           and map them over the next index.  This must use
           read-historical or read-committed access to avoid locking
           up the unisolated index, but that's going to be automatic
           since the join operator itself does not write on an index.
           the buffer in which we accumulate the join results needs to
           write somewhere, and the choices either back to the client,
           onto the focusStore, or onto the database.  All of those
           can be handled since the read-only procedure will be either
           returning a result or submitting a write procedure.  A join
           variable buffer should accumulate those results and then
           write only the selected variables into any of the
           appropriate locations on overflow (via subclassing or a
           ctor parameter for the writer).

	 - Any SPARQL query which can be directly mapped onto a series
           of JOINS can be directly translated into a rule and run by
           the existing rule engine.  If it has filters that need to
           be applied then they should be handled by a filter applied
           to the buffered join results.  That filter can even handle
	   batch filtering by inspection of datatyped literals.

	 - A further optimization is possible for the local data
           service since all indices are known to be local and the
           index lookups do not need to be batched since they will
           always be continuous unbuffered local reads.

	 - ***** Replace Sesame 2 JOINs by re-writes into our rule
                 engine.
      
      6. Publish on statement level provenance and truth maintenance
         for SPARQL end points.

      7. Defer "named-graph" style quad store for now.

      8. (***) implement prefix compression and apply to the lexicon.
         test it out also on the statement indices and see how it
         stacks up against the "fast rdf key" compression and compare
         with huffman encoding of the decoded long identifiers as
         well.

	 See it.unimi.dsi.fastutil.bytes#ByteArrayFrontCodedList.

	 Note: We should use front compression by default for the
	 nodes of the BTree.  Since the separator keys in the nodes
	 are dynamically determined prefixes, application specified
	 compression generally can't be used.

   - SemTech08 submission?

   - Done.  Correctness testing for scale-out with index partition
     split, move, and join.  See services/StressTestConcurrent. It can
     be parameterized for this purpose.

   - streaming io for block read/write.  note that asynch IO for the
     disk only strategy will not have an impact if we are doing
     commits whose size fits within a single write cache (10M by
     default).

   - write performance test drivers and run on cluster.

      - rdf concurrent query (rdf lubm is not designed to test with
        concurrent loads).

      - bigdata file system workload (must provide reverse traversal
        iterator to handle atomic append for the key-range partitioned
        indices).

      - the memory demand jumps up during overflow handling.  maybe I
        need to throttle the thread pool more on the data service?

      - (****) Write script to allocate services to nodes.

        - N data services; 1 MDS; 1 LBS; 1 TS, etc.

	- The script needs to start the services on a LOCAL disk on
          each machine (I am currently setup on NAS so that means the
          DISK is REMOTE).  This means replicating the environment
          onto the local host (at least the configuration) and then
          starting the service.  The classpath could be resolved on
          NAS or replicated onto the local host and resolve there. (I
          just need to copy [policy.all,
          bigdata-rdf/src/resources/logging/log4j.properties, and
          bigdata-rdf/src/resources/config/standalone ->
          .../src/resources/config/standalone {create the directory
          path first}].  I could also copy the classpath resources,
          but presumably they will be fetched quickly enough and
          become stable - or maybe not?

	- Need to touch up launch-all after the jini install as well
          as installing from a pre-touched version fixing the
          LD_ASSUME_KERNEL_VERSION bug.

	- Get clusterondemand account.

	- Support downloadable code in the configuration, including a
          an optional security model.

      - (*******) rdf concurrent data load.

	- Still an annoying problem with the service names as
          displayed by the jini browser....  This may well be an issue
          with failing to expose the interfaces and service classes
          via an http service as downloadable code.

	- Done. Verify that there is an atomic commit point for the
          IndexSegment so that partial index segment writes can be
          recognized.

	  ============================================================

	- Results:

	  Note: writing on local disk but reading the classpath and
          data from NAS.

	  Note: test on host1/host2 and get a sense of the platform
	  differences.

	  **** Try with overflow processing enabled.

	  **** Try with services running in distinct JVMs and on more
               than one host.

	  **** Try w/o disabling flush so that each document is parsed
               and written in a single pass.

	  **** Try w/ the DiskMode write cache enabled (it's been
               disabled for a lot of, but not all of, these runs).
	       
	  **** Try w/o checksum (i've enabled it in the
               AbstractTripleStore as of 8:30am 4/25)

	  http://192.168.20.28:8080/

	- U10, host3, LDS, 20 threads: 11517 tps <== verify again

	- U10, host3, LDS, 10 threads: 16837/16389/16942 tps.

	- U100, host3, LDS, 10 threads: ____ tps. OOM after 1098 tasks
          completed with 2G RAM trying to flush the write cache to the
          disk.  This occurred during a commit, resulting in an abort
          which interrupted concurrent tasks.

	  Evidentally the JVK requests a temporary direct byte buffer
          for this purpose, at least this JDK on Linux.  Perhaps the
          write cache SHOULD be a direct byte buffer to avoid this
          allocation, but remember that release of direct buffers can
          be a problem?

	- U10, host3, Jini, 20 threads: fails to progress.  (*) Why is
          the system so sensitive to the #of client threads?

	- U10, host3, Jini, 10 threads: 9692/9611/9406 tps

	- U100, host3, Jini, 10 threads: 11693 tps - saved copy of the
          log output.

	- U1000, host3, Jini, 10 threads: ______ tps.

	  ----------------- overflow enabled --------------

	- U10, host3, Jini, 10 threads: ______ tps.

	- U100, host3, Jini, 10 threads: ______ tps.

	  FIXME: 

	  Note: Our write cache was disabled for this test!

	  I have observed consistency problem in AddIds(line 159).
	  This problem is definately linked to synchronous overflow
	  processing.  I've been able to re-create this on the
	  workstation by lowering the initial and maximum extent to
	  50M, or even 10M or 1M and enabling overflow.

	  Caused by: java.lang.RuntimeException: Consistency problem: id=2090932
	      at com.bigdata.rdf.store.AddIds.apply(AddIds.java:159)

	  A message such as this appears shortly after synchronous
	  overflow for the data service on which the lexicon is hosted
	  completed generating new views (probably the term:id index
	  since that is the one responsible for assigning term
	  identifiers).
	
	  The forward map makes a persistent term identifier
	  assignment and SHOULD be consistent.  My current guess is
	  that overflow processing is not handled correctly, for
	  example synchronous overflow MUST NOT run concurrent with
	  any other tasks.  I've made several changes to the
	  WriteExecutorService to try and nail this down and have
	  identified at least one way in which the code was wrong (the
	  group commit did not ensure [nrunning == 0]).  I've changed
	  this, but I can still observe the problem.

	  I have modified the U10 test for the LDS (can not overflow)
	  and the embedded federation (overflow can be enabled or
	  disabled and restricted to one data service).  Both the LDS
	  and the embedded federation appear to be fine when overflow
	  is disabled.  When overflow is enabled, it triggers the
	  consistency problem noted above and can also trigger a
	  variety of other problems that I am documenting below.

	  FIXME Continue testing with the embedded data service and
	  overflow enabled with one data service and eager triggering
	  with a low initial and maximum extent.

	  FIXME Add post-load validation to make sure that the
	  concurrent data load is really correct and run that
	  validation after a db restart.  This will probably require
	  that I write the bulk term scan and get the validation unit
	  tests running for scale-out modes.
 
	- U1000, host3, Jini, 10 threads, no write cache, no overflow:
          ______ tps.

	     Note: I am re-running on host3 with some changes, but
             overflow is still disabled.  The run appears to have
             progressed fine until it ran out of disk after 6 hours
             (U1000-host3-disk-full).  At that point it had loaded
             17349 out of ~20000 files and consumed ~59G.

============================================================

	Looks like virtuoso is running a clustered triple store!

	http://virtuoso.openlinksw.com/wiki/main/Main/VOSArticleLUBMBenchmark

	http://www.openlinksw.com/weblog/oerling/?id=1336

	http://www.openlinksw.com/weblog/oerling/?id=1335

	http://docs.openlinksw.com/virtuoso/clusterprogrammingsqlexmod.html



============================================================

- ERROR: 39296     pool-1-thread-142 com.bigdata.resources.PostProcessOldJournalTask.call(PostProcessOldJournalTask.java:1436): java.lang.AssertionError: ndone=8, but #used=9
java.lang.AssertionError: ndone=8, but #used=9
	at com.bigdata.resources.PostProcessOldJournalTask.chooseTasks(PostProcessOldJournalTask.java:1299)
	at com.bigdata.resources.PostProcessOldJournalTask.call(PostProcessOldJournalTask.java:1344)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
	at java.lang.Thread.run()V(Unknown Source)
	
============================================================

TestConcurrentJournal#test_concurrentReadersAreOk - might have been write cache flush problem?

java.lang.AssertionError: limit=0, byteCount(addr)=392, addr={nbytes=392,offset=486601}
	at com.bigdata.btree.AbstractBTree.readNodeOrLeaf(AbstractBTree.java:2056)
	at com.bigdata.btree.Node.getChild(Node.java:2110)
	at com.bigdata.btree.ChildIterator.next(ChildIterator.java:163)
	at com.bigdata.btree.ChildIterator.next(ChildIterator.java:1)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:59)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:56)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:56)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:58)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at com.bigdata.btree.AbstractNode$PostOrderEntryIterator.hasNext(AbstractNode.java:669)
	at com.bigdata.btree.ReadOnlyEntryIterator.hasNext(ReadOnlyEntryIterator.java:58)
	at com.bigdata.journal.TestConcurrentJournal$1ReadTask.doTask(TestConcurrentJournal.java:1701)
	at com.bigdata.journal.AbstractTask.call2(AbstractTask.java:684)
	at com.bigdata.journal.AbstractTask.call(AbstractTask.java:603)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
	at java.lang.Thread.run(Thread.java:595)


============================================================

Counters error during concurrent data loader run.

WARN : 332347   pool-1-thread-7 com.bigdata.service.DataService$ReportTask.run(DataService.java:1070): Problem in report task?
java.lang.AssertionError: lastModified=0
	at com.bigdata.counters.History.add(History.java:547)
	at com.bigdata.counters.HistoryInstrument.add(HistoryInstrument.java:111)
	at com.bigdata.counters.HistoryInstrument.setValue(HistoryInstrument.java:117)
	at com.bigdata.counters.Counter.setValue(Counter.java:128)
	at com.bigdata.counters.CounterSet$MyHandler.endElement(CounterSet.java:1248)
	at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.endElement(AbstractSAXParser.java:633)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanEndElement(XMLNSDocumentScannerImpl.java:719)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(XMLDocumentFragmentScannerImpl.java:1685)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:368)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:834)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:764)

============================================================

Counters error during concurrent data loader run (U50 on dpp
workstation with lots of swapping going on).  Probable cause: failure
to synchronize writers during write cache flush.

aused by: java.nio.BufferOverflowException
	at java.nio.HeapByteBuffer.put(HeapByteBuffer.java:182)
	at com.bigdata.journal.DiskOnlyStrategy$WriteCache.write(DiskOnlyStrategy.java:348)
	at com.bigdata.journal.DiskOnlyStrategy.write(DiskOnlyStrategy.java:1504)
	at com.bigdata.journal.AbstractJournal.write(AbstractJournal.java:1714)
	at com.bigdata.btree.AbstractBTree.writeNodeOrLeaf(AbstractBTree.java:1990)
	at com.bigdata.btree.DefaultEvictionListener.evicted(DefaultEvictionListener.java:98)
	at com.bigdata.btree.DefaultEvictionListener.evicted(DefaultEvictionListener.java:1)
	at com.bigdata.cache.HardReferenceQueue.evict(HardReferenceQueue.java:273)
	at com.bigdata.cache.HardReferenceQueue.append(HardReferenceQueue.java:235)
	at com.bigdata.btree.AbstractBTree.touch(AbstractBTree.java:1752)
	at com.bigdata.btree.Node.lookup(Node.java:647)
	at com.bigdata.btree.Node.lookup(Node.java:653)
	at com.bigdata.btree.AbstractBTree.lookup(AbstractBTree.java:1148)
	at com.bigdata.btree.FusedView.lookup(FusedView.java:380)
	at com.bigdata.btree.FusedView.lookup(FusedView.java:353)
	at com.bigdata.btree.FusedView.lookup(FusedView.java:321)
	at com.bigdata.rdf.store.AddTerms.apply(AddTerms.java:194)
	at com.bigdata.journal.IndexProcedureTask.doTask(IndexProcedureTask.java:56)
	at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.call(AbstractTask.java:1029)

Also during U50 run:

java.lang.RuntimeException: java.lang.RuntimeException: Execution failed: ntasks=1, nfailed=1
	at com.bigdata.rdf.store.ConcurrentDataLoader.<init>(ConcurrentDataLoader.java:363)
	at com.bigdata.rdf.store.ConcurrentDataLoader.<init>(ConcurrentDataLoader.java:222)
	at com.bigdata.rdf.store.TestTripleStoreLoadRateWithJiniFederation.test_U50(TestTripleStoreLoadRateWithJiniFederation.java:141)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: java.lang.RuntimeException: Execution failed: ntasks=1, nfailed=1
	at com.bigdata.service.ClientIndexView.runParallel(ClientIndexView.java:1027)
	at com.bigdata.service.ClientIndexView.runTasks(ClientIndexView.java:948)
	at com.bigdata.service.ClientIndexView.submit(ClientIndexView.java:812)
	at com.bigdata.service.ClientIndexView.rangeCount(ClientIndexView.java:562)
	at com.bigdata.rdf.store.AbstractTripleStore.getStatementCount(AbstractTripleStore.java:814)
	at com.bigdata.rdf.store.ConcurrentDataLoader.process(ConcurrentDataLoader.java:569)
	at com.bigdata.rdf.store.ConcurrentDataLoader.<init>(ConcurrentDataLoader.java:335)
	... 18 more
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.StreamCorruptedException: invalid stream header
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:205)
	at java.util.concurrent.FutureTask.get(FutureTask.java:80)
	at com.bigdata.service.DataService.submit(DataService.java:1370)
	at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at net.jini.jeri.BasicInvocationDispatcher.invoke(BasicInvocationDispatcher.java:1126)
	at net.jini.jeri.BasicInvocationDispatcher.dispatch(BasicInvocationDispatcher.java:608)
	at com.sun.jini.jeri.internal.runtime.ObjectTable$6.run(ObjectTable.java:597)
	at net.jini.export.ServerContext.doWithServerContext(ServerContext.java:103)
	at com.sun.jini.jeri.internal.runtime.ObjectTable$Target.dispatch0(ObjectTable.java:595)
	at com.sun.jini.jeri.internal.runtime.ObjectTable$Target.access$700(ObjectTable.java:212)
	at com.sun.jini.jeri.internal.runtime.ObjectTable$5.run(ObjectTable.java:568)
	at java.security.AccessController.doPrivileged(Native Method)
	at com.sun.jini.jeri.internal.runtime.ObjectTable$Target.dispatch(ObjectTable.java:565)
	at com.sun.jini.jeri.internal.runtime.ObjectTable$Target.dispatch(ObjectTable.java:540)
	at com.sun.jini.jeri.internal.runtime.ObjectTable$RD.dispatch(ObjectTable.java:778)
	at net.jini.jeri.connection.ServerConnectionManager$Dispatcher.dispatch(ServerConnectionManager.java:148)
	at com.sun.jini.jeri.internal.mux.MuxServer$2.run(MuxServer.java:244)
	at java.security.AccessController.doPrivileged(Native Method)
	at com.sun.jini.jeri.internal.mux.MuxServer$1.run(MuxServer.java:241)
	at com.sun.jini.thread.ThreadPool$Worker.run(ThreadPool.java:136)
	at java.lang.Thread.run(Thread.java:595)
	at com.sun.jini.jeri.internal.runtime.Util.__________EXCEPTION_RECEIVED_FROM_SERVER__________(Util.java:108)
	at com.sun.jini.jeri.internal.runtime.Util.exceptionReceivedFromServer(Util.java:101)
	at net.jini.jeri.BasicInvocationHandler.unmarshalThrow(BasicInvocationHandler.java:1303)
	at net.jini.jeri.BasicInvocationHandler.invokeRemoteMethodOnce(BasicInvocationHandler.java:832)
	at net.jini.jeri.BasicInvocationHandler.invokeRemoteMethod(BasicInvocationHandler.java:659)
	at net.jini.jeri.BasicInvocationHandler.invoke(BasicInvocationHandler.java:528)
	at $Proxy2.submit(Unknown Source)
	at com.bigdata.service.ClientIndexView$AbstractDataServiceProcedureTask.submit(ClientIndexView.java:1267)
	at com.bigdata.service.ClientIndexView$AbstractDataServiceProcedureTask.submit(ClientIndexView.java:1226)
	at com.bigdata.service.ClientIndexView$AbstractDataServiceProcedureTask.call(ClientIndexView.java:1188)
	at com.bigdata.service.ClientIndexView$AbstractDataServiceProcedureTask.call(ClientIndexView.java:1)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
	at java.lang.Thread.run(Thread.java:595)
Caused by: java.lang.RuntimeException: java.io.StreamCorruptedException: invalid stream header
	at com.bigdata.io.SerializerUtil.deserialize(SerializerUtil.java:126)
	at com.bigdata.io.SerializerUtil.deserialize(SerializerUtil.java:179)
	at com.bigdata.btree.Checkpoint.load(Checkpoint.java:282)
	at com.bigdata.btree.BTree.load(BTree.java:1105)
	at com.bigdata.journal.AbstractJournal.getIndex(AbstractJournal.java:2037)
	at com.bigdata.journal.AbstractJournal.getIndex(AbstractJournal.java:1985)
	at com.bigdata.resources.IndexManager.getIndexOnStore(IndexManager.java:352)
	at com.bigdata.resources.IndexManager.getIndexSources(IndexManager.java:520)
	at com.bigdata.resources.IndexManager.getIndex(IndexManager.java:759)
	at com.bigdata.journal.AbstractTask.getIndex(AbstractTask.java:1266)
	at com.bigdata.journal.IndexProcedureTask.doTask(IndexProcedureTask.java:56)
	at com.bigdata.journal.AbstractTask.call2(AbstractTask.java:685)
	at com.bigdata.journal.AbstractTask.call(AbstractTask.java:604)
	... 5 more
Caused by: java.io.StreamCorruptedException: invalid stream header
	at java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:763)
	at java.io.ObjectInputStream.<init>(ObjectInputStream.java:278)
	at com.bigdata.io.SerializerUtil.deserialize(SerializerUtil.java:120)
	... 17 more

main com.bigdata.journal.AbstractJournal.getCommitRecordIndex(AbstractJournal.java:1880): addr={nbytes=76,offset=419565794}
java.lang.RuntimeException: java.io.StreamCorruptedException: invalid stream header
	at com.bigdata.io.SerializerUtil.deserialize(SerializerUtil.java:126)
	at com.bigdata.io.SerializerUtil.deserialize(SerializerUtil.java:179)
	at com.bigdata.btree.Checkpoint.load(Checkpoint.java:282)
	at com.bigdata.btree.BTree.load(BTree.java:1105)
	at com.bigdata.journal.AbstractJournal.getCommitRecordIndex(AbstractJournal.java:1905)
	at com.bigdata.journal.AbstractJournal.<init>(AbstractJournal.java:955)
	at com.bigdata.journal.Journal.<init>(Journal.java:86)
	at com.bigdata.journal.DumpJournal.dumpJournal(DumpJournal.java:196)
	at com.bigdata.journal.DumpJournal.main(DumpJournal.java:135)
Caused by: java.io.StreamCorruptedException: invalid stream header

FIXME The stack traces about result from trying to deserialize a
record containing only nul bytes.  In both cases it happens to be a
checkpoint record.  In the 2nd case it is the checkpoint record for
the commit record index, which means that we can't reopen the store.

Note: the commit record is ALSO all nuls (79 bytes).

rootBlock{ rootBlock=0, challisField=372, version=0,
nextOffset=419565870, localTime=1208792661585,
firstCommitTime=1208792332315,
 lastCommitTime=1208792661569,
commitCounter=372, commitRecordAddr={nbytes=79,offset=419564932},
commitRecordIndexAddr={nbytes=76,offset=419565794},
uuid=b4d8a601-89c4-49c2-9ead-ced80d91a22d, offsetBits=38,
checksum=-1902635405, createTime=1208792329877, closeTime=0}

While the root block was written, it seems that the commit record and
the commit record index checkpoint record were NOT written on the
disk.  Perhaps this is a failure of the OS write cache?

Also Caused by: java.lang.AssertionError
	at com.bigdata.btree.NodeSerializer.getNode(NodeSerializer.java:910)
	at com.bigdata.btree.NodeSerializer.getNodeOrLeaf(NodeSerializer.java:630)
	at com.bigdata.btree.AbstractBTree.readNodeOrLeaf(AbstractBTree.java:2074)
	at com.bigdata.btree.Node.getChild(Node.java:2110)
	at com.bigdata.btree.Node.lookup(Node.java:651)
	at com.bigdata.btree.AbstractBTree.lookup(AbstractBTree.java:1148)
	at com.bigdata.btree.IndexSegment.lookup(IndexSegment.java:357)
	at com.bigdata.btree.FusedView.lookup(FusedView.java:380)
	at com.bigdata.btree.FusedView.lookup(FusedView.java:353)
	at com.bigdata.btree.FusedView.lookup(FusedView.java:321)
	at com.bigdata.rdf.store.AddTerms.apply(AddTerms.java:194)
	at com.bigdata.journal.IndexProcedureTask.doTask(IndexProcedureTask.java:56)
	at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.call(AbstractTask.java:1029)

which might be similar and showed up when I changed the
OverflowManager log level to INFO.


Caused by: java.lang.AssertionError
	at com.bigdata.btree.BTree.writeCheckpoint(BTree.java:722)
	at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.checkpointIndices(AbstractTask.java:1133)
	at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.call(AbstractTask.java:1041)


Error during groupCommit while concurrent overflow processing is running.

java.lang.AssertionError
	at com.bigdata.btree.Node.getIndexOf(Node.java:1746)
	at com.bigdata.btree.Node.setChildKey(Node.java:542)
	at com.bigdata.btree.AbstractBTree.writeNodeOrLeaf(AbstractBTree.java:2043)
	at com.bigdata.btree.AbstractBTree.writeNodeRecursive(AbstractBTree.java:1896)
	at com.bigdata.btree.BTree.writeCheckpoint(BTree.java:726)
	at com.bigdata.btree.BTree.handleCommit(BTree.java:938)
	at com.bigdata.journal.Name2Addr.handleCommit(Name2Addr.java:283)
	at com.bigdata.journal.AbstractJournal.notifyCommitters(AbstractJournal.java:1419)
	at com.bigdata.journal.AbstractJournal.commitNow(AbstractJournal.java:1557)
	at com.bigdata.journal.AbstractJournal.commit(AbstractJournal.java:1517)
	at com.bigdata.journal.WriteExecutorService.commit(WriteExecutorService.java:1463)
	at com.bigdata.journal.WriteExecutorService.groupCommit(WriteExecutorService.java:926)
	at com.bigdata.journal.WriteExecutorService.afterTask(WriteExecutorService.java:547)
	at com.bigdata.journal.AbstractTask.doUnisolatedReadWriteTask(AbstractTask.java:800)


============================================================

INFO : 32406 com.bigdata.rdf.store.SPOIndexWriteProc [testSPO#0] 0
pool-1-thread-76
com.bigdata.resources.PostProcessOldJournalTask.chooseTasks(PostProcessOldJournalTask.java:1260):
index split: testids#0

the id:term index will be split on this overflow.

WARN : 42797
com.bigdata.resources.SplitIndexPartitionTask$UpdateSplitIndexPartition
[testids#0] 0 pool-1-thread-94
com.bigdata.journal.WriteExecutorService.afterTask(WriteExecutorService.java:643):
Task failed:
task=Task{com.bigdata.resources.SplitIndexPartitionTask$UpdateSplitIndexPartition,timestamp=0,resource=[testids#0]}

The atomic update failed.

Caused by: java.lang.IllegalStateException: Object already persistent
	at com.bigdata.btree.PO.setIdentity(PO.java:93)
	at com.bigdata.btree.AbstractBTree.writeNodeOrLeaf(AbstractBTree.java:2036)
	at com.bigdata.btree.DefaultEvictionListener.evicted(DefaultEvictionListener.java:98)
	at com.bigdata.btree.DefaultEvictionListener.evicted(DefaultEvictionListener.java:1)
	at com.bigdata.cache.HardReferenceQueue.evict(HardReferenceQueue.java:273)
	at com.bigdata.cache.HardReferenceQueue.append(HardReferenceQueue.java:235)
	at com.bigdata.btree.AbstractBTree.touch(AbstractBTree.java:1784)
	at com.bigdata.btree.AbstractNode.<init>(AbstractNode.java:320)
	at com.bigdata.btree.Leaf.<init>(Leaf.java:174)
	at com.bigdata.btree.Leaf.split(Leaf.java:594)
	at com.bigdata.btree.Leaf.insert(Leaf.java:472)
	at com.bigdata.btree.Node.insert(Node.java:637)
	at com.bigdata.btree.Node.insert(Node.java:637)
	at com.bigdata.btree.Node.insert(Node.java:637)
	at com.bigdata.btree.AbstractBTree.insert(AbstractBTree.java:1017)
	at com.bigdata.btree.AbstractBTree.insert(AbstractBTree.java:962)
	at com.bigdata.btree.AbstractBTree.rangeCopy(AbstractBTree.java:1499)
	at com.bigdata.resources.SplitIndexPartitionTask$UpdateSplitIndexPartition.doTask(SplitIndexPartitionTask.java:534)

On another run, it is again a SPLIT of the id:term index that
immediately causes problems once that split is made effective on the
MDI (atomic update).

1. Try setting the split threashold higher and see if build (vs split)
   causes problems.

   Ah.  Build seems ok.  However, the memory requirements ramp up
   quickly (perhaps it is fully buffering the nodes?) and I then saw
   an OOM exception.  Try specifying [maxBytesToFullyBufferNodes=1]
   and see if this fixes the OOM problem.  (Note that I increased the
   heap to 1G so change that too.)  This also shows that I need to be
   reporting out the #of open store files and index segments, perhaps
   on each open/close.

   If I push things then I eventually see this stack trace, but there
   seems to be disk left so I am not sure what this is.  Perhaps the
   command was constructed incorrectly?  I've changed the log level to
   WARN for transferFromDiskTo so that I will have more information if
   this occurs again.

Caused by: java.io.IOException: Not enough storage is available to process this command
	at sun.nio.ch.FileChannelImpl.map0(Native Method)
	at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:742)
	at sun.nio.ch.FileChannelImpl.transferFromFileChannel(FileChannelImpl.java:540)
	at sun.nio.ch.FileChannelImpl.transferFrom(FileChannelImpl.java:603)
	at com.bigdata.journal.AbstractBufferStrategy.transferFromDiskTo(AbstractBufferStrategy.java:442)
	at com.bigdata.journal.DiskOnlyStrategy.transferTo(DiskOnlyStrategy.java:1768)
	at com.bigdata.btree.IndexSegmentBuilder.writeIndexSegment(IndexSegmentBuilder.java:1338)
	at com.bigdata.btree.IndexSegmentBuilder.<init>(IndexSegmentBuilder.java:787)

Note: Another run w/o changes produced this trace instead, but I am no
longer seeing this after enabling checksums on the triple store indices.

Caused by: java.lang.AssertionError
	at com.bigdata.btree.NodeSerializer.getNode(NodeSerializer.java:910)
	at com.bigdata.btree.NodeSerializer.getNodeOrLeaf(NodeSerializer.java:630)
	at com.bigdata.btree.AbstractBTree.readNodeOrLeaf(AbstractBTree.java:2106)
	at com.bigdata.btree.Node.getChild(Node.java:2110)
	at com.bigdata.btree.Node.lookup(Node.java:651)
	at com.bigdata.btree.AbstractBTree.lookup(AbstractBTree.java:1149)
	at com.bigdata.btree.IndexSegment.lookup(IndexSegment.java:357)
	at com.bigdata.btree.FusedView.lookup(FusedView.java:380)
	at com.bigdata.btree.FusedView.lookup(FusedView.java:353)
	at com.bigdata.btree.FusedView.lookup(FusedView.java:321)
	at com.bigdata.rdf.store.AddTerms.apply(AddTerms.java:194)
	at com.bigdata.journal.IndexProcedureTask.doTask(IndexProcedureTask.java:56)
	at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.call(AbstractTask.java:1043)

2. Trace the logic of the atomic update for the split.  This appears
   to be a culprit.

   Yes, it was running the BuildIndexSegmentTask which was then
   chaining its own atomic update.  I have modified it to now run a
   BuildIndexSegmentSplitTask, collect all of the intermediate results
   for the split, and then chain the atomic update for the split task.


   Another run then produces this exception for the SPO index.

   *** Note that the SPO index was _EMPTY_ and was "copied" over.

   This suggests either a fence post in how "copy" is handled or a
   broken assumption regarding the active tasks when preparing the
   writeService for overflow.

Caused by: java.lang.IllegalStateException
	at com.bigdata.btree.PO.getIdentity(PO.java:64)
	at com.bigdata.btree.Checkpoint.<init>(Checkpoint.java:215)
	... 21 more

 - add option to verify writes by read back.

 - add post-run validation of the told triples.


WARN : 60529 91700c7b-7fde-4d88-8bb6-63bf21814f70
com.bigdata.resources.BuildIndexSegmentTask$AtomicUpdateBuildIndexSegmentTask
0 [testIndex0#13] pool-1-thread-505
com.bigdata.journal.WriteExecutorService.afterTask(WriteExecutorService.java:643):
Task failed:
task=Task{com.bigdata.resources.BuildIndexSegmentTask$AtomicUpdateBuildIndexSegmentTask,timestamp=0,resource=[testIndex0#13]}


ERROR: 60529      pool-1-thread-14 com.bigdata.resources.PostProcessOldJournalTask.call(PostProcessOldJournalTask.java:1455): Update task failed
java.util.concurrent.ExecutionException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.AssertionError: Expecting either 2 or 3 resources: [JournalMetadata{ size=1048576, filename=C:\Documents and Settings\bthompson\workspace\bigdata\test_stressTest1\91700c7b-7fde-4d88-8bb6-63bf21814f70\journals\journal60560.jnl, uuid=74817d57-30e3-4221-9230-1432fb0200fd, createTime=1209133010972}]
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:215)
	at java.util.concurrent.FutureTask.get(FutureTask.java:85)
	at com.bigdata.resources.PostProcessOldJournalTask.call(PostProcessOldJournalTask.java:1431)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
	at java.lang.Thread.run()V(Unknown Source)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.AssertionError: Expecting either 2 or 3 resources: [JournalMetadata{ size=1048576, filename=C:\Documents and Settings\bthompson\workspace\bigdata\test_stressTest1\91700c7b-7fde-4d88-8bb6-63bf21814f70\journals\journal60560.jnl, uuid=74817d57-30e3-4221-9230-1432fb0200fd, createTime=1209133010972}]
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:205)
	at java.util.concurrent.FutureTask.get(FutureTask.java:80)
	at com.bigdata.resources.SplitIndexPartitionTask.doTask(SplitIndexPartitionTask.java:264)
	at com.bigdata.journal.AbstractTask.call2(AbstractTask.java:695)
	at com.bigdata.journal.AbstractTask.call(AbstractTask.java:612)
	... 5 more
Caused by: java.lang.RuntimeException: java.lang.AssertionError: Expecting either 2 or 3 resources: [JournalMetadata{ size=1048576, filename=C:\Documents and Settings\bthompson\workspace\bigdata\test_stressTest1\91700c7b-7fde-4d88-8bb6-63bf21814f70\journals\journal60560.jnl, uuid=74817d57-30e3-4221-9230-1432fb0200fd, createTime=1209133010972}]
	at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.call(AbstractTask.java:1080)
	at com.bigdata.concurrent.LockManagerTask.call(LockManagerTask.java:325)
	at com.bigdata.journal.AbstractTask.doUnisolatedReadWriteTask(AbstractTask.java:777)
	at com.bigdata.journal.AbstractTask.call2(AbstractTask.java:718)
	... 6 more
Caused by: java.lang.AssertionError: Expecting either 2 or 3 resources: [JournalMetadata{ size=1048576, filename=C:\Documents and Settings\bthompson\workspace\bigdata\test_stressTest1\91700c7b-7fde-4d88-8bb6-63bf21814f70\journals\journal60560.jnl, uuid=74817d57-30e3-4221-9230-1432fb0200fd, createTime=1209133010972}]
	at com.bigdata.resources.BuildIndexSegmentTask$AtomicUpdateBuildIndexSegmentTask.doTask(BuildIndexSegmentTask.java:329)
	at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.call(AbstractTask.java:1043)
	... 9 more

and also:

ERROR: 65685      pool-1-thread-9 com.bigdata.resources.PostProcessOldJournalTask.call(PostProcessOldJournalTask.java:1520): java.lang.AssertionError: ndone=3, but #used=4 : [testIndex0#1, testIndex0#8, testIndex0#5, testIndex0#3]
java.lang.AssertionError: ndone=3, but #used=4 : [testIndex0#1, testIndex0#8, testIndex0#5, testIndex0#3]
	at com.bigdata.resources.PostProcessOldJournalTask.chooseTasks(PostProcessOldJournalTask.java:1328)
	at com.bigdata.resources.PostProcessOldJournalTask.call(PostProcessOldJournalTask.java:1383)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
	at java.lang.Thread.run()V(Unknown Source)
