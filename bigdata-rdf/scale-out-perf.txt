nciOncology.owl, embedded federation.

INFO : 31844   Main Thread com.bigdata.rdf.rio.BasicRioLoader.loadRdf(BasicRioLoader.java:194): parse complete: elapsed=28187ms, toldTriples=464841, tps=16491

INFO : 882875   Main Thread com.bigdata.rdf.store.DataLoader.loadData(DataLoader.java:517): Loaded 1 resources: 464841 stmts added in 28.265 secs, rate= 528, commitLatency=0ms
rule    	ms	#entms	entms/ms
RuleFastClosure13	15	0	0
RuleOwlEquivalentProperty	16	0	0
RuleRdfs02	5890	395806	67
RuleRdfs03	3297	395806	120
RuleRdfs08	31	41618	1342
RuleRdfs09	12109	41724	3
RuleRdfs10	110	41618	378
RuleRdfs11	720126	5324314	7
totals: elapsed=741594, nadded=376849, numComputed=6241034, added/sec=508, computed/sec=8415

Note: this appears to be incremental TM rather than database at once closure.

========================================

Modified to use database at once closure.

nciOncology.owl, embedded federation.

INFO : 36188   Main Thread com.bigdata.rdf.store.DataLoader.loadData2(DataLoader.java:628): 464841 stmts added in 32.109 secs, rate= 14476, commitLatency=0ms

rule    	ms	#entms	entms/ms
RuleOwlEquivalentProperty	157	0	0
RuleRdf01	110	43	0
RuleRdfs02	8859	395958	44
RuleRdfs03	7125	395958	55
RuleRdfs08	250	41631	166
RuleRdfs09	11406	41759	3
RuleRdfs10	219	41631	190
RuleRdfs11	240719	3951672	16
totals: elapsed=268845, nadded=3951672, numComputed=4868778, added/sec=14698, computed/sec=18109

Computed closure in 301500ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1249


============================================================


nciOncology.owl, no closure.

ids: #entries(est)=289871
SPO: #entries(est)=464993
POS: #entries(est)=464993
OSP: #entries(est)=464993
just: #entries(est)=0

!!!Note: be careful to choose the line that reports after the commit on the store!!!

local, unisolated:

run 1: Loaded 1 resources: 464841 stmts added in 23.656 secs, rate= 19650, commitLatency=172ms
run 2: Loaded 1 resources: 464841 stmts added in 24.094 secs, rate= 19292, commitLatency=156ms
run 3: Loaded 1 resources: 464841 stmts added in 24.328 secs, rate= 19107, commitLatency=235ms (after refactor for procedures)
(Computed closure in 141047ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=2671)

local, isolated:

run 1: Loaded 1 resources: 464841 stmts added in 26.735 secs, rate= 17386, commitLatency=438ms
run 2: Loaded 1 resources: 464841 stmts added in 25.719 secs, rate= 18073, commitLatency=297ms

embedded data service:
run 1: Loaded 1 resources: 464841 stmts added in 27.532 secs, rate= 16883, commitLatency=0ms (SPOArrayIterator)
(Computed closure in 375953ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=1002)
run 2: Loaded 1 resources: 464841 stmts added in 27.016 secs, rate= 17206, commitLatency=0ms (SPOIterator - no mem cap)
(Computed closure in 482453ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=781)
ren 3: Loaded 1 resources: 464841 stmts added in 27.485 secs, rate= 16912, commitLatency=0ms (SPOIterator - no mem cap)
(Computed closure in 436266ms yeilding 841842 statements total, 376849 inferences, entailmentsPerSec=863)

embedded federation:

run 1: Loaded 1 resources: 464841 stmts added in 32.313 secs, rate= 14385, commitLatency=31ms

jini federation:

run 1: Loaded 1 resources: 464841 stmts added in 57.204 secs, rate= 8126, commitLatency=16ms
run 2: Loaded 1 resources: 464841 stmts added in 49.172 secs, rate= 9453, commitLatency=16ms

(done) Report more data about the scale-out indices, including the #of
partitions, where each partition is located, and the size on disk on
the partition (the btrees on the journal are conflated so the journal
space needs to be factored out but we can report the #of entries on
the journal and maybe even the bytes written on the journal by the
btree).  Call out the time spent on each index - we need better
counters to report that correctly, or even counters on the data
service.

The embedded federation has a substantial drop in performance when
compared to the local store using isolated indices (the data services
always use isolated indices so that is the point for comparison), but
the big drop is the jini federation - presumably that cost is entirely
attributable to the serialization overhead for RPCs.

Examine in more depth why the embedded federation is slower.  Try a
run on a larger data set and see if this is related to start up costs.

Thesaurus.owl: #terms=586945, #stmts=1,047,647

local, unisolated  : Loaded 1 resources: 1086012 stmts added in  59.609 secs, rate= 18218, commitLatency=312ms
                   : Loaded 1 resources: 1086012 stmts added in  57.765 secs, rate= 18800, commitLatency=328ms
                   : Loaded 1 resources: 1086012 stmts added in  58.313 secs, rate= 18623, commitLatency=312ms
		   : Loaded 1 resources: 1086012 stmts added in  58.687 secs, rate= 18505, commitLatency=312ms (keybuilder refactor)
local,   isolated  : Loaded 1 resources: 1086012 stmts added in  64.562 secs, rate= 16821, commitLatency=156ms
embedded federation: Loaded 1 resources: 1086012 stmts added in  76.969 secs, rate= 14109, commitLatency=31ms
                   : Loaded 1 resources: 1086012 stmts added in  76.938 secs, rate= 14115, commitLatency=16ms
jini federation    : Loaded 1 resources: 1086012 stmts added in 103.734 secs, rate= 10469, commitLatency=0ms
                   : Loaded 1 resources: 1086012 stmts added in 103.859 secs, rate= 10456, commitLatency=31ms

Results for a variety of serialization/compression approaches for the
various Procedures (IndexWriteProc, JustificationWriteProc, etc), but
NOT for serialization changes to the ResultSet (which is really only
used during inference).  In all cases these results are obtained for
the jini federation since that is the only case where we are forced to
serialize the data in a Procedure or a ResultSet for RPC.

NoCompression.  This serializes each key and value as a full length
byte[].

   Loaded 1 resources: 1086012 stmts added in 107.922 secs, rate= 10062, commitLatency=0ms
   Loaded 1 resources: 1086012 stmts added in 105.531 secs, rate= 10290, commitLatency=16ms

NoCompression, but writing on a DataOutputBuffer and then copying the
results to the output stream (see if this case improves if we reuse
the buffer for each request or using a thread-local variable):

   Loaded 1 resources: 1086012 stmts added in 149.484 secs, rate= 7265, commitLatency=16ms

BTreeCompression.  This uses prefix compression on the keys and simple
serialization of the values.

   Loaded 1 resources: 1086012 stmts added in 103.203 secs, rate= 10523, commitLatency=16ms

FastRDFCompression

   Loaded 1 resources: 1086012 stmts added in 102.109 secs, rate= 10635, commitLatency=16ms
   Loaded 1 resources: 1086012 stmts added in  99.75  secs, rate= 10887, commitLatency=16ms (NIO)
   Loaded 1 resources: 1086012 stmts added in  99.313 secs, rate= 10935, commitLatency=15ms (NIO)

The "FastRDF" approach is probably as good as I can make it for the
statement indices.  It performs only marginally better than the no
compression approach.

Perhaps the additional overhead is a mixture of:

 - de-serialization to support RPC;
 - the mechanisms of RPC (client, server, protocol, network)
 - the added burden on the heap

NIO for the RPC protocol appears to help a bit, but it runs out of
memory in the test suite (this shows up as an NPE in ByteBuffer).


Concurrent load rates:

Explore interaction of the group commit policy.  If we check point vs
commit vs do not wait around then how does that effect the
throughput!!!

Note: smaller buffer sizes (1000 statements) makes the total run much
slower.  Try this with more threads, but we will probably have to wait
on the group commit so that won't help with the current policy.

Note: larger buffer sizes will cap out since there is only so much
data in the LUBM files.

U10

embedded data service:

Finished: #loaded=189 files in 96015 ms, #stmts=1272577, rate=13253.0
(#threads=3, largestPoolSize=3, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 73797 ms, #stmts=1272577, rate=17244.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 85625 ms, #stmts=1272577, rate=14862.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 78750 ms, #stmts=1272577, rate=16159.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 73203 ms, #stmts=1272577, rate=17384.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 63172 ms, #stmts=1272577, rate=20144.0
(#threads=20, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=189, #ok=189, #err=0)

All done: #loaded=189 files in 59734 ms, #stmts=1272577, rate=21304.0
(#threads=20, class=LocalTripleStoreWithEmbeddedDataService,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=189,
#ok=189, #err=0)

embedded federation:

Finished: #loaded=189 files in 191828 ms, #stmts=1272577, rate=6633.0
(#threads=1, largestPoolSize=1, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 122343 ms, #stmts=1272577, rate=10401.0
(#threads=3, largestPoolSize=3, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 282140 ms, #stmts=1272577, rate=4510.0
(#threads=3, largestPoolSize=3, bufferCapacity=1000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 90860 ms, #stmts=1272577, rate=14005.0
(#threads=10, largestPoolSize=10, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 85735 ms, #stmts=1272577, rate=14843.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 88453 ms, #stmts=1272577, rate=14387.0
(#threads=20, largestPoolSize=20, bufferCapacity=20000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 87359 ms, #stmts=1272577, rate=14567.0
(#threads=30, largestPoolSize=30, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 105203 ms, #stmts=1272577, rate=12096.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

All done: #loaded=189 files in 106109 ms, #stmts=1272577, rate=11993.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

   disk: 1,230,029,630 {osp,spo,terms} + 51,870,457 {ids,pos}

Alternative index allocation: 

   Note: This case appears to be much more efficient in term and
   space, at least for the embedded federation:

   disk: 80,506,107 {terms,spo} + 90,515,091 {ids,pos,osp}

   All done: #loaded=189 files in 88016 ms, #stmts=1272577,
   rate=14458.0 (#threads=20, class=ScaleOutTripleStore,
   largestPoolSize=20, bufferCapacity=100000, autoFlush=false,
   #done=189, #ok=189, #err=0)

jini federation:

Finished: #loaded=189 files in 392078 ms, #stmts=1272578, rate=3245.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

Finished: #loaded=189 files in 371297 ms, #stmts=1272582, rate=3427.0
(#threads=20, largestPoolSize=20, bufferCapacity=10000, #done=189,
#ok=189, #err=0)

All done: #loaded=189 files in 82328 ms, #stmts=1272577, rate=15457.0
(#threads=20, class=ScaleOutTripleStore, largestPoolSize=20,
bufferCapacity=100000, autoFlush=false, #done=189, #ok=189, #err=0)

    Note: This is an extremely odd result.  It was obtained by running
    immediately after the previous jini federation run.  Overall, jini
    seems very sensitive to initial conditions.  Perhaps this is
    related to memory limits on the laptop platform?  Often the jini
    run appears to be very nearly single threaded.

All done: #loaded=189 files in 241672 ms, #terms=314871,
#stmts=1272577, rate=5265.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=189,
#ok=189, #err=0)

server1: All done: #loaded=190 files in 74049 ms, #terms=314871,
#stmts=1272577, rate=17185.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=190,
#ok=189, #err=1)

server1: All done: #loaded=190 files in 76956 ms, #terms=314871,
#stmts=1272577, rate=16536.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false, #done=190,
#ok=189, #err=1)

   disk: 90,926,328 {terms,spo} + 90,926,328 {ids,pos,osp}

server1: All done: #loaded=2008 files in 739904 ms, #terms=3301736,
#stmts=13405383, rate=18117.0 (#threads=20, class=ScaleOutTripleStore,
largestPoolSize=20, bufferCapacity=100000, autoFlush=false,
#done=2008, #ok=2007, #err=1) (U100 is 13M triples)

   disk: 1,110,058,584 {terms,spo} + 1,071,640,537 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         140.096 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         112.644 s (pause 15.700 s)

server1: #loaded=20022 files in 11419382 ms, #terms=32885169,
#stmts=133573856, rate=11697.0 (#threads=20,
class=ScaleOutTripleStore, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=20022, #ok=20020, #err=2)

   disk: 14,110,887,061 {terms,spo} + 12,039,810,264 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         1319.038 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         693.036 s (pause 103.692 s)

server1: All done: #loaded=20022 files in 11633794 ms, #terms=32885169,
#stmts=133573856, rate=11481.0 (#threads=20,
class=ScaleOutTripleStore, largestPoolSize=20, bufferCapacity=100000,
autoFlush=false, #done=20022, #ok=20020, #err=2)

   disk: 14,093,839,353 {terms,spo} + 12,038,914,891 {ids,pos,osp}

[INFO ][memory ] Memory usage report
[INFO ][memory ] young collections
[INFO ][memory ]     total GC time =         1279.318 s
[INFO ][memory ] old collections
[INFO ][memory ]     total GC time =         666.388 s (pause 100.395 s)

============================================================

Notes on store level record checksums and record compression.

1. many record compression schemes will fail if the data are corrupt,
   but logically you compress first and then checksum the record.

   compression is often a technique using a stream of blocks.

   checksum is a streaming technique.

   // IRawStore#write()
   write(ByteBuffer b) : addr

   // AbstractJournal#write()

   if(compress) {

      b = compress(b)

   }

   int chksum;
   if(useChecksum) {

      bytesRequired = b.remaining() + 4;

      chmsum = computeChecksum( b );

   } else bytesRequired = b.remaining();

   bufferStrategy.write(b,chksum,useChecksum)

   Note: buffer strategy write() probably needs to have the checksum
   value pass along in addition to the record to avoid re-allocation
   of the ByteBuffer just to tack on the additional 4 bytes.  We could
   either always write those additional 4 bytes or optionally write
   them if checksums are enabled.

2. the root block needs to hold the critical data indicating whether
   or not checksums in use and what record compression technique, if
   any, to apply.  We need this on hand before we can either read or
   write a record on the store.

3. we need 4 bytes (int32) for the checksum.  this should be at the
   end of the record, so the size in the store is extended by 4 bytes
   and the address for the record on the store is adjusted to also
   include those 4 bytes.  However, when you read from the store it
   will give you a slice WITHOUT those four bytes.  Further, if it is
   using compression then it will decompress the slice, resulting in a
   new slice that can be much larger than the record on the store
   whose size is encoded within the address.  This will probably break
   a variety of asserts that assume that the returned ByteBuffer will
   be exactly the size of the byte count encoded in the address.

4. Compression should run on the byte[] not on the slower ByteBuffer.
   Serialization generally writes on a byte[], but sometimes that is
   wrapped up as a ByteBuffer - and it can even be a slice() onto a
   larger array (NodeSerializer does this since it returns a view onto
   an internal buffer).

    /**
     * The {@link Adler32} checksum. This is an int32 value, even through the
     * {@link Checksum} API returns an int64 (aka long integer) value. The
     * actual checksum is in the lower 32 bit.
     */
    static final int SIZEOF_CHECKSUM = Bytes.SIZEOF_INT;

    /**
     * Offset of the int32 value that is the {@link Adler32} checksum of the
     * serialized node or leaf. The checksum is computed for all bytes exclusing
     * the first 4 bytes, on which the value of the computed checksum is
     * written.
     */
    static final int OFFSET_CHECKSUM = 0;

    /**
     * When <code>true</code>, checksums will be generated for serialized
     * nodes and leaves and verified on read. Checksums provide a check for
     * corrupt media and make the database more robust at the expense of some
     * added cost to compute a validate the checksums.
     * <p>
     * Computing the checksum is ~ 40% of the cost of (de-)serialization.
     * <p>
     * When the backing store is fully buffered (it is entirely in RAM) then
     * checksums are automatically disabled.
     * 
     * @deprecated See {@link #setUseChecksum(boolean)}
     */
    public final boolean getUseChecksum() {return useChecksum;}

============================================================

    - Tune indices

      - The ids index should benefit from value compression since the
        values are the serialized terms.  This will require custom
        code to break the values into symbols and then use huffman
        encoding.  Alternatively, simply treat each value as a symbol
        and code from that (assuming that value reuse is common - if
        not then at least URIs can be broken down into common
        symbols).

	Done. Do not store bnodes in the id:term index.

      - The terms (term:id) index is on the order of 5x larger than
        the ids (id:term) index.  Presumably this is because updates
        are distributed more or less randomly across the terms index
        as new terms become defined but are strictly append only for
        the ids index since new ids are always larger than old ids.
	
         - A larger branching factor may benefit the ids index.

	 - A compacting merge of the terms index should greatly reduce
           its size.

	 - Nearly ALL _read_ time between the SPO and TERMS index is
           reading the TERMS index (99%).

	 - Nearly ALL _write_ time between the SPO and the TERMS index
           is writing the SPO index (99%).  Statements are more likely
           to be distinct than terms, so it makes sense that we write
           on the statement index more often.  However, note that this
           is true even though the TERMS index is 3x larger than the
           SPO index.

    - BTree

     - The RecordCompressor as utilized by the NodeSerializer is NOT
       thread-safe as it relies on a single cbuf field.  Either the
       static buffer pool (if direct buffers are performant for this),
       a heap buffer pool, dynamic heap allocations for
       (de-)compression, or a serialized access to an instance per
       NodeSerializer instance (and hence per BTree instance).

     - Change checksums to be at the store/record level.  Interpret
       the record length as having 2 additional bytes for read/write
       of the checksum.  Put it at the end of the record.
       Enable/disable at the store level.

       Add an option for read-back validation of writes?
       
       Add an option for a fully synchronized raw store interface on
       the Journal?

     - IAutoboxBTree

       - Write tests of the autobox API for BTree, FusedView,
         ClientIndexView, and DataServiceIndex.

       - Should be able to instantiate a resource that is a BigdataMap
         or BigdataSet, so perhaps make these classes extend
         AbstractResource?  Same for SparseRowStore?

       - Need [isNull] for ClientIndexView and DataServiceIndex impls
         to reconstruct the object by allowing reconstruction of the
         ITuple.

	 Could modify the ResultBuffer to provide this additional
         information as an option and specify an appropriate
         constructor for the point test to get back that metadata.

	 Really, should define crudTuple() methods and rework the
	 batch crud methods in terms of tuples.  That is the general
	 framwork.  Bit flags can be used to indicate that certain
	 information (keys, vals, etc). are not required for a given
	 op.  (keys are always available on the client for these ops
	 so there is never a need to send them with the data: just
	 {val, isNull, isDeleted} and the option to read deleted
	 tuples.

	 BigdataMap and BigdataSet will not scale-out until this issue
	 is resolved.

    - Distributed file repository

         - handle overflow of blocks to the index segments during MOVE

	 - provide streaming socket api on data service for reading
           blocks (low level in the DiskOnlyStrategy - if in the write cache
           then return directly else return buffered input stream reading on
           the disk file and interrupt if journal is closed).

	 - range delete

	 - logical row scan for headers of documents in a key range.

         - Write test for forward and reverse scans starting at the
           fence posts around a partition boundary.

    - (**) Map/Reduce demo jobs.

      - Rework the map/reduce implementation to use local writes and
        distributed gathers.

      - Download, prepare, extract.

      - Concurrent RDF data load as a map/reduce job.

    - Tune network IO

      - huffman encoding is appropriate for network IO, but hu-tucker
        is not required since we have to decompress keys to get them
        inserted into the btree.

      - tokenization needs to be specified for RDF Value types for the
        purposes of compression.  In fact, we are guarenteed that
        values are NOT duplicated in a given batch so tokenization
        needs to uncover common symbols.  This is easy for URIs but
        less so for literals and impossible for BNodes (which do not
        really need to be in the lexicon anyway).

    - Try jini federation using only the terms index to assign
      consistent term identifiers, bulk loading into local SPO-only
      indices, and then range partitioning the indices into global
      SPO, POS, and OSP orders and bulk loading the global statement
      indices.  The data loader should be concurrent and a filter
      should be applied such that each "host" loads only the files
      that hash MOD N to that host.  (note that only AddTerms and
      AddIds go across the network API in this case.)

    - The temp triple store supports concurrent read only but not
      concurrent write, so it is not appropriate for a concurrent bulk
      loader.

    - An extended transaction model can be used for truth maintenance.
      The focus store is built up within isolated indices (that do not
      actually correspond to persistent indices, they only exist on
      the per-tx per-dataservice TemporaryStore).  The application can
      simply combine sets of assertions or retractions within a single
      transaction.  Either the application or an extension of the
      transaction manager MUST serialize the commits.  Within the
      commit processing, first do retractions then do assertions.

      - Provide for transaction local indices.  The index is dropped
        when the tx completes.

      - Provide for registration of a global index within a
        transaction, but the transaction will fail if the index
        already exists when it commits.

      - For full transactions, explore a synchronous overflow variant
        from a managed journal hosting named indices (as isolated by
        the tx) backed by a transient buffer to a managed journal
        backed by a disk buffer that would let us keep full
        concurrency.  The overflow should be a buffer -> disk transfer
        and then the disk file should be allowed to grow without
        bounds (or to the resource limit of the tx).  Asynchronous
        overflow processing for transaction journals would add a layer
        of complexity throughout as the MDI would need to be
        instantiated on a per-tx basis.

	The WriteExecutorService for the transaction would identify
	and support synchronous overflow exactly as it does now for
	unisolated journals.

     - Raw temporary stores such as are used by the index manager must
       be handled differently since (a) there are no indices in use;
       and (b) the use is always single threaded (no write executor
       service).  This case requires a direct buffer to disk transfer,
       but the API can be declare an assumption that the caller is
       single threaded and the writer can simply block during that
       transfer.

     * Some of the buffer strategy implementations appear to assume
       that by synchronizing one method, such as truncate() or
       transferTo(), that concurrent writers are automatically
       synchronized for those operations.  This is NOT true unless the
       write() method is also synchronized, and it is not for at least
       the DirectBufferStrategy.  This could show up as a concurrency
       problem with the indices when the store is used in a mode that
       is in fact concurrent and an overflow is triggered.

Short term tasks:

   - (*) Builds and releases.
   
      - Change over to subversion so that the edit trail does not get
        lost (complex process).

      - add alternative license.

      - Maven 2.x build
      
         - Start doing snapshot releases.

	 - Start periodic project documentation builds, perhaps on SF.
           Publish on the www.bigdata.com site.

         - Change the dependency to dsiutils.  I tried to do this with
           dsiutils-1.0.4 and ran into problems with
           (de-)serialization when compared to the lgpl-utils versions
           of the same classes.  Try this again and pay close
           attention to the lgpl-utils versions of the classes now
           located in dsiutils and see if I can isolated the problem.
           The problem was demonstrated by the bigdata-rdf test suites
           for both the temp and local triple stores but not for the
           bigdata test suites.

	 - Done. Update the Sesame 2.x dependency.

	 - Put all properties into the com.bigdata namespace.

   - Counters

     - Done. Work the counter path, name, and date(s) into the table
       which shows the counters history values so that it can all get
       copied easily into a worksheet.

     - Done. #commit is not being encoded property and shows up as a
       URL anchor and not as part of the PATH parameter.

     - Done. Counter XML MUST persist the HISTORY in the XML so that
       the log files can be useful for post-mortem.
       
     - Done. Write a final log file ('-final.xml') when the LBS
       terminates.

     - Done. This is now a configuration property.  The load balancer
       is not writing its counters into the correct location (logDir).
       The directory needs to be relative to the service directory, so
       a method needs to expose that directory to the service.

     - Done. (Not quite sure what the problem was here, but I made a
       few changes and it appears to be fixed.)  The concurrent data
       loader was failing to halt once it started the flush tasks.

     - Done.  (Modified to accept samples out of timestamp order and
       to record the #of and total of samples falling within a given
       period.)  Loosing some samples through reporting w/in the same
       period.  Round up to the next period if this period is filled.
       An alternative is to sum the samples in the period and report
       their average by also tracking the #of samples in the period!

     - Done. When writing the path in the table rows, only write the
       path from the selected root.

     - Done. Problem with double-decoding of URL in NanoHTTP.

     - Done. (Can be a bit odd when also using a regex filter.) Add
       depth query parameter to limit the #of levels resolved from the
       path.

     - Done. (Currently using engineering notation, should be query
       parameter).  Set to 6 digits precision, not {3,6} after the
       decimal.  Or right justify decimal value with fixed N digits
       after the decimal (could be query param).

     - Done. (Also added the timestamp itself.) When converting to
       minutes, hours, and days in httpd make sure to have a few
       digits after the decimal -- otherwise false boundaries.

     - Done. (uses wildcards before and after and ORs together.) The
       filter needs to accept regex characters or prefix and post-fix
       with ".*".  Since things are quoted, right now nothing is
       actually matched.

     - Done. Since the log files provide post-mortem, there should be
       a way to view the files through the same httpd tool - a mode
       where it reads a single named counter XML file and then lets
       you browse it.  This will make it easy to find interesting
       views.

     - Done. The IndexManager should report the #of index views open
       concurrently.  Either sample once per second and take a moving
       average track the total number and compute the instanteous
       average per minute.
 
     - Done (reports the #of stores in the weak value
       cache). Likewise, the StoreManager should report the #of open
       journals and index segments.

     - Done. Anything with "%" or "percent" in the name should be
       formatted as a percentage in [0.00:1.00].

     - Done.  The data service should report its configuration
       properties under "Info".

       Done. This should be done for the other services as well.
       Refactor the code in DataService, moving it into the counters
       package.

       Note: Servers should add their Jini configuration information
       as well.  This probably has to be done explicitly for the
       configuration items of interest.

     - Done. Compute average response time.  Throughput is 1/average
       response time.

     - Done. Add counters for #of index partition split, move, and
       join operations (OverflowManager).

       Done. Also report #of errors during asynchronous overflow
       processing.

       Note: There should also be a counter of the #of index
       partitions moved onto a data service.  However there is no
       place in the code to easily note this on the target data
       service since the move is made atomic by an action on the
       metadata service.

     - Done. Add counter to the write service that reports the #of
       tasks which have their locks and are actually doing their work
       concurrently (LockManager defines such a counter but we need
       its moving average not the instantaneous value).  This is the
       real concurrency of the tasks.  The #of active tasks in the
       write service is a red herring since those tasks could be
       waiting for their locks.

     - Done. Add per-process counters for GarbageCollectorMXBeans.

     - Done. (NanoHTTPD was not reporting errors in the serve() method
       anywhere and was failing to send back an error to the client.)
       For some reason a query for the hostname on host3 does not
       return the counters in the browser.  Response is fast both
       beneath that level at at the root.  Maybe the problem is in the
       /host/CPU and /host/Info counter sets - those appear to hang
       while /host/service is fine.... that does not seem to pay out
       either.

     - Done.  (I am assuming that Format was not thread safe - it was
       being used concurrently by the Sar collector and the pidstat
       collector, even with only one service). pidstat : Problem
       parsing [02:08:24 PM] as a time for input string "".  This is
       odd.  I can test this out and it works for the specified
       format.  And it is the only error reported for pidstat parsing.
       Ah.  Format is doubtless not thread-safe.

     - Done. Fixed issue where sar and pidstat would overflow the
       field, eating into whitespace to the right.  The data lines are
       now split based on whitespace after first skipping over a date
       field (based on the ISO date format).

     - Done. The per-process counters for linux are not being reported
       under "service" but instead directly under the service UUID.

     - Done. Group services under their service type in the counters.

     - Done (also fixed a bug where the LDS was not sending a join
       message to the LBS and modified notify(), warn(), and urgent()
       to invoke join() on the behalf of remote clients). The LBS
       should add counters for the host scores.  This will provide
       transparency in how it interprets the data from the various
       hosts.

     - Done. The client can discover the LBS and report data every so
       often.  This would result in redundent reporting when there is
       more than one client if the counter reflects the database
       state, but there is no harm in that.

     - Done. Could report the #of files read, #of triples processed,
       triples in the db, average throughput rate for that client (or
       all clients), etc

       - (**??) tps appears low as reported by the client to the LBS
         when compared to the final value computed by the client.
         This may be a function of the outstanding writers that have
         not yet completed, in which case the loader clearly needs to
         force the report of the final counter values when a load
         completes.

         Maybe this is reporting the upper bound for statements?  That
         does not make sense though since only deleted entries or
         views with an index segment cause the upper bound to be
         higher than the actual entey count.

       - Compute bytes per statement in the db (requires a db op to
         correlate the journal size with the #of stmts or #of terms)

       - Done. report success and errors from the concurrent data
         loader.

       - Done. incremental evaulate futures for the concurrent data
         loader.

     - Done. Report response time measures on the client, which will
       require a class similar to TaskCounters that is intimate with
       the ClientIndexView.  That will give a client perspective on
       the latency of tasks, which will aggregate across the data
       services that it uses and include the costs of RMI, in contrast
       to a data service perspective, which aggregates across the
       clients using that service and discount RMI.

       Note: The LDS does not use RMI - it submits tasks directly to
       the data service queues.

     - Done. Need to aggregate statistics for the partitions of an
       index as reported to the LBS for analytics.

     - Need to remove index partitions which are known to be stale
       from the LBS after a bit, or at least allow them to be hidden.
       The data will eventually grow beyond what can be held by an LDS
       if we do nothing.

     - Done. Show metadata about the open index segments in the
       IndexManager

     - Done. Send stale locator notices to the LBS and replace the
       counters (or nest them under) on the LBS when the index
       partition is reported as stale.

       Done. Note: having a local httpd for the data service would
       make it much easier to inspect the indices.
    
     - (**) The IndexManager should report the #of open indices.  We
       can have an exact count of that if we use a static atomic
       integer in AbstractBTree.
       
       Likewise, we can get the exact count of the #of open journals
       vs index segment stores and BTree vs IndexSegments using the
       same technique.

       Done. The statistics that are used by the overflow manager to
       compute which indices are move candidates are not being exposed
       via the counters to the LBS.  This includes all of the
       per-index bytes read, bytes written, etc. data.

     - Done. The IndexManager should report the index partitions on the
       data service, at least until this exceeds 100s of indices.
       This should be done not via counters but rather via the httpd
       service itself making an RMI request to the data service and
       listing out the named indices. Present additional information
       when the service is a metadata service, e.g., by listing out
       the tuples of the index, which are in fact the locators for the
       index partitions.  Also, provide some aggregation over the MDI,
       including the total #of partitions and tuples.

     - The counters are overflowing to days before midnight.  Check
       the locale and see when timestamp causes an overflow to the
       next hour and the next day in some unit tests.

     - Done. When restoring the LBS counters from XML, the history on
       the counters is being ignored.

     - (****) Make it possible to have more than 60 minutes in the
       buffer but still overflow after 60 minutes onto the hours.
       This will allow a longer reachback at a given level of
       aggregation.

     - Add UI elements to set the filter(s), depth, decimalFormat,
       etc.  These should be a FORM with a GET action.

     - May be loosing some samples by running multiple typeperf's at
       once.  Explore.  If true, then trying combining all w/in same
       JVM using reference counter for process or identifying one
       process in the JVM which will have responsibility for those
       counters.

     - syslogd integration so that I see ERROR and FATAL messages for
       the hosts in the federation.

     - Done. Add option to NOT run typeperf and use for the unit tests
       of the services when performance counters are not required.

     - Done. Add reporting by the client on the indices that it is
       using.

     - Done. Add reporting to the concurrent data loader for #errors.

     - Done. The CDL tps counter needs to stop counter time once the
       load is complete.  As it is the rate continues to drop with
       elapsed non-load time.  This makes the value reported to the
       load balancer wrong! (Even the value available from the client
       is wrong since it can be off by up to 60 seconds of load time).

     - Done. Problem w/ correlated view of counters. Was plotting a
       String[] rather than its elements.

     - Add counters to the client index view showing the times for
       each operation and also showing the times for querying the
       metadata index to split an key[] operation or map a key-range
       operation.  This will be help identify if/when the MDI is a
       bottleneck for the client.

   - Admin UI
   
     - It would be nice to be able to drill down into the index data,
       but that should be an admin UI not the counters UI.

     - Expose the known triple stores, a view on the global sparse row
       store, etc.

     - Offer commands to force overflow of a data service.

   - Sparse row store support.

     - JSON API

       - Add a web application that let's people easily write on or
         read from the sparse row store using JSON or the like.

       - The JSON API should be compatible to the extent possible with
         HBASE and GAE.

       - This web application will have to be distributed in order to
         eliminate bottlenecks.  One approach is to re-direct HTTP
         clients to an embedded web service co-located with the data
         service on which the row resides.  HTTP clients can continue
         to address that server until they receive a redirect.

     - Add a BLOB reference column type.  There are at least two
       design options for this.  I think that we should support at
       least (1) and (2).

       (1) the blocks are stored locally and hence are always
           available from the same data service as the BLOB reference
           column value - this might limit the maximum effective blob
           size (in bytes) since the data will have to fit in the same
           index partition and hence be co-located on a single host.
           In fact, the blocks will be in the same index segment as
           the column value once the journal overflows.  One advantage
           of this approach is the block updates can be atomic with
           block metadata updates - a feature that is not otherwise
           available without full transactions.
       
       (2) the blob reference includes the name of the scale-out index
           on which the blocks for that blob are stored - in this
           model the blocks can reside anywhere and splits of the
           blocks will have no relevance to splits of the metadata.
           This also makes it easier to locate the partitions of the
           index containing the blocks on data services that are
           specifically provisioned for large data blocks.

       (3) the blob reference contains the primary key for the blob
           and the blob is stored either in the same index or in
           another index.  I am not sure that this variant adds value
           over (1) and (2).

     - Refactor the BigdataRepository to use the BLOB reference type.

   - IndexSegment

     - Done. Verify that there is an atomic commit point for the
       IndexSegment so that partial index segment writes can be
       recognized.

     - Done. The addrLeaves and addrNodes can not be expressed as a
       single long since they are regions spanning many records and a
       long would limit their byteCount to whatever was allowed by the
       offsetBits.

     - Done. The logic to buffer the nodes was actually examining the
       extent of the leaves.

     - Done. Added checksum to the index segment store checkpoint
       record.

     - Done (the problem was failing to flush the write cache to the
       disk before initiating the channel to channel transfer - since
       the data were not on the channel they naturally were not being
       transferred.)  Stress test of U100 and very large runs of
       TestIndexSegmentBuilderWithLargeTrees (m=32, nentries=m**4)
       cause corrupt leaves in an index segment build.

     - Done. Modify IndexSegmentBuilder to support fast reverse scan,
       at least in the data and in DumpIndexStore.

     - Done. IndexSegmentBuilder should be refactored to implement
       Runnable/Callable.  The ctor can do the setup and the index
       build will run in run()/call().  The return can be a status code
       for the operation or statistics on the operation.  Throw an
       exception for an error.

     - Done. Modify IndexSegmentBuilder to support fast forward scan,
       at least in the data and in DumpIndexStore.

     - An index segment build that could operate without the actual #of
       entries would avoid one pass over the data.  See how expensive
       that traversal is (it is done in one location) and decide
       whether it is worth trying to operate without that information
       on hand.  Eg, by using the upper bound on the entry count and
       then allowing leaves and nodes to underflow.

     - Done. Support transparent byte[] <=> value (de-)serialization
       using a serializer object associated with the index metadata.

     - Done. Add Map and Set impls. based on the BTree.

     - Done (IUpdateStore). I need to create an interface extending
       IRawStore, put those methods on that interface, and then declare
       that interface on TemporaryRawStore in order to be able to
       access these methods.

   - DiskOnlyStrategy

     - Done. write tests of transferTo.  I did find a bug in
       writeAll(), but that was not the problem (it was writing each
       pass at the same offset in the file when a write required
       multiple IOs).

     - Done. Refactor to create a Journal using a Temporary BufferMode.

     - Done. Refactor DiskOnlyStrategy to permit lazy creation of the
       backing file.

     - Done. Refactor of TemporaryRawStore to use DiskOnlyStrategy with
       lazy creation of the backing file.

     - Done. Found a bug in DiskOnlyStrategy where it was always
       reading a record three times....  Probably there was little
       performance impact since the read would have come from the OS
       cache after the first time, but still....

     - Evaluate effect of the write cache capacity.  If 1M is as good
       as 10M then just use the DirectBufferPool for the write cache
       for the live journal, which will simplify some things.

     * Add counters reporting the #of bytes written/commit.

     * Test suite for Temporary mode journals.

   - DirectBufferPool

     - Done. Static config via System#getProperties() for the direct buffer pool

     - Done. Counters for reporting out the pool state.

     - Done. Report "bytesUsed" for the DirectBufferPool.

   - Transaction support.

     - Overhaul transaction processing and support full, 2-/3- phase
       transactions

     - A problem is reported by StressTestConcurrentTx.  Revisit this
       when I overhaul the full transaction support.

     - Transaction identifiers need to be "symbols" that respect the
       timestamp ordering for historical reads and the transaction
       start time.  Since the timestamps are discrete it is possible
       that the factory will have to wait until it can assign a
       transaction identifier for the interval implied by some desired
       historical start time.

     - ** Change tx timestamps to negative and use positive timestamps
       for historical reads.  Changes to AbstractTask, ITx,
       ITransactionManager, StoreFileManager, IsolationEnum, and the
       post-processing tasks.  This will greatly simplify thinking
       about historical read operations since they will simply use the
       actual commit time while transactions will use a free
       (-timestamp) value selected by the transaction manager.

     - AbstractResourceManagerTask - documents a potential problem
       with MDI updates without 2-phase commits.  Look into this
       further and see if this problem can be addressed without using
       a full transaction.  If not, then we will need to use a full
       transaction to avoid this issue.

  - Support hash partitioned indices.  The index would be marked in
    the IndexMetadata as being hash partitioned, including the #of
    index partitions (N) and a hash function (can be defaulted).  A
    data service UUID is assigned to each index partition.  Write a
    ClientHashPartitionedIndex (vs the ClientIndexView, which could be
    renamed as KeyRangePartitionedIndex)

  - (*****) Iterator refactor

     * Change the striterator heirarchy into an implementation
       heirarchy (StriteratorImpl) for co-varying generics and a use
       hierarchy (IStriterator and Striterator) in which only the
       element type is generic. This should be significantly easier to
       use and understand.

     - Modify the procedure logic to abstract a 'next key/val'
       iterator using a shared buffer for de-compression in order to
       minimize heap churn on the data server.

     - Support copy in/out of keys and vals in lookup(), insert(),
       remove(), and rangeIterator so that we can (a) be more
       efficient in handling keys and vals by copying; (b) handle keys
       and vals that are byte aligned or bit aligned in the node or
       leaf; (c) reduce GC by converting to a compacting record for
       the node/leaf; and (d) expose the version counter and deletion
       marker for fused views of indices with isolation.

     - Turn off sendVals for rangeIterators if we recognize the value
       serialized as the NoDataSerializer?

     - Write unit test for read-consistent semantics for key-range,
       key-array, and rangeIterator (PartitionedRangeQueryIterator).
       The test would perform concurrent writes and verify that those
       writes were not visible to the operation.
       
       A stress test variant should also force concurrent overflow
       operations and verify that the index is reading from a
       read-consistent view of the metadata index and hence does not
       see the locator updates.

   - LocalTripleStoreWithEmbeddedDataService

     - Benchmark with owl:sameAs backchainer.

     - (*****) Optimized JOIN that assumes that all indices are local
       within the data service and reads locally on both access paths.
       This would probably be implemented as a AbstractTask and it
       would need to declare access to the indices being used for the
       left and right hand sides of the join.

     - test small and large document sets with and without incremental
       closure:

       -server -Xmx500m -DtestClass=com.bigdata.rdf.store.TestLocalTripleStore -Ddocuments.directory=../rdf-data/metrics/smallDocuments -Ddocuments.ontology=../rdf-data/metrics/metricsOntology_v1.9.rdfs -Dfile=C:/smallDocuments.jnl -DdataLoader.commit=None -DdataLoader.closure=None

   - ScaleOutTripleStore

     - ConcurrentDataLoader

       - ? stagger the entrance of the first few tasks to help stagger
         the nature of the their work.

       - retry long running tasks (map/reduce style).  In fact, this
         already happens because the ClientIndexView times out the
         request to the data service which results in a
         CancelledException and the task is marked as an error and
         then retried.

       - More cleanup.

       - Reconcile with m/r architecture and bigdata repo.

     - (*****) Optimized JOIN for the scale-out triple store.  It
       needs to block up a set of right hand tuples that will be
       joined against data on a given data service and then send those
       tuples to that data service, recieving the results in
       return. It will also have to handle stale locators if the join
       is running in an read-committed mode, but not if it is
       transactional or a historical read.

       Inference is slow due to a large #of small join results.
       Parallel sub-query is probably the way to beat that.  After
       tuning, compare to the purely local unconcurrent line.

       Consider batching a set of rangeQueries together in a single
       operation vs parallel submits.

     - Done. (get() and find() were running as unisolated tasks.)  I
       am seeing a lot of tasks in the concurrency manager for the
       metadata service, but few commits on the live journal.  What
       the heck is the being reported for the metadata service?

   - OverflowManager

      - Could optionally convert from  a fully-buffered to a disk-only
        store  in  order to  reduce  the  memory  footprint for  fully
        buffered  stores,  but in  that  case  this conversion  should
        happen once asynchronous overflow handling was complete.

   - StoreManager / IndexManager

      - Done. Add counter for bytes under management on the
        StoreManager.  I want to see bytes placed under management,
        bytes for stores that have been deleted, and bytes for stores
        that are remaining.  It would also be great to see the bytes
        remaining on the volume where the data are stored in the same
        view.

      - Modify  LRU  to  purge  entries  older than  a  specified  age
        (including an asych  daemon thread to make sure  that they get
        purged even if the LRU is not being touched).  Do this for the
        index segment cache in the IndexManager as well.

      - (*) Better concurrency for openIndex, openStore, getJournal,
        and getIndexOnStore

      - Modify WeakValueCache to use ConcurrentHashMap and support an
        atomic putIfAbsent operation.  This will reduce the latency
        imposed when we need to re-open an index segment from a store.

      - Should recognize a "disk full" situation and shutdown the data
        service cleanly.

   - LockManager

       - Use a WeakValueCache to purge unused resources.  The size of
         its internal map from resource name to resource queue will
         grow without bound on a data service as index partitions are
         split and moved around.  There are notes on this issue in the
         LockManager class.

   - DiskOnlyStrategy
   
     - Done. Lazy creation of the backing file.

     - Done (No performance change for small stores - I still need to
       review the data for large stores.  Of interest, the write cache
       on a large store is nearly never a hit when trying to read a
       record - this suggests that a read cache will be of no
       benefit).  An LRU read cache for records.

       This could be a big win for the DiskOnlyStrategy.  Either make
       this its own layer that can be interposed between the journal
       and the DiskOnlyStrategy or add directly to the
       DiskOnlyStrategy since a read cache is not required for the
       fully buffered modes. Regardless, allow configuration of the
       cache size.

       Also, efficient nextLeaf could improve read performance by
       reducing node reads.

       Write through to the write cache and flush through to the disk.
       On read, test the read cache.  If not found, read the write
       cache, then the disk.  These are simple layering semantics.

       Use an LRU with a capacity of ~5k records.  The records are
       read-only so we do not need to worry about a canonicalizing
       mapping.

       The read cache is specific to a journal.  Each journal gets its
       own read cache.  Historical journals might have a smaller read
       cache capacity, or maybe 2k records is enough for any journal.
       Experiment and find out.

       (***) Look at the effect [host3] on readSecs, on the ratio of
       readSecs to writeSecs, and on IOWait, especially as the size of
       the journal grows.  If the LRU is not paying its way then
       disable it by default.

     - CounterSets

       - Add counters designed to give insight into whether the write
         cache tends to full up completely or only partly before the
         next group command and the #of bytes that tend to be written.
         What I want to understand is whether the cache is too large
         and whether an asynchronous of the cache to the disk would be
         a benefit.

	 *** #bytes/commit (measured delta in offset from commit to
              commit).

	      Also, #flushes / commit - when ~ 1:1 the write cache is
	      at least large enough.

	 Note that writes which would exceed the remaining cache size
         cause the existing cache to be flushed while writes that
         exceed the cache capacity are written directly to the disk -
         the cache itself is always dense in terms of the bytes
         written on the address space.

   - Done. Full text indexing for KB.

       - Done. Analyzers are not thread-safe.

       - Compare performance with lucene and mg4j on indexing and
         search, at least for the RDF DB application.

       - Try out an mg4j integration for an alternative text indexer
         and search.

   - Done.  Either never retry an error task when the queue is likely
     to be full or make the retry itself robust.  Otherwise the CDL
     risks reporting fatal error for a task which could have been run
     successfully.

   - Done (uses a static pool). Provide option to pass in a write
     cache buffer for a temporary store and use that buffer as the
     in-memory store before it overflows onto the disk.

   - *** Batch API for extractor, allowing runs directly against the
         KB.  Form a single prefix-scan query from a sort of all
         simple terms in the document and then piece together the
         phrases from the result.

   - Consider dropping the BasicRioLoader, PresortRioLoader, etc.  All
     of the benefit is in the use of the StatementBuffer.  These
     loaders just obscure the RIO mechanism and make them harder to
     configure.

     The DataLoader might be a utility class.

     The ConcurrentDataLoader is certainly a useful utility class.

   - Quad store.

    - ** Sesame 2 TCK (integration tests)

         (temp fix) You should add the following URL as a maven
	 repository to your maven settings.xml file:

	    http://repo.aduna-software.org/maven2/releases/

    - ***** Two database modes: named-graph mode (quads with all 6
      indices) and provenance mode (3+1 where the context position
      holding a bnode with a 1:1 relationship to the triple and
      therefore serving as a statement identifier and is stored as the
      value associated to the triple in the index; The source
      extension for RDF/XML needs to be supported such that the given
      BNode or URI for the source is correlated to the use of that
      same Resource elsewhere in the same RDF/XML document - we need
      to extend RIO for this).

      1. 3+1

	 - Done. value serializer must be different when using sids

	 - Note: any partition of the term:id index may be used to
           assign term identifiers for bnodes since the bnode ID is
           only required to be distinct, but not stable.

	 - Done. TMStatementBuffer needs to recursively wipe out
           statements using a statement identifier.  this should be
           part of truth maintenance.  when we get the original set of
           explicit statements to be removed we collect their
           statement identifiers and then collect all statements using
           those statement identifiers in either the subject or object
           position and add them to the original set of statements to
           be removed.

	   Done. AccessPath#removeAll() we also need to collect the
	   set of statements using a statement identifier and delete
	   them as well.  this will wind up being a double test for
	   the original set of statements if TM is being used, but it
	   is required when TM is not in use.

	   Done. Do not generate statement identifiers if the SPO is
	   marked as an inference (an optimization).
	   
	   Done. Modify the procedure that actually writes on the
	   statement index to write a ZERO (0L) statement identifier
	   if the statement is (in fact at the time that we examine
	   the statement index) an inference or an axiom.  If an
	   explicit statement is later asserted for the same SPO, then
	   we need to overwrite that 0L with the assigned statement
	   identifier.

	   Done. override ISPOIterator#close() when returning an
	   iterator backed by a temporary store.

	   Done. infinite loop test case fails when sids NOT used. I'm
	   not sure how the test was succeeding before, but the
	   problem was failing to verify that a statement was in the
	   database before adding it to the focusStore.

	   Done. TestTripleStore#removeStatements() has problem with
	   sids.  The problem is that the sid is not getting placed
	   onto the SPO by the unit test, but it highlights the fact
	   that with sids you need to either have the SID on hand
	   before calling removeStatements() or I need to modify the
	   code to resolve the sids as a first step when I compute
	   their fixed point (at which point I could also discard any
	   statements that were not actually in the database).
	   (addStatements already resolves sids so as to always make
	   them consistent).

	   Done. write unit tests for TM cases when using statement
	   identifiers to make metadata statements.

	 - Work through an import scenario from an application that is
           using URIs generated from the {s,p,o} to represent the
           statement identifier.

	 - Survey all of the ways in which reportStatement/3 gets
           called in RDFXMLParser and decide whether or not "context"
           (the variable set based on bigdata:sid) is always correct
           (either "" or the sid) or if there are some uses, such as
           reification, where "context" should be ignored and update
           the calls to reportStatement/3 or reportStatement/4 as
           appropriate.
      
      2. Done. RDF/XML w/ statement identifiers in/out.

         - Reduce to only an "explicit" flag rather than {explicit,
           axiom, inferred} since we can not differentiate between
           constructed statements and inferences.

      3. High level query for reading variable bindings out.

	 - Done. Verify a CONSTRUCT query.

	 - Done. Verify a SELECT query using statement identifiers.
      
      6. Publish on statement level provenance and truth maintenance
         for SPARQL end points.

      7. Defer "named-graph" style quad store for now.

      8. (***) implement prefix compression and apply to the lexicon.
         test it out also on the statement indices and see how it
         stacks up against the "fast rdf key" compression and compare
         with huffman encoding of the decoded long identifiers as
         well.

	 See it.unimi.dsi.fastutil.bytes#ByteArrayFrontCodedList.

	 Note: We should use front compression by default for the
	 nodes of the BTree.  Since the separator keys in the nodes
	 are dynamically determined prefixes, application specified
	 compression generally can't be used.

   - Done.  Correctness testing for scale-out with index partition
     split, move, and join.  See services/StressTestConcurrent. It can
     be parameterized for this purpose.

   - streaming io for block read/write.

     - Asynch IO for the disk only strategy will not have an impact if
       we are doing commits whose size fits within a single write
       cache (10M by default).

     - Use a pool of direct buffers (the same pool that is used for
       the TemporaryRawStore) for the data service and handle all
       block io/out requests with that pool.  this will help about OOM
       problems with "temporary" direct byte buffers.

     - It is possible that we will need to use the TemporaryRawStore
       static buffer pool for all reads from the store (or writes that
       are not via a write cache backed by a direct buffer) since
       otherwise Java could allocate for us, and then fail to
       correctly release, a "temporary" direct buffer. This would
       impose ReentrantLock on all reads....

   - write performance test drivers and run on cluster.

      - rdf concurrent query (rdf lubm is not designed to test with
        concurrent loads).

      - bigdata file system workload (must provide reverse traversal
        iterator to handle atomic append for the key-range partitioned
        indices).

      - (****) Write script to allocate services to nodes.

        - N data services; 1 MDS; 1 LBS; 1 TS, etc.

	- The script needs to start the services on a LOCAL disk on
          each machine (I am currently setup on NAS so that means the
          DISK is REMOTE).  This means replicating the environment
          onto the local host (at least the configuration) and then
          starting the service.  The classpath could be resolved on
          NAS or replicated onto the local host and resolve there. (I
          just need to copy [policy.all,
          bigdata-rdf/src/resources/logging/log4j.properties, and
          bigdata-rdf/src/resources/config/standalone ->
          .../src/resources/config/standalone {create the directory
          path first}].  I could also copy the classpath resources,
          but presumably they will be fetched quickly enough and
          become stable - or maybe not?

	- Need to touch up launch-all after the jini install as well
          as installing from a pre-touched version fixing the
          LD_ASSUME_KERNEL_VERSION bug.

	- (***) Get clusterondemand account.

	- Support downloadable code in the configuration, including a
          an optional security model.

        - Make sure that yum-updatesd does not run on the servers.  It
          absorbs an entire CPU for quite a while.

      - (*******) rdf concurrent data load.

	- Still an annoying problem with the service names as
          displayed by the jini browser....  This may well be an issue
          with failing to expose the interfaces and service classes
          via an http service as downloadable code.
 
============================================================

	Looks like virtuoso is running a clustered triple store!

	http://virtuoso.openlinksw.com/wiki/main/Main/VOSArticleLUBMBenchmark

	http://www.openlinksw.com/weblog/oerling/?id=1336

	http://www.openlinksw.com/weblog/oerling/?id=1335

	http://docs.openlinksw.com/virtuoso/clusterprogrammingsqlexmod.html

============================================================

    - Done. (There was a fence post when the releaseTime was less than
      the first commit time for any journal.) Saw "no data for release
      time" error.  Twice.  This will prevent overflow from succeeding
      since it occurs during synchronous overflow.  I have changed the
      configuration for the data services to set [minReleaseAge = 0],
      which is probably more appropriate for this test, but this needs
      to be debugged.

    - Done. Get the basic overflow tests running.

    - Done. ClientIndexView : retry count exceeded - need to report
      the underlying cause(s)

    - Done. (Added more detailed warnings and made robust to such
      failures) Fence post for DefaultSplitHandler (#ntuples == 0)

    - Done (DataService needed to notify() the LBS during startup).
      Jini client is not reporting counters on either machine.  The
      problem is the client configuration.

    - Done. Dynamically refresh the httpd view for the data service to
      make the counter set for the index manager live.

	Done. Modify to report the counters retained by the
	concurrency manager as they are longer lived.

	Done. Report partition metadata for the view, including the
	checkpoints for the index segments.

	Done. Report stale locators.

	*** I am not seeing StaleLocatorExceptions for read-committed
            views, at least not in the counter set generated by the
            concurrency manager.  Part of the problem is that the fast
            overflow rate is making the views update very quickly.

============================================================

**** The group commit behavior when interrupted during shutdown needs
     to be reviewed with respect to the recent changes to AbstractTask
     and Name2Addr.

ERROR: 397768 pool-1-thread-559   commitCounter=397 com.bigdata.resources.SplitIndexPartitionTask$AtomicUpdateSplitIndexPartitionTask [test_POS#18, test_POS#40, test_POS#41] 0 waitingOnCommit  com.bigdata.journal.WriteExecutorService.groupCommit(WriteExecutorService.java:1102): Problem with commit? : java.lang.InterruptedException
java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1878)
	at com.bigdata.journal.WriteExecutorService.waitForRunningTasks(WriteExecutorService.java:1230)
	at com.bigdata.journal.WriteExecutorService.groupCommit(WriteExecutorService.java:1024)
	at com.bigdata.journal.WriteExecutorService.afterTask(WriteExecutorService.java:655)
	at com.bigdata.journal.AbstractTask.doUnisolatedReadWriteTask(AbstractTask.java:1638)
	at com.bigdata.journal.AbstractTask.call2(AbstractTask.java:1546)
	at com.bigdata.journal.AbstractTask.call(AbstractTask.java:1437)


============================================================

This indicates passing an old locator that could not be found in the
MDI.  I've modified the code to report back the old locator in the
exception.

Caused by: java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:205)
	at java.util.concurrent.FutureTask.get(FutureTask.java:80)
	at com.bigdata.service.MetadataService.splitIndexPartition(MetadataService.java:280)
	at com.bigdata.resources.SplitIndexPartitionTask$AtomicUpdateSplitIndexPartitionTask.doTask(SplitIndexPartitionTask.java:805)
	at com.bigdata.journal.AbstractTask$InnerWriteServiceCallable.call(AbstractTask.java:1829)
	at com.bigdata.concurrent.LockManagerTask.call(LockManagerTask.java:325)
	at com.bigdata.journal.AbstractTask.doUnisolatedReadWriteTask(AbstractTask.java:1605)
	at com.bigdata.journal.AbstractTask.call2(AbstractTask.java:1546)
	... 6 more
Caused by: java.lang.NullPointerException
	at com.bigdata.mdi.PartitionLocator.equals(PartitionLocator.java:231)
	at com.bigdata.service.MetadataService$SplitIndexPartitionTask.doTask(MetadataService.java:502)
	... 10 more

============================================================

Observed once: clearly a timing issue.

ERROR: 62 pool-1-thread-5         com.bigdata.resources.StoreManager$Startup.run(StoreManager.java:895): Problem during startup? : java.lang.IllegalStateException
java.lang.IllegalStateException
	at com.bigdata.resources.ResourceManager.getConcurrencyManager(ResourceManager.java:340)
	at com.bigdata.resources.StoreManager$Startup.start(StoreManager.java:963)
	at com.bigdata.resources.StoreManager$Startup.run(StoreManager.java:888)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:417)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
	at java.lang.Thread.run()V(Unknown Source)

============================================================

1. Run U10, U100 on host3.

2. Write script and run U10, U100, U1000 on host{1,2,3}.

   - Done. pidstat is reporting under /host/client/UUID not
     /host/service/iface/UUID

   - Done. sar/pidstat parsing problem.

   - verify syslogd reporting and configure on host{2,3}.
   
     I need to look further into how the stuff gets logged by syslog
     and what, if anything, needs to be configured for this to work.
     This is only interesting to obtain a combined log of the services
     - and primarily to obtained a combined log of their ERROR level
     messages.

   - Done. reduce log levels (review log4j config).

   - Done. Reduced the branching factor default for the indices as it
     was overflowing the maximum record size.

   - document setup.

   - jini class setup, httpd, and codebase property.
   
   - Done. The various server setups either all need to be copied into
     appropriate locations on the host on which they will run or they
     need to specify a dataDir that is local to the host on which they
     will run, e.g., /var/bigdata/DataServer0

   - Done. verify jini using nio.

   - timestamp service should notify the load balancer.  this will be
     extended to be the transaction service and that will have things
     to report.

   - Startup should be event driven.

      - The server startup for bigdata should be more event driven.
        You should be able to discover jini itself and then any of the
        services in any order.  some services clearly must wait for a
        join (e.g., the client and the data service must wait for the
        timestamp service) while others can come and go as they like
        (if the load balancer is not there then things should run
        anyway but centralized reporting will not work and index
        partition moves will not happen).

      - Done? Modify to have an observable event or callback that
        assigns the service UUID and that indicates when the resource
        manager is running and refactor the LDS and DS startup logic
        to use that to configure the reporting of counters and an
        optional httpd service (at least for the LDS). The relevant
        Jini method is ServiceIDNotify().  For the moment I have
        disabled the httpd for the LDS.

   - Done. review jdk (1.6.0_03), sysstat (8.0.3) {version 7 on
     host{1,2}, solved using rpm -U to update rather than install},
     /etc/hosts, /etc/fstab{/NAS vs /nas}, jini (not really required
     on all hosts since we are bundling the jars) for consistency.
     also emacs install.

   - Done. problem with multicase. solved using unicast to host3.

   - Run multiple clients as well as multiple servers specifying
     nclients=3 and clientNum={0,1,2}

   - (*) Ala log4j, use a set named property model for property
     values.  This will let us warn people when the property value is
     not defined.
   
LDS U10 13.6k

JF host3 U10 7.7k

JF host3 U10 startAll 6.7k

====================
jrockit:

LDS U10 SIDS=true nthreads=10 bufferCapacity=100000 wrkstn : 6.6k

LDS U10 SIDS=false nthreads=10 bufferCapacity=100000 wrkstn : 7.2k

LDS U10 SIDS=false nthreads=10 bufferCapacity=100000 wrkstn 1G : 7.5k

LDS U10 SIDS=false nthreads=20 bufferCapacity=100000 wrkstn 1G : 8.4k

LDS U10 SIDS=false nthreads=30 bufferCapacity=100000 wrkstn 1G : 7.8k

LDS U10 SIDS=false nthreads=20 bufferCapacity=200000 wrkstn 1G : 8.6k

LDS U10 SIDS=false nthreads=20 bufferCapacity=100000 wrkstn 1G noText : 10.8k, 11.6k, 11.3k

LDS U10 SIDS=false nthreads=20 bufferCapacity=200000 wrkstn 1G noText : 11.6k, 12.7k

LDS U10 SIDS=false nthreads=30 bufferCapacity=200000 wrkstn 1G noText : 11.8k

LDS U10 SIDS=false nthreads=40 bufferCapacity=200000 wrkstn 1G noText : 11.9k

LDS U10 SIDS=false nthreads=20 bufferCapacity=300000 wrkstn 1G noText : 12.0k

LDS U10 SIDS=false nthreads=20 bufferCapacity=400000 wrkstn 1G noText : 12.3k

LDS U10 SIDS=false nthreads=20 bufferCapacity=500000 wrkstn 1G noText : 12.7k

LDS U10 SIDS=false nthreads=20 bufferCapacity=300000 wrkstn 1G : 9.1k

LDS U10 SIDS=false nthreads=30 bufferCapacity=1000 wrkstn : 4.5k

jdk 1.6.0_03 -server

LDS U10 SIDS=false nthreads=20 bufferCapacity=200000 host3 2G noText : 14.5k, 14.4k

LDS U10 SIDS=false nthreads=20 bufferCapacity=200000 host3 2G noText localData : 14.6k, 15.0k

LDS U100 SIDS=false nthreads=20 bufferCapacity=200000 host3 2G noText localData : 16.4k
	 Run saved as bigdata-rdf/host3-U100-LDS-countersfinal.xml (13M stmts, 3M terms)

LDS U100 SIDS=false nthreads=20 bufferCapacity=200000 host2 2G noText localData : 16.2k

LDS U1000 SIDS=false nthreads=20 bufferCapacity=200000 host2 2G noText localData : 11.4k
    #terms=32,905,188; #stmts=133,613,894; rate=11412; 68G journal.
    Run saved as: U1000-host2-countersfinal.xml

    - Done. Add moving average w/ and w/o locks.  Perhaps there is a
      queue but it is before the locks are obtained?

      Done. Also add the commit wait time and commit service time
      measures.

    - The LockManager could report the queue size and queue waiting
      time per resource.  This would let us know which resources are
      the bottlenecks.

    - Done. The LockManager could report the total #of waiting tasks,
      which is not showing up anywhere right now.

    - (*) Per-procedure/index/isolation counters {#submitted,
      #completed, task service time (queue waiting times are always
      shared by a queue but lock waiting times and task service times
      can differ by task)}.

(**) Note: Compare the performance for LDS against 16k tps with
     [autoFlush=false] (before sids and before the text indexing).
     Experiment more with buffer sizes, combining writes, etc.

============================================================

Main issues:

============================================================

(*) map/reduce processing.

    (-) If the ConcurrentDataLoader was a map/reduce job then I could
        start the whole thing by running a single master.  I would of
        course have to have map and reduce services running on the
        cluster.

    (-) ...

============================================================

(*) Optimize JOINs

    (Done) LDS optimization (all indices are local, use read-committed
           view).

	   - Might be able to further optimize the closure rounds -
             currently each round is submitted as a separate task.

    (-) Scale-out optimization (essentially unrolling the loops).

    (-) Integrate optimized JOINs with Sesame 2.

    (Done) Generalize the JOIN paradigm so that it can be used for
           things other than RDF.

	   - Explore lexicon + SPO joins.

	   - Explore distinct term scan as a filter on an SPO
             predicate.

    (-) Original LUBM benchmark.

    (-) Modified LUBM benchmark.

	Can this be modified to generate the data dynamically for
	distributed clients?

============================================================

(*) Index and other performance tuning.

    (*) Implement prefix compression algorithm using mg4j.

    (*) Try prefix compression for statement indices to see if I can
	reduce the (de-)serialization times!

    (-) Replace use of "immutable nodes" throughout and introduce the
        default prefix compression algorithm in its place.

    (-) Experiment with higher branching factors on the journal
        indices now that we will overflow periodically.

    (*) Try co-threading the forward index writes for the statements with
	the reverse index writes for the term identifiers.

============================================================

(*) Scale-out performance

    - Estimate parameters for the scale-out model.

      Note: Concurrency and throughput SHOULD NOT increase as indices
      are broken down into index partitions on a single server since
      we are CPU bound in the unisolated index tasks.  However, an
      increase SHOULD be observed with or without index partition
      splits (and with or without overflow) when running on more than
      one host since we have more CPU resources.

      Note: Caching by the disk controller, the OS, and the B+Tree
      keep the IO costs quite low when the depth is small and the
      B+Tree is CPU bound in that regiem.  The B+Tree does become IO
      bound as the depth of the B+Tree increases.

      Find the LDS baseline for U100.

      Find the EF baseline w/ one data service and no overflow for
      U100.

      Find the EF baseline w/ two data services and no overflow for
      U100.      

      Find the intercept for a linear scale-out model using JF with
      two data services (and hence RMI) on U100 w/o overflow.
      
      Find the slope of the linear scale-out using JF where one of
      data services is located on a second machine, still w/o
      overflow.

      Done. (Multicast configuration issue was resolved.)  There is
      still a problem with multicast which is preventing scale-out
      runs.

      Note: Ideally the client will also be distributed but that is
      not critical to proving the point as the client is faster than a
      single host can service.


All runs:

    -DtextIndex=true -DstatementIdentifiers=true -Xmx=2G -Dnthreads=20
    -DbufferCapacity=200000

    Note: EF runs use -DoverflowEnabled=false

    Note: JF runs configure overflowEnabled=false in
    bigdata.properties for DataService4 on host3.

LDS-U100-host3-noText-noSids : 16784 (798910 ms (13m)) 13M stmts.

LDS-U100-host3 : 16783

EF-U100-host3-1DS-noOverflow: 12877

EF-U100-host3-2DS-noOverflow: 11332

JF-U100-host3-1DS-noOverflow: 8528

JF-U100-host3-2DS-noOverflow: 

JF-U100-host23-2DS-noOverflow: 

	*** Why does EF run so much slower than LDS?  Is the overhead
            the MDI?  The cost of setup for the client index
            operations (ClientIndexView vs DataServiceIndex)?
            Removing the existing journal files (try the EF runs with
            the [test] directory pre-deleted)?

      Retest all of the above on U1000 (requires use to use host1 or
      host2 for the additional disk space).

LDS-U1000-host2
EF-U1000-host2-1DS-noOverflow
EF-U1000-host2-2DS-noOverflow
JF-U1000-host2-2DS-noOverflow
JF-U1000-host12-2DS-noOverflow

============================================================

(*) Dynamic index partitioning

    - What cost is associated with overflow processing?

      Evaluate for:

      (a) EF with one data service;

EF-U100-host3-1DS-5M: 5965

	Note: This used a 5M initial extent and 5M maximum extent.
	There were 15 overflows in 46 minutes (a stress test run).
	The initial and maximum extent were configured in the test
	suite code.  Run was successful.  The only reported errors
	were index partition split tasks that were still executing
	when the run was terminated.

EF-U100-host3-1DS: 5922 (this is not the final tally since the client
		         died during shutdown).

        w/ 200M initial extent and 200M maximum extent.

	--------------------

      (b) for JF with one data services with on one machine; 

JF-U100-host3-1DS: 5543

	Note: This run came close to using all RAM on the machine
	80%).  The RAM was mostly going to the sole data service.  In
	fact, the data service RSS was 2G, which is maxed out.

	Note: Some overflow tasks were cancelled! (timeout).  

	Several index partition split tasks began but all were
	cancelled due to timeout.  I have increased the default
	timeout and I will run again.  I have also added counters to
	report failed and cancelled async overflow tasks.

	* Once splits start, look for the overhead of the MDS (there
          is still an MDS overhead since we need to find the locator
          to decide that there are no splits)

	* Look at the LBS host and service scores.

	* Look for index _moves_.

	* Network counters begin to become interesting.

	--------------------

      (c) for JF with two data services with on one machine; and

JF-U100-host3-2DS:

	--------------------
      
      (d) for JF with two data services on two machines.

JF-U100-host23-2DS:  ******** RUNNING NOW *********

	Note: I am only running a single client for this test.  The
	client is on host3.  This should be Ok since we are data
	service bound.  If the client can not keep up with the data
	services on two machines then that's good news :-)

	--------------------

      Choose runs where we have the data w/o overflow from above.

      Do at least one run with post-facto validation turned on.

      Note: It is important to also save the service nohup.out files
      since they show interesting data about the asynchronous index
      partition overflow tasks and any errors reported by the service
      during such tasks (those errors do not make it back to the
      clients since the tasks are run by the services themselves).

    --------------------

    Note: It is best to run EF with one data service to test index
    partitioning questions and JF to test marshalling, robustness, and
    index move questions.

    Running JF U100 on host3 right now to get some data on queue
    behavior with dynamic index partitioning.  Some questions are:

    - Change the default split point and verify build and split
      behavior on both the workstation and the server using U100 and
      one data service.

    - Do I need to increase the client timeout for Jini?  It seems
      that I probably do.  Perhaps double it to 40 seconds?  Or
      perhaps make the default much longer (1 minute, 5 minutes or
      infinite) since tasks could be queued up.

    - Are good decisions being made with regard to build and split of
      index partitions?

    - Are good decisions being made with regard to index partition
      moves?

    * Configure to hold more indices open per data service.  This
      should be done in the bigdata.properties files for each data
      service.

    * Observe the metadata service response time and verify that it
      does not become a bottleneck since the current implementation is
      NOT caching.

    * Verify that we use read-committed or read-historical operations
      whenever possible (e.g., for asynchronous overflow processing,
      joins, metadata service reads, etc).

    - Should report the #of tasks, action on each index, and the
      duration of overflow processing for each event, but that event
      oriented data does not fit well within the counters model.

(*) Load-balancing

    * Watch the load balancer and see how host utilization and service
      response time change as the run progresses, for different #of
      client threads, and as index splits occur, and as index moves
      occur.

    * Review the LBS host and service scores.  Can they predict host
      and service load well enough to move index partitions around
      without the host-based physical disk counters under linux?

    - Try U10000 reading the data from NAS with 2 clients, 10 threads
      each and 2 or 3 servers.  See if scale-out holds as we increase
      the data size.  The point of comparison is the 1B run that we
      did on server2 (single host, non-scale-out architecture,
      non-concurrent load).

    - Delay start of some data services, either on each host or on one
      of the hosts and then see how the load changes once we start
      additional data services (this could be expanded into a variety
      of hardware add and hardware fail tests).

============================================================


- MetadataService

    - Caching.

       - Collect statistics and monitor the MDS as we split the
         indices.

       - Consider caching to reduce RMI for splitting operations
         across index partitions.

- Scale-out scripts

    - Script to collect nohups, config files, and counters in a
      directory and tarball for post-mortem.

- Service and host statistics:
      
   - **** There are no majorFaultsPerSec or percentFreeDiskSpace
          numbers on a per-host basis so only defaults are being used
          for those values when computing the host score.  I need to
          write a new SAR collector to get those data.

    ** The per-host physical disk counters for linux are not being
       collected, including the major page faults per second for the
       _host_.  This is one of the primary clues so we need that.

       Write the Sar, iostat, or vmstat utility to collect these data.
       This is a bit more complex under Linux since the data are
       reported by device and the relationship of the devices to the
       file system should be explicated.

       Use [df] to report the % free space remaining?  There should be
       one value for the logical disk (perhaps), and there should be a
       report of the % free space remaining on the volume on which the
       dataDir is located and the volume on which the tmpDir is
       located.

       Note: The StoreManager also reports the free space on the
       volume for both the data dir and the temp dir.

   *** IP Addr shows up for host2 but not host1 or host3.

       192.168.20.27 is showing up as a host for DataServer3 but no
       host statistics are being reported for that IP addr (they are
       reported for the hostname instead) with the result that the IP
       addr is getting the default values for the metrics used to
       compute the host scores.

       There is still the problem with how that IP addr is getting
       reported and with whether or not any services are understood to
       be running on that host (they are not).
       
       *** A host without services running on it effects the ranking
           of the hosts and the host with the highest score. Does it
           also effect any recommendations made by the LBS?

   - Done. Modified host score to interpret high IO Wait as high
     utilization.

   - Done (mostly). The counter names need to be symbolic to avoid
     edits causing counters to not be found during analysis.

- Overflow processing

   *** If overflow processing is taken so long then there is a
       problem.  Perhaps there needs to be an alternative that lies
       between an index partition "copy" and a full compacting build,
       e.g., an incremental build that lets us discard the old
       journal.  It would generate an index segment having just the
       last committed state for the BTree absorbing writes for that
       index partition on the old journal.  The view of the index
       partition on the new journal would be updated when the task was
       complete.  A full build would have the effect of combining the
       history from several such incremental builds.

       Likewise, we may need to trade off how many splits we perform
       choosing to do incremental builds instead to keep down the
       total processing time/costs for asynchronous overflow
       processing.

       Can an analysis of the queue concurrency with locks held help
       decide whether some index partition should be moved to another
       data service?

   - Done. Asynch overflow needs to report the index and task for each
     failure as part of the exception.

   - Done. Metadata service needs to report the scale-out index name
     (or the index partition name) for "No such locator" errors.

   - Done. Add the createTime to the live journal counters.

   - Done. Move the LBS counters from /var/log/bigdata to
     /var/bigdata.  Update scripts in CVS and on the server and the
     script documentation.

   * It would be useful to have AND as well as OR semantics for
     "filter=".

   * Change the TITLE for the httpd counter view to the hostname and
     the last component of the path, e.g.:

	  hostname  ... serviceIFace ... lastComponent

     Or use the last 60 characters of the path, etc.  The point is to
     have titles that are somewhat easier to figure out.

   * It could be useful to have a view by serviceIface rather than
     host.  This could be assembled dynamically by a scan of the
     services across the hosts.

   - The IndexManager view of the LBS needs to aggregate some key
     statistics by index across the index partitions including the #of
     index entries on the data service and the time spent on the index
     (perhaps broken down by IO, serialization, key search, etc., but
     definately the aggregate time).  I am just not getting enough
     information from this view and digging down makes it too
     difficult to get the gestalt state of the indices without copying
     a correlated view of the counters into Excel.

   - Done. Review the per-service scores, but I really need to have
     more than one service on a host for this.  It should probably be
     using a response time measure, e.g., averageQueuingTime.

   - Done (problem was timestamp parsing since only the time of day
     was being reported, not the UTC time - it now uses the system
     clock). Now I am not seeing any host CPU scores aggregated by the
     load balancer.

   - Done. The normalized [score] is not being computed for hosts or
     services (it is always zero).

============================================================

Index allocation:

    The initial index allocation appears to assign much more of the
    effort to host3.  Either try random assignment or reconsider the
    2-host assignment behavior.

log files:

    The log files do not interleave the logs by timestamp.  Clearly
    there is no way to do this exactly in a distributed system.
    However it might be done better if it all went through syslog.
    Rather than an elapsed ms (or in addition) I need the UTC time in
    a field that I can readily identify.  With that I could even merge
    sort the logs together.

DataServer3:

The problem here is that there is no entry under the leftSeparatorKey
for the partition in the MDS.  I am not clear why.  I have added a
test for this condition and an exception which will provide more
information.

Caused by: java.lang.NullPointerException
	at com.bigdata.mdi.PartitionLocator.equals(PartitionLocator.java:231)
	at com.bigdata.service.MetadataService$MoveIndexPartitionTask.doTask(MetadataService.java:803)

What led up to this was:

testSPO#0	 = willBuild(name=testSPO#0)
testterm2id#0	 = willSplit(name=testterm2id#0)

and

testSPO#0	 = willMove(name=testSPO#0,target=ba2214f9-b220-43f5-9f50-8f1095ce3ec6)
testterm2id#1	 = willBuild(name=testterm2id#1)
testterm2id#2	 = willBuild(name=testterm2id#2)

and the exception was for the move.

--------------------

DataServer4:

Caused by: java.lang.RuntimeException: Expected

oldLocator={ partitionId=1, dataServices=[ba2214f9-b220-43f5-9f50-8f1095ce3ec6], leftSeparator=[], rightSeparator=null}, but
    actual={ partitionId=0, dataServices=[2eed9964-6c1c-40ee-8b3f-ceb5b30278d5], leftSeparator=[], rightSeparator=null}
	at com.bigdata.service.MetadataService$SplitIndexPartitionTask.doTask(MetadataService.java:519)

This is what was happening at the time.

__global_namespace_index#0	 = wasCopied(name=__global_namespace_index#0)
testOSP#1	 = willBuild(name=testOSP#1)
testOSP#2	 = willBuild(name=testOSP#2)
testPOS#1	 = willBuild(name=testPOS#1)
testPOS#2	 = willBuild(name=testPOS#2)
testSPO#1	 = willSplit(name=testSPO#1)
testid2term#0	 = willBuild(name=testid2term#0)
testjust#0	 = wasCopied(name=testjust#0)
testsearch#0	 = willBuild(name=testsearch#0)

So this looks like a cascade of the move problem.  So we need some
compensating action for the failed move since it seems to have been at
least partly effective even through there was an exception thrown.

--------------------

This was in the client's stack trace.  i've added the index name to
the stack trace, but again it looks linked to the problem with the SPO
MOVE failure.

Caused by: java.lang.NullPointerException
	at com.bigdata.service.ClientIndexView.splitKeys(ClientIndexView.java:1751)
	at com.bigdata.service.ClientIndexView.submit(ClientIndexView.java:877)
	at com.bigdata.rdf.store.SPOIndexWriter.call(SPOIndexWriter.java:280)
	at com.bigdata.rdf.store.SPOIndexWriter.call(SPOIndexWriter.java:73)
	... 5 more

============================================================

   *** Snapshot release

       - web app for row store & how to

       - how to for Sesame 2.

       - publish current javadoc.

   - Monitor the [executorService] queue for {Journal, TemporaryStore}
	 
   - ScaleOutTripleStore {LDS, EDS, JDS}

         Note: You CAN place indices onto specific data services
         running on a set of machines and set [enableOverflow :=
         false] such that the indices never become partitioned. In
         that case you can have optimized joins for some relations on
         one data service and for other relations on another data
         service. E.g., locating the statement indices for the triple
         store on one data service, the lexicon on another, and a repo
         on a third. This will give very good performance for Query
         and Truth Maintenance since the JOINs will be mostly
         executing against live index objects.

       - Verify serialization of the JoinNexusFactory, setup of the
         JoinNexus on the data service, and distributed execution.

       - Axioms need to be serialized for the DoNotAddFilter for
         scale-out joins.  (We could serialize the Axiom model
         identifiers and the cache it for a given db instance, but
         serializing the axioms themselves is not much effort - it is
         an array of ~200 SPOs.)

       - Export a proxy for IChunkedOrderedIterator when using an
         async process, but if LT 1000 results then fully buffer
         instead of creating a proxy (have to wait to figure that
         out).

       - *** The LDS is running each round as a separate procedure
         submitted to the concurrency manager.  Why not run all rounds
         as a single procedure?

	 Ah.  This was being done so that we could update the read
	 behind point after (or before) each round of closure.
	 However, we do NOT need to do that if we are using
	 UnisolatedReadWriteIndex.

	 If I go this way, then need to change AbstractRelation to use
	 the UnisolatedReadWrite index in this case as well.  However,
	 that could decrease performance if there are point tests
	 since the locks would be requested per point test.

	 *** Perhaps this could be an option to the AbstractTask?
   	     I.e., whether or not the procedure itself will use
   	     concurrent threads and therefore needs the unisolated
   	     indices to be thread-safe.

       - Modify TM to remove constraints on scaling imposed by the use
         of fully buffered iterators to avoid concurrency problems.

	 Consider the use of magic sets as an alternative to "fixing"
	 justifications tracking for scale-out.

       - BlockingBuffer - tune for effective chunk sizes.  The buffer
         sizes can be easily configured by the IJoinNexusFactory, as
         can parameters for timeouts, etc. for the blocking buffer's
         iterator.

Translate Sesame2 queries to Rules dynamically so that they can
execute on the native rule engine.
   
   - allow rules with an empty head (for query).

   - verify that query is against the last committed state of the kb.

   - simple conjunctive query.

   - optionals (requires a change to the rule evaluation plan since a
     zero range count for an optional does not mean that the rule will
     have no solutions.)

   - resolution of terms to term ids and visa versa using JOINs?

   - filters for various kinds of things, especially those that can be
     computed directly from the bit markings on the term identifiers.

   - filters that require JOINs to the lexicon and ordered scans by
     datatype literals.

   * LUBM performance numbers.

- Snapshot release:

   - Move the ant build to the bigdata module and also the startup kit
     and its documentation.

============================================================

x. misc tests, performance problems and improvements for the RDF DB.

  - Finish the TestDefaultResourceLocator

  - asReadCommittedView() is broken for the LocalTripleStore.  This
    effects the BigdataSail and
    com.bigdata.rdf.store.TestLocalTripleStoreTransactionSemantics.
    The problem is that the BTrees are not truely read-committed
    views.  They are views as of the last commit time.  However
    asReadCommittedView() is a singleton so the view never advances.

    Could have ReadCommitted BTree variant that was notified of (or
    simply noticed) commits and re-loaded itself from the new
    checkpoint.  This could be done in getRoot() and in a few
    additional methods that return fields (index metadata, entryCount,
    etc).

    Could also lookup the last commit time (or the last closure time)
    and use that timestamp rather than specifying READ_COMMITTED.

  - (defer) Optimize by assigning variables in a rule a positional
    index and use a long[] for bindings for SPORelation self-joins.

  - Performance for unrolled joins?

  - Refactor the bulk filter, statement buffer, bulk complete,
    etc. operations as stackable, chunked, ordered, filtering
    iterators using some stiterator extensions similar to what I have
    done for the B+Tree.

  - Replace IElementFilter with ITupleFilter or ITupleFilter[].
       
    This will allow us to use the prefix scan and distinct term scans
    directly from an IRule by specifying the appropriate predicate
    filter(s)!

  - Verify that the distinct term scan can correctly advance across
    index partition boundaries.  One way to do this is by setting a
    very small split point and then loading a known data set,
    computing its closure, and comparing it to ground truth.

  - performance testing for owl:sameAs processing at scale.

  - performance testing for distinct term sca

  - TruthMaintenance performance should be vetted.  Handling TM at
    scale is its own issue and might be best solved using: (a) a magic
    sets integration; and (b) periodic map/reduce style updates with
    closure.  (Or database at once closure on a new data set, which is
    similar to how scale-out text indices are built.)

- Performance testing of Fast vs Full and then set the default closure
  method appropriately.

  Re-evaluate the effect of the buffer capacity for the chunked
  iterators.  When we are running parallel rules and asynchronous
  iterators it could make sense to use much smaller buffers (10k vs
  200k).

* Test concurrent data loader on multiple servers

  - with and without closure

  - with and without overflow enabled (e.g., with and without
    autopartitining)

* Verify TM.

* Sesame 2 TCK integration.

============================================================

Priority list:

x. *** Get 1-2B scaling numbers on the DPP cluster for DaveL and post
       at http://esw.w3.org/topic/LargeTripleStores.

       * Benchmark requirements to DaveL and DanH.

       * Increase the journal chunk size.

       * Verify correctness for U1 before beginning server-based runs.

       - Note the time writing on the statement indices during closure
         and compare it with the time reading on those indices.  What
         takes more time?  For which rules?

       * Modify test harness for concurrent data load, or simply load
         data via our own test harness but with the same set of
         options for the triple store (justify=false, testIndex=false,
         statementIdentifiers=false).

       First, do a pure bulk load - triples only.

	    - Without compression this will have to be on more than
	      one machine or using NAS.

	    - Without overflow processing the triple store will take
              up more space due to the WORM nature of the store.

	    - leading prefix compress can help to compenate for this,
              but it has not been implemented yet.

       Then compute the database at once closure.

	    - why does serial rule execution do better?  Will this
              also have an impact on concurrent query performance?

	    - add some of the missing rules for LUBM competeness,
              e.g., owl:inversePropertyOf, etc.

       Then run LUBM benchmarks (rewrite Sesame2 queries to native
       JOINs).

	    - Update our Sesame disto since they have been working on
              their optimizer.

       Do this for U100, U1000, and U8000 on one machine and then for
       the same data sets on three machines using MONOLITHIC indices.
       Put the statement indices on one host (make sure that optimized
       joins are enabled) and the lexicon on another.  The full text
       index can be on a third.  Read the source data from NFS.

       - Jini-aware iterator required for the asynchronous distributed
         join iterator (and even for the special case where the
         statement indices are co-located so we can do optimized
         joins, which applies here).

       - Once I have these numbers, go back and try again to work
         through dynamic partitioning, load balancing.

       - Compute the database-at-once closure once the data are safely
         loaded (verify!) using dynamic partitioning, run some LUBM
         benchmarks.  (This will force validation of the scale-out
         joins, including serialization, the async iterator, and
         efficiency).

       - Then run the LUBM benchmarks again against the distributed,
         key-range partitioned indices.

x. *** unroll loops for faster distributed joins.

   There is a strong relationship between a byte[][] keys iterator
   such as the {@link PrefixFilter} and an unrolled JOIN operator.
   This is especially true when the unrolled N is small, since it is
   cheap to send the tuples that we want to JOIN to the target data
   service.

x. Full text index performance.

   - Do not re-index terms found in the forward index?  This is
     definately faster but it lacks the robust guarentee of the
     unisolated write pattern (all clients write on both all indices
     to guarentee consistent index states).

4. performance testing for data load and closure

   try w/o UnisolatedReadWriteIndex 

   alibaba: 7/31

   w/ text index and sids, no closure, DPP2 workstation.
   Loaded 1 resources: 45655 stmts added in 8.375 secs, rate= 5451, commitLatency=250ms

   w/o text index or sids, no closure, DPP2 workstation.
   Loaded 1 resources: 45655 stmts added in 6.485 secs, rate= 7040, commitLatency=265ms

   w/o text index or sids, database-at-once closure, forceSerial=false, DPP2 workstation.
   Loaded 1 resources: 45655 stmts added in 6.375 secs, rate= 7161, commitLatency=203ms
   Computed fixed point: program=fullForwardClosure, rounds=3, elapsed=11922ms

   w/o text index or sids, database-at-once closure, forceSerial=true, DPP2 workstation.
   Loaded 1 resources: 45655 stmts added in 6.516 secs, rate= 7006, commitLatency=234ms
   Computed fixed point: program=fullForwardClosure, rounds=1, elapsed=3468ms

   D620 after loading several other files.
   WordnetN+S:  273681 stmts added in 15.407 secs, rate= 17763, commitLatency=266ms
   Alibaba:      45655 stmts added in 1.672 secs, rate= 27305, commitLatency=172ms
   NCIOncology: 464841 stmts added in 26.187 secs, rate= 17750, commitLatency=266ms

   * Verify correct handling migration of elements from the
     Unsynchronized buffer to the target buffer.
   
   * Hypothesis for poor performance when using parallel rule
     execution is lock contention for the buffer on which the
     solutions are being written by the rule set (or possible,
     contention for the index locks).  If true, then there should be
     less effect for the fastClosure method since that is mostly a
     sequential program (it has a few fix point steps).

     One solution is to give each thread (this equates to each rule
     instance that is being executed) its own local buffer (100s to
     1000s of solutions for the local buffer) and to flush those
     buffers onto the shared buffer when they overflow.  Make sure
     that the per-rule buffers are not larger than the global buffer,
     and probably not larger than 1/10 of the global buffer.

     When the local buffer is flushed, we need to acquire and hold a
     lock on the shared buffer as 100s to 1000s of solution references
     are copied (and to flush if this would overflow the shared
     buffer).  This means that we can't use the BlockingQueue since it
     will always acquire a lock per add().

     A variant is to have the BlockingBuffer collection ISolution[]s.
     That way each local buffer is copied in its entirety into the
     next ISolution[] and we only acquire a single lock.

     Also make sure that there is no lock contention when flushing the
     shared buffer during execution.  This is possible since
     concurrent threads are still writing on the shared buffer.  (It
     is not possible when the shared buffer is flushed after a round
     since there are no threads writing on the buffer at that time.)

   - Done. Details missing for fast closure programs (total and
     details).

x. GOM style "rows" for IRelation yeilding schema flexible JOINs.
   Could even just use GOM for this, but I would also have to extend
   JOINs to link vs attribute JOINs and make some decisions about
   object state caching, invalidation and update (only applies if you
   want an updatable view of the object).
   
   There is also a strong relationship to the solutions generated by
   the rules, which are basically binding sets.

x. Map/reduce harvest pipeline and info arch changes w/ MikeP towards
   open sourcing the "web 3" framework.

   *** FileVersionDeleter is not quite correct yet.  Make sure that we
       have good tests for the fence posts on this one.

   *** Text indexer for the repo.

       - define a schema property for the text index.  if the property
         exists for a file then index the file when it is written
         (only file at once, not block-based).

       - perf test the text indexer.

       * Handle "delete" from the full text index.  We need to do this
         when a document is deleted whose indexText property was set.

	 One solution is to filter the results by whether the document
	 is still in the file metadata index.  That might not be too
	 bad.

       - consider alternative integration using mg4j and lucene for
         text indexing.

       * MetadataSchema#IndexText may be too blunt a device.  We may
         want to specify a variety of metadata for the text indexer,
         including the URI of the source document, the text index to
         be used (there can be several, each corresponding to a
         collection), tokenization preferences (local to the text
         index I guess), etc.

       - Focus on two CONOPS: incremental updates (crud) and map /
         reduce style index builds (bulk write).   

x. ZooKeeper integration for resource lock manager?

============================================================

   - Bulk loads on the DPP (U100, U1000, U8000) and LUBM query tests.

      - Is the closure really at a fixed point?  Re-load and re-close
        on U1 shows a non-zero (2439) mutation count.  rdfs02 (12) and
        rdfs09 (2421) are not at a fixed point.  However, this is with
        the fast closure program, so I could believe that re-execution
        of fast closure might produce additional entailments.
	
	Look into this with MikeP.

      - Unroll joins then distribute joins.  w/ MikeP

      - cache range counts (and maybe more) for historical access
        paths.

      - Test w/ Sesame 2 custom lubm benchmarking extensions - look
        for other places where we can boost performance.

      * The LDS should be able to run the entire closure program as a
	single task on the concurrency manager -- refactor until it
	does!
	
	LDS U10 Load rate w/o closure on d620 with 10 threads and 100k
	statement buffer.  Not bad.

	Database: #terms=315059, #stmts=1272953, rate=12355.0 in 103031 ms.

      - TempTripleStore use and abuse:
 
        * add ITemporaryStoreFactory, add same to IJoinNexus, add as
          argument to {@link BackchainAccessPath} (this may wind up as
          an IIndexManager feature at some point).

        * Fix TruthMaintenance#applyExistingStatements() - should use
          bulk filter not point tests!

        * Closure for StatementIdentifers should reuse the same
          TemporaryStore and close() vs closeAndDelete().

        * TruthMaintenance should reuse the same TemporaryStore (at
          least for a given closure) and close() vs closeAndDelete().

      * The EDS and JDS should be able to run with overflow disabled
        and the SPO indices located on the same data service just as
        efficiently as the LTS or LDS.

        It would be nice if we could constrain the placement of the
        SPO indices onto a host and run with (much of) the same
        efficiency as the LTS or LDS but with overflow.  There will be
        some cost if the MDS is remote as the #of partitions grows.

      - Make sure that the SAIL query is against the last commited
        state of the store.

      - Change the SAIL setup to use the Locator mechanism.

      - Add LUBM factory variants for EDS and JDS.  LDS and EDS must
        handle the federation create.  For JDS, the federation should
        pre-exist and the client just connects.  Open and close for
        JDS is just connect and disconnect.  For LDS and EDS, we also
        just connect and disconnect, but that has the semantics of
        shutting down the federation when we disconnect.

      * Examine the pure access path read rate for LTS, LDS, EDS, and
        JDS.  This is query6 and query 14.  Both queries produce a lot
        of tuples.  Tune up the access path read until it is good on
        all of these configurations.

      * JDS

        - Jini smart proxy iterator and then test scale-out joins
          (EDS, JDS).  note that EDS does not use RMI and therefore
          does not serialize anything while JDS requires that we cross
          machine boundaries with RMI.

	- Can use the ConcurrentDataLoader with one host directing the
          loads using the LUBM harness or can use the
          ConcurrentDataLoader directly and have multiple machines
          loading concurrently (something that will become a m/r job
          eventually).

	- Can test a single host running queries easily enough.

	- Testing concurrent query under load will require more
          adaptation (it should be a m/r job).

   - (Various MikeP)

     - Setup some test data w/ owlSameAs and tune the sameAs
       backchainer and the type resource backchainer.

     * Specialize type resource for the case when s is not null
       to do less work.
     
     * Make the backchainer run async and expose an iterator that the
       caller reads on to reduce latency.  right now we have to wait
       for the complete set of sameAs entailments before the client
       can read the 1st triple from the iterator.
     
     * Consider a rewrite of the owl:sameAs backchainer as a rewrite
       of the statement pattern in the query into 3 statement patterns
       that use the native rule execution to accomplish the same ends
       as the owl:sameAs backchainer.

     * Handle inverseOf and TransitiveProperty.

     * Are any additional owl axioms required for LUBM?  Are any owl
       axioms required?	  

     * Is there any information in the ontology that could be used to
       eliminate or otherwise optimize some joins?

     * Consider when there is an unbound s or o for the owl:sameAs
       backchainer whether we can fix point a chunk of subjects (or
       objects) at a time, rather than fix pointing one at a time.

============================================================

Probable concurrency problem.  I suspect that the iterator needs to
re-seek since there has been an intervening writer on the B+Tree.

Caused by: java.lang.RuntimeException: De-serialization problem: addr={nbytes=232,offset=9492165} from store=C:\DOCUME~1\BRYANT~1\LOCALS~1\Temp\bigdata44921.tmp : cause=java.lang.RuntimeException: Child is not persistent: index=6
	at com.bigdata.btree.AbstractBTree.readNodeOrLeaf(AbstractBTree.java:2460)
	at com.bigdata.btree.Node.getChild(Node.java:2116)
	at com.bigdata.btree.ChildIterator.next(ChildIterator.java:163)
	at com.bigdata.btree.ChildIterator.next(ChildIterator.java:1)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:59)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:56)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:56)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:58)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at com.bigdata.btree.AbstractNode$PostOrderEntryIterator.hasNext(AbstractNode.java:657)
	at com.bigdata.btree.ResultSet.<init>(ResultSet.java:896)
	at com.bigdata.btree.filter.ChunkedLocalRangeIterator.getResultSet(ChunkedLocalRangeIterator.java:123)
	at com.bigdata.btree.UnisolatedReadWriteIndex$ChunkedIterator.getResultSet(UnisolatedReadWriteIndex.java:700)
	at com.bigdata.btree.filter.AbstractChunkedTupleIterator.rangeQuery(AbstractChunkedTupleIterator.java:305)
	at com.bigdata.btree.filter.AbstractChunkedTupleIterator.hasNext(AbstractChunkedTupleIterator.java:461)
	at com.bigdata.btree.FusedTupleIterator.hasNext(FusedTupleIterator.java:197)
	at cutthecrap.utils.striterators.Resolverator.hasNext(Resolverator.java:48)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at com.bigdata.striterator.ChunkedWrappedIterator.hasNext(ChunkedWrappedIterator.java:162)
	at com.bigdata.relation.rule.eval.LocalNestedSubqueryEvaluator.apply1(LocalNestedSubqueryEvaluator.java:189)
	at com.bigdata.relation.rule.eval.LocalNestedSubqueryEvaluator.call(LocalNestedSubqueryEvaluator.java:136)
	at com.bigdata.relation.rule.eval.RunRuleAndFlushBufferTask.call(RunRuleAndFlushBufferTask.java:47)
	at com.bigdata.relation.rule.eval.RunRuleAndFlushBufferTask.call(RunRuleAndFlushBufferTask.java:1)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
Caused by: java.lang.RuntimeException: Child is not persistent: index=6
	at com.bigdata.btree.AddressSerializer.putChildAddresses(AddressSerializer.java:70)
	at com.bigdata.btree.NodeSerializer.putNode2(NodeSerializer.java:517)
	at com.bigdata.btree.NodeSerializer.putNode(NodeSerializer.java:475)
	at com.bigdata.btree.AbstractBTree.writeNodeOrLeaf(AbstractBTree.java:2345)
	at com.bigdata.btree.AbstractBTree.writeNodeRecursive(AbstractBTree.java:2235)
	at com.bigdata.btree.DefaultEvictionListener.evicted(DefaultEvictionListener.java:111)
	at com.bigdata.btree.DefaultEvictionListener.evicted(DefaultEvictionListener.java:1)
	at com.bigdata.cache.HardReferenceQueue.evict(HardReferenceQueue.java:273)
	at com.bigdata.cache.HardReferenceQueue.append(HardReferenceQueue.java:235)
	at com.bigdata.btree.AbstractBTree.touch(AbstractBTree.java:2122)
	at com.bigdata.btree.AbstractNode.<init>(AbstractNode.java:321)
	at com.bigdata.btree.Leaf.<init>(Leaf.java:159)
	at com.bigdata.btree.BTree$NodeFactory.allocLeaf(BTree.java:1197)
	at com.bigdata.btree.NodeSerializer.getLeaf(NodeSerializer.java:930)
	at com.bigdata.btree.NodeSerializer.getNodeOrLeaf(NodeSerializer.java:415)
	at com.bigdata.btree.AbstractBTree.readNodeOrLeaf(AbstractBTree.java:2456)
	at com.bigdata.btree.Node.getChild(Node.java:2116)
	at com.bigdata.btree.ChildIterator.next(ChildIterator.java:163)
	at com.bigdata.btree.ChildIterator.next(ChildIterator.java:1)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:59)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:56)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:56)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Appenderator.hasNext(Appenderator.java:52)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at cutthecrap.utils.striterators.Expanderator.hasNext(Expanderator.java:58)
	at cutthecrap.utils.striterators.Striterator.hasNext(Striterator.java:55)
	at com.bigdata.btree.AbstractNode$PostOrderEntryIterator.hasNext(AbstractNode.java:657)
	at com.bigdata.btree.ResultSet.<init>(ResultSet.java:896)
	at com.bigdata.btree.filter.ChunkedLocalRangeIterator.getResultSet(ChunkedLocalRangeIterator.java:123)
	at com.bigdata.btree.UnisolatedReadWriteIndex$ChunkedIterator.getResultSet(UnisolatedReadWriteIndex.java:700)
	at com.bigdata.btree.filter.AbstractChunkedTupleIterator.rangeQuery(AbstractChunkedTupleIterator.java:305)
	at com.bigdata.btree.filter.AbstractChunkedTupleIterator.hasNext(AbstractChunkedTupleIterator.java:461)


------
