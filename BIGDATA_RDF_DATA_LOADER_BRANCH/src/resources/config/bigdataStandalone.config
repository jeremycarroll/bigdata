import net.jini.jeri.BasicILFactory;
import net.jini.jeri.BasicJeriExporter;
import net.jini.jeri.tcp.TcpServerEndpoint;

import net.jini.discovery.LookupDiscovery;
import net.jini.core.discovery.LookupLocator;
import net.jini.core.entry.Entry;
import net.jini.lookup.entry.Name;
import net.jini.lookup.entry.Comment;
import net.jini.lookup.entry.Address;
import net.jini.lookup.entry.Location;
import net.jini.lookup.entry.ServiceInfo;
import net.jini.core.lookup.ServiceTemplate;

import java.io.File;

import com.bigdata.util.NV;
import com.bigdata.journal.BufferMode;
import com.bigdata.jini.lookup.entry.*;
import com.bigdata.service.IBigdataClient;
import com.bigdata.service.jini.*;
import com.bigdata.service.jini.lookup.DataServiceFilter;
import com.bigdata.service.jini.master.ServicesTemplate;
import com.bigdata.jini.start.config.*;
import com.bigdata.jini.util.ConfigMath;

import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.data.ACL;
import org.apache.zookeeper.data.Id;

// imports for various options.
import com.bigdata.btree.IndexMetadata;
import com.bigdata.rdf.sail.BigdataSail;
import com.bigdata.rdf.spo.SPORelation;
import com.bigdata.rdf.spo.SPOKeyOrder;
import com.bigdata.rdf.lexicon.LexiconRelation;
import com.bigdata.rdf.lexicon.LexiconKeyOrder;
import com.bigdata.rawstore.Bytes;
import java.util.concurrent.TimeUnit;

/*
 * This is a sample configuration file for a bigdata federation using
 * jini and zookeeper.
 * 
 * Note: This file uses the jini configuration mechanism.  The syntax
 * is a subset of Java.  The properties for each component are grouped
 * within the namespace for that component.
 *
 * See the net.jini.config.ConfigurationFile javadoc for more
 * information.
 *
 * @todo should generate a JiniClient.config file as an output. You
 * can use this file, but the contents can be pruned down quite a bit
 * for a standard client.  Or we could store the config in zookeeper
 * and then you just need the federation name and list of host:port
 * pairs for zookeeper.
 */

/*
 * A namespace use for static entries referenced elsewhere in this
 * ConfigurationFile.
 */
bigdata {

    /**
     * The name for this federation.
     *
     * Note: This is used to form the [zroot] (root node in zookeeper
     * for the federation) and the [serviceDir] (path in the file
     * system for persistent state for the federation).
     *
     * Note: If you will be running more than one federation, then you
     * MUST use unicast discovery and specify the federation name in
     * the [groups].
     */
    static private fedname = "E:\\lubmFed";

    // The default root for all persistent state.
    static private serviceDir = new File(fedname);

    /*
     * Example cluster configuration.
     *
     * Data services are load balanced.  Index partitions will be
     * moved around as necessary to ensure hosts running data
     * service(s) are neither under nor over utilized.  Data services
     * can be very resource intensive processes.  They heavily buffer
     * both reads and writes, and they use RAM to do so.  They also
     * support high concurrency and can use up to one thread per index
     * partition.  How many cores they will consume is very much a
     * function of the application.
     *
     * Zookeeper services use a quorum model.  Always allocate an odd
     * number.  3 gives you one failure.  5 gives you two failures.
     * Zookeeper will sync the disk almost continuously while it is
     * running.  It really deserves its own local disk.  Zookeeper
     * also runs in memory.  Since all operations are serialized, if
     * it starts swapping then performance will drop through the floor.
     *
     * Jini uses a peer model.  Each service registers with each
     * registrar that it discovers.  Each client listeners to each
     * registrar that it discovers.  The default jini core services
     * installation runs entirely in memory (no disk operations, at
     * least not for service registration). A second instance of the
     * jini core services provides a safety net.  If you are using
     * multicast then you can always add another instance.
     */

    static private lbs = "localhost";
    static private txs = "localhost";
    static private mds = "localhost";
    // client servers (containers for executing distributed tasks).
    static private cs = "localhost";
    // jini server(s).
    static private jini1 = "localhost";
//     static private jini2 = "localhost";
//     static private jini = new String[]{jini1,jini2};
    // 2 class servers
//     static private cls1 = "192.168.6.21";
//     static private cls2 = "192.168.6.23"; // doubled up with zoo2
//     static private cls = new String[]{cls1,cls2};
    // 3 zookeeper machines (one instance per).
    static private zoo1 = "localhost";
//     static private zoo2 = "192.168.6.23";
//     static private zoo3 = "192.168.6.24";
    static private zoo = new String[] {zoo1};
    // data service machines (could be more than one instance per).
    static private ds1 = "localhost";
//     static private ds2 = "192.168.6.26";
//     static private ds3 = "192.168.6.27";
//     static private ds4 = "192.168.6.28";
//     static private ds5 = "192.168.6.29";
//     static private ds6 = "192.168.6.30";
//     static private ds7 = "192.168.6.31";
//     static private ds8 = "192.168.6.32";
//     static private ds9 = "192.168.6.33";
//     static private ds10 = "192.168.6.34";
//     static private ds11 = "192.168.6.35";
    //static private ds = new String[]{ds1,ds2,ds3,ds4,ds5,ds6,ds7,ds8,ds9,ds10,ds11};
    static private ds = new String[]{ds1};
    static private dataServiceCount = 2;
	static private maxDataServicePerHost = 2;
    static private clientServiceCount = 1;
	static private maxClientServicePerHost = 2;

    /**
     * A String[] whose values are the group(s) to be used for discovery
     * (no default). Note that multicast discovery is always used if
     * LookupDiscovery.ALL_GROUPS (a <code>null</code>) is specified.
     */

    // one federation, multicast discovery.
    static private groups = LookupDiscovery.ALL_GROUPS;

    // multiple federations, MUST use unicast discovery.
    //groups = new String[]{bigdata.fedname};

    /**
     * One or more unicast URIs of the form <code>jini://host/</code>
     * or <code>jini://host:port/</code> (no default).
     *
     * This MAY be an empty array if you want to use multicast
     * discovery <strong>and</strong> you have specified the groups as
     * LookupDiscovery.ALL_GROUPS (a <code>null</code>).
     */
    static private locators = new LookupLocator[] {

	// runs jini on the localhost using unicast locators.
	//new LookupLocator("jini://localhost/")
	
	// runs jini on two hosts using unicast locators.
	//new LookupLocator("jini://"+jini1),
	//new LookupLocator("jini://"+jini2),

    };

    /**
     * The policy file that will be used to start services.
     *
     * Note: The default policy is completely open.
     */
    private static policy = "policy.all";
    
    /**
     * Where jini is installed.
     */
    private static JINI_HOME = new File("C:\\Program Files\\jini2_1");

}

/*
 * Service configuration defaults.  These can also be specified on a
 * per service-type basis.  When the property is an array type, the
 * value here is concatenated with the optional array value on the per
 * service-type configuration.  Otherwise it is used iff no value is
 * specified for the service-type configuration.
 */
com.bigdata.jini.start.config.ServiceConfiguration {

    /* 
     * Default java command line arguments that will be used for all
     * java-based services
     *
     * Note: [-Dcom.sun.jini.jeri.tcp.useNIO=true] enables NIO in
     * combination with the [exporter] configured below.
     */
    defaultJavaArgs = new String[]{
	"-server",
	"-ea",
	"-Dcom.sun.jini.jeri.tcp.useNIO=true",
	"-Djava.security.policy="+bigdata.policy
	// , "-Dlog4j.debug=true"
    };

    // Optional classpath components.
    //classpath=new String[]{};

    /* A handy idiom that may be used to add things to the classpath.

    classpath = (String[])ServiceConfiguration.concat
	( new String[]{
	    "../bigdata-lubm/bigdata-lubm.jar",
	    "../bigdata-rdf/lib/slf4j-api-1.4.3.jar",
	    "../bigdata-rdf/lib/slf4j-log4j12-1.4.3.jar",
	    "../bigdata-rdf/lib/openrdf-sesame-2.2.4-onejar.jar"
	},
	  ServiceConfiguration.getClassPath()
	  );
*/

    /* Default path for service instances and their persistent
     * data. This may be overriden on a per service-type basis. 
     *
     * Note: For logical services that support failover, the concrete
     * service directory is assigned dynamically when a physical
     * service instance is created.
     */
    serviceDir = bigdata.serviceDir;
 
    /* The bigdata services default logging configuration (a URI!)
     *
     * You need log all the services.  You can use SocketAppender
     * and SimpleSocketServer or struggle with  syslog or write the
     * logs into well known files.  If files, they really need to be on
     * a shared volume so that you can see all of the output.
     *
     * @see http://wiki.apache.org/logging-log4j/syslog
     * @see http://threebit.net/mail-archive/tomcat-users/msg00219.html
     * @see http://www.linuxjournal.com/article/5476
     */
    log4j="file:src/resources/logging/log4j.properties";

    /*
     * Set up some default properties values that will be inherited
     * (copy by value) by all clients and services started using this
     * configuration file.
     */
    properties = new NV[] {

    /* 
     * Each JiniClient (and hence all bigdata services) can run an
     * httpd that will expose performance counters for the service and
     * the host on which it is running.  This property specifies the
     * port for that httpd service.  Valid values are port number,
     * zero (0) for a random open port, MINUS ONE (-1) to disable the
     * httpd service.
     */
    new NV(IBigdataClient.Options.HTTPD_PORT, "-1"),

    /*
     * Option to disable collection of performance counters for the
     * host on which the client or service is running.
     *
     * Note: The load balancer relies on this information!
     */
    new NV(IBigdataClient.Options.COLLECT_PLATFORM_STATISTICS,"false"),

    /* Option to disable collection of performance counters on the
     * queues used internally by the client or service.
     *
     * Note: The load balancer relies on this information!
     */
    new NV(IBigdataClient.Options.COLLECT_QUEUE_STATISTICS,"false"),

    };

}

/**
 * Class server.
 *
 * The class server exposes downloadable code to services and clients.
 * It does this using an httpd service which runs on the specified
 * port.  Clients and servers need to know the URL(s) at which they
 * can connect to the class server in advance, so you have to specify
 * which host(s) will run the class server.
 *
 * That information gets put into the
 */
com.sun.jini.tool.ClassServer {

    classpath = new String[] {

	bigdata.JINI_HOME+File.separator+"lib"+File.separator+"classserver.jar"

    };

    /**
     * Port used for the httpd.
     */
    port = 8082;

}

/**
 * Jini service configuration.
 *
 * WARNING: Starting the jini services browser requires a GUI.  If you
 * are running from a terminal without a GUI then you MUST edit the
 * JINI_HOME/installverify/support/startAll.config file and comment
 * out the "Browser" component.  Unfortunately, you have to install it
 * with a GUI in the first place.
 *
 * Note: You can probably comment out several of the other components
 * as well.  We are using reggie (the service registrar), and the
 * classserver might be a depenency for reggie, but I don't believe
 * that anything else is being used.
 */
jini {

    /**
     * Where jini was installed.
     */
    serviceDir = bigdata.JINI_HOME;

    /**
     * The #of instances to run.
     *
     * Note: A jini service instance may be started on a host if it is
     * declared in [locators].  If locators is empty, then you are
     * using multicast discovery.  In this case an instance may be
     * started on any host, unless [constraints] are imposed.  In any
     * case, no more than [serviceCount] jini services will be started
     * at any given time.  This is checked against the #of discovered
     * instances.
     */
    serviceCount = 1;

}

/**
 * Zookeeper server configuration.
 */
org.apache.zookeeper.server.quorum.QuorumPeerMain {

    /* Directory for zookeeper's persistent state.  The [id] will be
     * appended as another path component automatically to keep
     * instances separate.
     */
    dataDir = new File(bigdata.serviceDir,"zookeeper");

    /* Optional directory for the zookeeper log files.  The [id] will
     * be appended as another path component automatically to keep
     * instances separate.
     * 
     * Note: A dedicated log device is highly recommended!
     */
    //dataLogDir=new File("/var/zookeeper-log");

    // required.
    clientPort=2181;

    tickTime=2000;

    initLimit=5;

    syncLimit=2;

    /* A comma delimited list of the known zookeeper servers together
     * with their assigned "myid": {myid=host:port(:port)}+
     *
     * Note: You SHOULD specify the full list of servers that are
     * available to the federation. An instance of zookeeper will be
     * started automatically on each host running ServicesManager that
     * is present in the [servers] list IF no instance is found
     * running on that host at the specified [clientPort].
     * 
     * Note: zookeeper interprets NO entries as the localhost with
     * default peer and leader ports. This will work as long as the
     * localhost is already running zookeeper.  However, zookeeper
     * WILL NOT automatically start zookeeper if you do not specify
     * the [servers] property.  You can also explicitly specify
     * "localhost" as the hostname, but that only works for a single
     * machine.
     */
    // standalone
    servers="1=localhost:2888:3888";
    // ensemble
    /* @todo use ctor[] for zoo server entry?
    servers =   "1="+bigdata.zoo1+":2888:3888"
            + ",2="+bigdata.zoo2+":2888:3888"
	    + ",3="+bigdata.zoo3+":2888:3888"
	    ;
    */

    /* This is all you need to run zookeeper.
    classpath = new String[] {
    	"bigdata-jini/lib/apache/zookeeper-3.1.0.jar",
		"bigdata/lib/apache/log4j-1.2.15.jar"
    };
    */

    /* Optional command line arguments for the JVM used to execute
     * zookeeper.
     *
     * Note: swapping for zookeeper is especially bad since the
     * operations are serialized, so if anything hits then disk then
     * all operations in the queue will have that latency as well.
     */
    //args=new String[]{"-Xmx2G"};

    // zookeeper server logging configuration (value is a URI!)
    log4j="file:src/resources/logging/log4j.properties";

}

/*
 * Zookeeper client configuration.
 */
org.apache.zookeeper.ZooKeeper {

    /* Root znode for the federation instance. */
    zroot = "/"+bigdata.fedname;

    /* A comma separated list of host:port pairs, where the port is
     * the CLIENT port for the zookeeper server instance.
     */
    servers = "localhost:2181";

    /* Session timeout (optional, but heavy load can cause disconnects with
     * the default timeout).
     */
    sessionTimeout = 20000;

    /* 
     * ACLs for the federation zroot.
     *
     * Note: zookeeper ACLs are not transmitted over secure channels
     * and are placed into plain text Configuration files by the
     * ServicesManagerServer.
     */
    acl = new ACL[] {

	new ACL(ZooDefs.Perms.ALL, new Id("world", "anyone"))

    };

}

/*
 * Jini client configuration
 */
com.bigdata.service.jini.JiniClient {

    /* Default Entry[] for jini services.  Also used by the
     * ServicesManagerService as is.
     *
     * Note: A Name attribute will be added automatically using the
     * service type and the znode of the service instance.  That Name
     * will be canonical.  It is best if additional service names are
     * NOT specified as that might confuse somethings :-)
     *
     * Note: A Hostname attribute will be added dynamically.
     */
    entries = new Entry[] {
	// Purely informative.
	new Comment(bigdata.fedname),
    };

    groups = bigdata.groups;

    locators = bigdata.locators;

    // optional JiniClient properties.
    //properties = new NV[] {};

}

/**
 * Options for the bigdata services manager.
 */
com.bigdata.jini.start.ServicesManagerServer {

    /*
     * This object is used to export the service proxy.  The choice
     * here effects the protocol that will be used for communications
     * between the clients and the service.
     */
    exporter = new BasicJeriExporter(TcpServerEndpoint.getInstance(0),
                                     new BasicILFactory()); 

    /*                                          
     * The data directory and the file on which the serviceID will be
     * written.
     *
     * Note: These properties MUST be specified explicitly for the
     * ServicesManager since it uses this as its Configuration file.
     * For other services, it generates the Configuration file and
     * will generate this property as well.
     */

    serviceDir = new File(bigdata.serviceDir,"ServicesManager");

    serviceIdFile = new File(serviceDir,"service.id");
    
    /* The services that will be started.  For each service, there
     * must be a corresponding component defined within this
     * configuration file.  For each "ManagedServiceConfiguration", an
     * entry will be made in zookeeper and logical and physical
     * service instances will be managed automatically.  For unmanaged
     * services, such as jini and zookeeper itself, instances will be
     * started iff necessary by the services manager when it starts
     * up.
     */
    services = new String[] {
	
      	"jini",
 	"org.apache.zookeeper.server.quorum.QuorumPeerMain",
  	"com.bigdata.service.jini.TransactionServer",
    	"com.bigdata.service.jini.MetadataServer",
    	"com.bigdata.service.jini.DataServer",
    	"com.bigdata.service.jini.LoadBalancerServer",
    	"com.bigdata.service.jini.ClientServer",
	
    };

    /*
     * Additional properties passed through to the JiniClient or the
     * service.
     *
     * Note: I am leaving the OS statistics collection enabled for the
     * services manager and plan to run an instance on each host.
     * That will let me see the load on all hosts where I otherwise
     * would not have reports available for the zookeeper only
     * machines.
     */
    properties = new NV[]{

	/* Don't collect statistics from the OS since there is no load
	 * balancing for the ServicesManager itself.
	 */
	new NV(IBigdataClient.Options.COLLECT_PLATFORM_STATISTICS, "false"),

	// Don't sample the various queues
	// 
	// Note: Disabled for single-machine runs since performance counters.
	// don't matter with only one host.
	// 
	// @TODO Enable this for any multiple machine runs!
	//
	new NV(IBigdataClient.Options.COLLECT_QUEUE_STATISTICS,"false"),

	// Don't run the httpd service (data are still reported to the
	// load balancer).
	new NV(IBigdataClient.Options.HTTPD_PORT, "-1"),

    };

    /* Note: I am deliberately running this on every host to get OS
     * performance counters aggregated within the load balancer, even
     * for hosts that will not run bigdata services, such as the
     * zookeeper machines.
     */
    constraints = new IServiceConstraint[] {

    	//new HostAllowConstraint(sms)

    };

}

/**
 * Initial configuration for new instances of the transaction server.
 */
com.bigdata.service.jini.TransactionServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.txs),
	
    };

	properties = new NV[] {
	
	/* The #of milliseconds that the database will retain history no
	 * longer required to support the earliest active transaction.
	 *
	 * A value of ZERO means that only the last commit point will
	 * be retained.  The larger the value the more history will be
	 * retained.  You can use a really big number if you never want
	 * to release history and you have lots of disk space :-)
	 *
	 * Note: The most recent committed state of the database is
	 * NEVER released.
	 */
	new NV(TransactionServer.Options.MIN_RELEASE_AGE, "0"),

	};

}

com.bigdata.service.jini.MetadataServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),
	new TXRunningConstraint(),

	new HostAllowConstraint(bigdata.mds),

    };

    properties = new NV[]{

        /*
         * The MDS does not support overflow at this time so
         * overflow MUST be disabled for this service.
         */
        new NV(MetadataServer.Options.OVERFLOW_ENABLED,"false")

    };

}

com.bigdata.service.jini.DataServer {

    serviceCount = bigdata.dataServiceCount;

    args = new String[]{
	//"-Xmx4G", // grant lots of memory.
	//"-XX:-UseGCOverheadLimit", // @todo might not be required.
    };

    // restrict where the data services can run.
    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),
	new TXRunningConstraint(),

	new HostAllowConstraint(bigdata.ds),

	new MaxDataServicesPerHostConstraint(bigdata.maxDataServicePerHost),

    };

    /*
     * Note: the [dataDir] will be filled in when a new service
     * instance is created based on the [servicesDir], so don't set it
     * here yourself.
     */
    properties = new NV[]{

	//new NV(DataServer.Options.BUFFER_MODE,com.bigdata.journal.BufferMode.Disk.toString()),

	/* Override the initial and maximum extent so that they are
	 * more suited to large data sets (200M).  Overflow will be
	 * triggered as the size of the journal approaches the maximum
	 * extent.
	 */

	new NV(DataServer.Options.INITIAL_EXTENT, "209715200"),
	new NV(DataServer.Options.MAXIMUM_EXTENT, "209715200"),

	// Enable concurrent overflow task processing.
	new NV(DataServer.Options.OVERFLOW_TASKS_CONCURRENT,"true"),

	// Overflow timeout (this is 5 minutes, which is just for testing).
	new NV(DataServer.Options.OVERFLOW_TIMEOUT,"300000"),

	/* Option may be used to disable index partition moves for
     * testing purposes.
     */
	new NV(DataServer.Options.MAXIMUM_MOVES_PER_TARGET,"3"),

	/* Option effects how much splits are emphasized for a young
	 * scale-out index.  If the index has fewer than this many
	 * partitions, then there will be a linear reduction in the
	 * target index partition size which will increase the likelyhood
	 * of an index split under heavy writes.
	 */
	//new NV(DataServer.Options.ACCELERATE_SPLIT_THRESHOLD,"20"),
	new NV(DataServer.Options.ACCELERATE_SPLIT_THRESHOLD,"0"),

	/* Options accelerates overflow for data services have fewer than
	 * the threshold #of bytes under management.  Acceleration is
	 * accomplished by reducing the maximum extent of the live journal
	 * linearly, but with a minimum of a 10M maximum extent.  When the
	 * maximum extent is reduced by this option, the initial and the
	 * maximum extent will always be set to the same value for that
	 * journal.
	 */
	//new NV(DataServer.Options.ACCELERATE_OVERFLOW_THRESHOLD,""+com.bigdata.rawstore.Bytes.gigabyte),
	new NV(DataServer.Options.ACCELERATE_OVERFLOW_THRESHOLD,"0"),

	/* Configuration for the write service (unisolated write operations).
	 *
	 * Note: the corePoolSize will never increase for an unbounded queue
	 * so the value specified for maximumPoolSize will essentially be
	 * ignored in this case.
	 *
	 * Note: When the writeServiceQueueCapacity is ZERO (0), a SynchronousQueue
	 * is used and the maximumPoolSize is ignored.  This allow the #of threads
	 * to change in response to demand while ensuring that tasks are never
	 * rejected.
	 */
	new NV(DataServer.Options.WRITE_SERVICE_QUEUE_CAPACITY,"0"),
	new NV(DataServer.Options.WRITE_SERVICE_CORE_POOL_SIZE,"5"),
	//new NV(DataServer.Options.WRITE_SERVICE_MAXIMUM_POOL_SIZE,"5"),
	new NV(DataServer.Options.WRITE_SERVICE_PRESTART_ALL_CORE_THREADS,"true"),

    };

}

/**
 * Configuration options for the containers used to distribute application tasks
 * across a federation.
 *
 * @todo There should be a means to tag certain client servers for one purpose
 * or another.  This could be handled by subclassing, but it really should be
 * declarative.
 */
com.bigdata.service.jini.ClientServer {

    serviceCount = bigdata.clientServiceCount;

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.cs),

	new MaxClientServicesPerHostConstraint(bigdata.maxClientServicePerHost),
	
    };

	properties = new NV[] {
	
	};

}

com.bigdata.service.jini.LoadBalancerServer {

    constraints = new IServiceConstraint[] {

	new JiniRunningConstraint(),
	new ZookeeperRunningConstraint(),

	new HostAllowConstraint(bigdata.lbs)

    };

    /*
     * Override some properties.
     */
    properties = new NV[] {

    /* 
     * Each JiniClient (and hence all bigdata services) can run an
     * httpd that will expose performance counters for the service and
     * the host on which it is running.  This property specifies the
     * port for that httpd service.  Valid values are port number,
     * zero (0) for a random open port, MINUS ONE (-1) to disable the
     * httpd service.
     *
     * Note: The load balancer httpd normally uses a known port so
     * that it is easy to find.  This is where you will find all of
     * the performance counters aggregated for the entire federation,
     * including their history.
     */
    new NV(IBigdataClient.Options.HTTPD_PORT, "8081"), // 8080

    /*
     * Note: The load balancer SHOULD NOT collect platform statistics
     * itself since that interfers with its ability to aggregate
     * statistics about the host on which it is running.  Instead it
     * should rely on the presence of at least one other service
     * running on the same host to report those statistics to the load
     * balancer.
     */
    new NV(IBigdataClient.Options.COLLECT_PLATFORM_STATISTICS,"false"),
		
    /* 
     * The directory where the aggregated statistics will be logged.
     *
     * You only need to specify this if you want to put the files into
     * a well known location.
     *
     * @todo verify that an override here works.
     */
    //new NV(LoadBalancerServer.Options.LOG_DIR,"/opt2/var/log/bigdata")

    };

}

/**
 * Configuration options for the distributed LUBM data generator which
 * have been broken out for easier editing.
 *
 * @todo For query, we can run the LUBM test harness.  However, it
 * should take the kb namespace from the label in the file.
 */
lubm {

    // #of clients to run.
    static private nclients = 1; // was 2 @todo bigdata.clientServiceCount

    // The #of universities to generate. 
    static private univNum = 1; // was 10

    // the job name (based on the #of universities by default).
    static private jobName = "U"+univNum+"b1";

    // the kb namespace (same as job by default).
    static private namespace = jobName;

    // data directory.  may be NAS or local file system.
    static private dataDir = "E:\\"+jobName;

    // parent for log files for runs.
    static private logDir = "E:\\lubmLog";

    // the ontology to be loaded.
    static private ontology = new File("src/java/edu/lehigh/swat/bench/univ-bench.owl");

    // minimum #of data services to run.
    static private minDataServices = bigdata.dataServiceCount;

    // How long the master will wait to discover the minimum #of data
    // services that you specified (ms).
    static private awaitDataServicesTimeout = 4000;

    // Multiplier for the scatter effect.
    static private scatterFactor = 2;

    /* The #of index partitions to allocate on a scatter split.  ZERO
     * (0) means that 2 index partitions will be allocated per
     * data service which partiticpates in the scatter split.
     * Non-zero values directly give the #of index partitions to
     * create.
     */
    static private scatterSplitIndexPartitionCount = ConfigMath.multiply
        ( scatterFactor,
          bigdata.dataServiceCount
          );

    // Use all discovered data services when scattering an index.
    static private scatterSplitDataServiceCount = 0;

    /* Scatter split trigger point.  The scatter split will not be
     * triggered until the initial index partition has reached
     * this percentage of a nominal index partition in size.
     */
    static private scatterSplitPercentOfSplitThreshold = 0.5;

    /*
     * Specify / override some triple store properties.
     *
     * Note: You must reference this object in the section for the
     * component which will actually create the KB instance, e.g.,
     * either the SplitFinder or the LubmGeneratorMaster.
     */
    static private properties = new NV[] {
	
        /*
         * When "true", the store will perform incremental closure as
         * the data are loaded. When "false", the closure will be
         * computed after all data are loaded. (Actually, since we are
         * not loading through the SAIL making this true does not
         * cause incremental TM but it does disable closure, so
         * "false" is what you need here).
         */
        new NV(BigdataSail.Options.TRUTH_MAINTENANCE, "false" ),

        /*
         * Enable rewrites of high-level queries into native rules (native JOIN
         * execution). (Can be changed without re-loading the data to compare
         * the performance of the Sesame query evaluation against using the
         * native rules to perform query evaluation.)
         */
        new NV(BigdataSail.Options.NATIVE_JOINS, "true"),

        /*
         * May be used to turn off inference during query, but will
	 * cause ALL inferences to be filtered out when reading on the
	 * database.
         */
        // new NV(BigdataSail.Options.INCLUDE_INFERRED, "false"),

        /*
         * May be used to turn off query-time expansion of entailments such as
         * (x rdf:type rdfs:Resource) and owl:sameAs even through those
         * entailments were not materialized during forward closure (this
	 * disables the backchainer!)
         */
        new NV(BigdataSail.Options.QUERY_TIME_EXPANDER, "false"),

        /*
         * Option to restrict ourselves to RDFS only inference. This
         * condition may be compared readily to many other stores.
         * 
         * Note: While we can turn on some kinds of owl processing
         * (e.g., TransitiveProperty, see below), we can not compute
         * all the necessary entailments (only queries 11 and 13
         * benefit).
         * 
         * Note: There are no owl:sameAs assertions in LUBM.
         * 
         * Note: lubm query does not benefit from owl:inverseOf.
         * 
         * Note: lubm query does benefit from owl:TransitiveProperty
         * (queries 11 and 13).
         * 
         * Note: owl:Restriction (which we can not compute) plus
         * owl:TransitiveProperty is required to get all the answers
         * for LUBM.
         */
        new NV(BigdataSail.Options.AXIOMS_CLASS, "com.bigdata.rdf.axioms.RdfsAxioms"),
        // new NV(BigdataSail.Options.AXIOMS_CLASS,"com.bigdata.rdf.axioms.NoAxioms"),

        /*
         * Produce a full closure (all entailments) so that the
         * backward chainer is always a NOP. Note that the
         * configuration properties are stored in the database (in the
         * global row store) so you always get exactly the same
         * configuration that you created when reopening a triple
         * store.
         */
        // new NV(BigdataSail.Options.FORWARD_CHAIN_RDF_TYPE_RDFS_RESOURCE, "true"),
        // new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_SAMEAS_PROPERTIES, "true"),

        /*
         * Additional owl inferences. LUBM only both inverseOf and
         * TransitiveProperty of those that we support (owl:sameAs,
         * owl:inverseOf, owl:TransitiveProperty), but not owl:sameAs.
         */
        new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_INVERSE_OF, "true"),
        new NV(BigdataSail.Options.FORWARD_CHAIN_OWL_TRANSITIVE_PROPERY, "true"),

        // Note: FastClosure is the default.
        // new NV(BigdataSail.Options.CLOSURE_CLASS, "com.bigdata.rdf.rules.FullClosure"),

        /*
         * Various things that effect native rule execution.
         */
        // new NV(BigdataSail.Options.FORCE_SERIAL_EXECUTION, "false"),
        // new NV(BigdataSail.Options.MUTATION_BUFFER_CAPACITY, "20000"),
        new NV(BigdataSail.Options.CHUNK_CAPACITY, "100"),
        // new NV(BigdataSail.QUERY_BUFFER_CAPACITY, "10000"),
        // new NV(BigdataSail.FULLY_BUFFERED_READ_THRESHOLD, "10000"),

        /*
         * Turn off incremental closure in the DataLoader object.
         */
        new NV(com.bigdata.rdf.store.DataLoader.Options.CLOSURE, "None"),
	//com.bigdata.rdf.store.DataLoader.ClosureEnum.None.toString()),

        /*
         * Turn off commit in the DataLoader object. We do not need to commit
         * anything until we have loaded all the data and computed the closure
         * over the database.
         */
        new NV(com.bigdata.rdf.store.DataLoader.Options.COMMIT,"None"),
	//com.bigdata.rdf.store.DataLoader.CommitEnum.None.toString()),

        /*
         * Turn off Unicode support for index keys. This is a big win
         * for load rates if the data set does not use Unicode data,
         * but it has very little effect on query rates since the only
         * time we generate Unicode sort keys is when resolving the
         * Values in the queries to term identifiers in the database.
	 *
	 * @see com.bigdata.btree.keys.CollatorEnum
         */
        new NV(BigdataSail.Options.COLLATOR,"ASCII"),

        /*
         * Turn off the full text index (search for literals by keyword).
         */
	new NV(BigdataSail.Options.TEXT_INDEX, "false"),

        /*
         * Turn on bloom filter for the SPO index (good up to ~2M
         * index entries for scale-up -or- for any size index for
         * scale-out).  This is a big win for some queries on
         * scale-out indices since we can avoid touching the disk if
         * the bloom filter reports "false" for a key.
         */
        new NV(BigdataSail.Options.BLOOM_FILTER, "true"),

	/* The #of low order bits from the TERM2ID index partition
	 * local counter that will be reversed and written into the
	 * high-order bits of the term identifier.  This has a strong
	 * effect on the distribution of bulk index read/write
	 * operations for the triple store.
	 *
	 * Note: The SplitFinder may only be used when this value is
	 * ZERO (0) due to how it figures out the split points.
	 */
        new NV(BigdataSail.Options.TERMID_BITS_TO_REVERSE,"6"),

        /*
         * Turn off statement identifiers (support for statements
         * about statements).
         */
        new NV(BigdataSail.Options.STATEMENT_IDENTIFIERS, "false"),

        /*
	 * Option may be enabled to store blank nodes such that they
	 * are stable (they are not stored by default).
	 *
         * @todo LUBM uses blank nodes. Therefore re-loading LUBM will
         * always cause new statements to be asserted and result in
         * the closure being updated if it is recomputed. Presumably
         * you can tell bigdata to store the blank nodes and RIO to
         * preserve them, but it does not seem to work (RIO creates
         * new blank nodes on reparse). Maybe this is a RIO bug?
         */
        // new NV(BigdataSail.Options.STORE_BLANK_NODES,"true");

        /*
         * Turn off justification chains.  This impacts only the load
         * performance, but it is a big impact and only required if
         * you will be doing truth maintenance (TM). Also, truth
         * maintenance based on the justification chains does not
         * scale-out.  (We are planning a magic sets integration to
         * take care of that).
         */
        new NV(BigdataSail.Options.JUSTIFY, "false"),

        /*
         * Choice of the join algorithm.
         * 
         * false is pipeline, which scales-out.
         * 
         * true is nested, which is also the default right now but
         * does not scale-out.
         */
        new NV(BigdataSail.Options.NESTED_SUBQUERY, "false"),

        /*
         * Maximum #of subqueries to evaluate concurrently for the 1st
         * join dimension for native rules. Zero disables the use of
         * an executor service. One forces a single thread, but runs
         * the subquery on the executor service. N>1 is concurrent
         * subquery evaluation.
	 *
	 * Note: parallel subquery does not work for pipeline joins at
	 * this time so this option is only safe for nested subquery
	 * join, and nested subquery joins do not scale-out.
         */
        // new NV(BigdataSail.Options.MAX_PARALLEL_SUBQUERIES, "5"),
        new NV(BigdataSail.Options.MAX_PARALLEL_SUBQUERIES, "0"),

	/*
         * This controls how much data the mutable BTree on the live
         * journal and the read-historical BTrees will buffer.  The
         * default should work well unless you are resource
         * constrained. There is no point making this value too large
         * since periodic overflow keeps down the #of tuples in these
         * BTree objects.  There are separate parameters which control
         * the buffering for the IndexSegments.
         */
	//new NV(IndexMetadata.Options.BTREE_READ_RETENTION_QUEUE_CAPACITY, "10000"),

	/*
	 * Turn on direct buffering for index segment nodes.
	 */
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.INDEX_SEGMENT_BUFFER_NODES
		  ), "true"),

	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.INDEX_SEGMENT_BRANCHING_FACTOR
		 ), "512"),// default 512

	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.INDEX_SEGMENT_LEAF_CACHE_CAPACITY
		 ), "20"),// default 100

	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.INDEX_SEGMENT_LEAF_CACHE_TIMEOUT
		 ), ""+ConfigMath.s2ns(10)),// default 30s

	// Disable per-child locking.
	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace,
		 IndexMetadata.Options.CHILD_LOCKS
		 ), "false"), 

	/*
	 * Tweak the asynchronous write API parameters.
	 */

	// dial down the master/sink queue capacities to rein in RAM.
	// default is 5k.  The sink is the big RAM consumer since there
	// is one since per index partition (and one thread per sink).
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.MASTER_QUEUE_CAPACITY
		  ), "2000"), // was 5000
	// default is 5k
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SINK_QUEUE_CAPACITY
		  ), "500"), // was 5000

	// Override master/sink queue capacity for TERM2ID since uses KVOLatch
	// and requires all results to be in before writes on the other indices
	// may proceed.
	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace + "."
				    + LexiconRelation.NAME_LEXICON_RELATION + "."
				    + LexiconKeyOrder.TERM2ID,
		 IndexMetadata.Options.MASTER_QUEUE_CAPACITY
		 ), "5000"),// default 5000
	new NV(com.bigdata.config.Configuration.getOverrideProperty
	       ( namespace + "."
				    + LexiconRelation.NAME_LEXICON_RELATION + "."
				    + LexiconKeyOrder.TERM2ID,
		 IndexMetadata.Options.SINK_QUEUE_CAPACITY
		 ), "5000"),// default 5000

	// default is 10k
	new NV( com.bigdata.config.Configuration.getOverrideProperty
		( namespace,
		  IndexMetadata.Options.SINK_CHUNK_SIZE
		  ), "10000"), // was 20000

// 	// default is infinite
// 	new NV( com.bigdata.config.Configuration.getOverrideProperty
// 		( namespace,
// 		  IndexMetadata.Options.SINK_CHUNK_TIMEOUT_NANOS
// 		  ), ""+Long.MAX_VALUE),//ConfigMath.s2ns(120)),

// 	// default is infinite.
// 	new NV( com.bigdata.config.Configuration.getOverrideProperty
// 		( namespace,
// 		  IndexMetadata.Options.SINK_IDLE_TIMEOUT_NANOS
//		  ), ""+Long.MAX_VALUE),//ConfigMath.s2ns(320)),

	/* The TERM2ID index uses idle timeout to preserve liveness for async writes.
	 * This is required in order for the async loader to not wait forever for the
	 * last full chunk when bulk loading with async writes on the TERM2ID index.
	 * The value of this timeout effects both how large the chunks will be and
	 * how long after the last document is parsed it will be until the TERM2ID
	 * async write buffers decide that they are idle and evict the last of their
	 * chunks.  This also effects the throughput since the larger chunk sizes
	 * correlates very well with higher throughput.
	 */
         new NV(com.bigdata.config.Configuration.getOverrideProperty
 	       ( namespace + "." + LexiconRelation.NAME_LEXICON_RELATION + "." + LexiconKeyOrder.TERM2ID,
 	         IndexMetadata.Options.SINK_IDLE_TIMEOUT_NANOS
 	         ), ""+ConfigMath.s2ns(20)),

        /*
         * Tweak parameters designed to result in good split points
         * for the various indices to achieve 100-200M segments (based
         * on U50 data set).
         * 
         * @todo These changes should be applied to the triple store,
         * but in a manner that permits explicit override. They result
         * in good sizes for the index partitions, at least for the
         * LUBM data. [Actually, I may rework the split logic to be
         * based purely on the size of the index partition after a
         * compacting merge in which case I won't need to deal with
         * tweaking these parameters any more.]
         */

        //
        // scatter split overrides
        //

        new NV( com.bigdata.config.Configuration.getOverrideProperty
                ( namespace,
                  IndexMetadata.Options.SCATTER_SPLIT_PERCENT_OF_SPLIT_THRESHOLD
                  ), ""+scatterSplitPercentOfSplitThreshold),

        new NV( com.bigdata.config.Configuration.getOverrideProperty
                ( namespace,
                  IndexMetadata.Options.SCATTER_SPLIT_DATA_SERVICE_COUNT
                  ), ""+scatterSplitDataServiceCount),

        new NV( com.bigdata.config.Configuration.getOverrideProperty
                ( namespace,
                  IndexMetadata.Options.SCATTER_SPLIT_INDEX_PARTITION_COUNT
                  ), ""+scatterSplitIndexPartitionCount),

        // The TERM2ID index is different since it uses Sync RPC and
        // therefore should use either NO scatter splits or only
        // scatter the index onto a few data services.
        new NV(com.bigdata.config.Configuration
               .getOverrideProperty(namespace + "."
                                    + LexiconRelation.NAME_LEXICON_RELATION + "."
                                    + LexiconKeyOrder.TERM2ID,
                                    IndexMetadata.Options.SCATTER_SPLIT_ENABLED),
               "false"),

        //
        // normal split overrides
        //

        // statement indices (10x the default)
        new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + SPORelation.NAME_SPO_RELATION,
				    IndexMetadata.Options.SPLIT_HANDLER_MIN_ENTRY_COUNT),
	       "" + ConfigMath.multiply( 5000, Bytes.kilobyte32)),
	new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + SPORelation.NAME_SPO_RELATION,
				    IndexMetadata.Options.SPLIT_HANDLER_ENTRY_COUNT_PER_SPLIT),
	       "" + ConfigMath.multiply(10, Bytes.megabyte32)),

        // term2id index (5x the default).
        new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace + "."
				    + LexiconRelation.NAME_LEXICON_RELATION + "."
				    + LexiconKeyOrder.TERM2ID,
				    IndexMetadata.Options.SPLIT_HANDLER_MIN_ENTRY_COUNT),
	       "" + ConfigMath.multiply(2500, Bytes.kilobyte32)),
        new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace
				    + "."
				    + LexiconRelation.NAME_LEXICON_RELATION
				    + "." + LexiconKeyOrder.TERM2ID,
				    IndexMetadata.Options.SPLIT_HANDLER_ENTRY_COUNT_PER_SPLIT),
	       "" + ConfigMath.multiply(5, Bytes.megabyte32)),

        // id2term index (2x the default)
        new NV(com.bigdata.config.Configuration
                .getOverrideProperty(namespace + "."
                        + LexiconRelation.NAME_LEXICON_RELATION + "."
                        + LexiconKeyOrder.ID2TERM,
                        IndexMetadata.Options.SPLIT_HANDLER_MIN_ENTRY_COUNT),
	       "" + ConfigMath.multiply(1000, Bytes.kilobyte32)),
        new NV(com.bigdata.config.Configuration
	       .getOverrideProperty(namespace
				    + "."
				    + LexiconRelation.NAME_LEXICON_RELATION
				    + "." + LexiconKeyOrder.ID2TERM,
				    IndexMetadata.Options.SPLIT_HANDLER_ENTRY_COUNT_PER_SPLIT),
	       "" + ConfigMath.multiply(2, Bytes.megabyte32)),

	/* Disable reporting to the LBS for the SplitFinder so that it
	 * can run standalone without issuing warnings about not
	 * finding the LBS.
	 */
	new NV(IBigdataClient.Options.REPORT_DELAY,""+Long.MAX_VALUE),

    };

}

/**
 * Distributed bulk loader configuration.
 *
 * Note: This may be enabled by changing the name of the component
 * which is being configured here.  When enabled, it will load files
 * written by a previous run of the LUBM generator. Or you can change
 * the dataDir to load files from some other location.
 */
//com.bigdata.rdf.load.RDFDataLoadMaster {

/**
 * LUBM distributed bulk loader configuration.  This is designed to
 * dynamically generate and load the files so you can load very large
 * data sets without having to pre-generate and store the data on a
 * shared file system.
 */
edu.lehigh.swat.bench.ubt.bigdata.LubmGeneratorMaster {

	/*
	 * General options for a master executing distributed tasks.
	 */

    // The job name.
    jobName = lubm.jobName;

	// The #of client tasks to execute.
    nclients = lubm.nclients;

	// Properties to be made visible to JiniFederation#getProperties()
	properties = lubm.properties;

	// where to write scheduled dumps of the index partition metadata.
	indexDumpDir = new File(lubm.logDir,jobName);

	// must be specified for indexDumpDir to work.
	indexDumpNamespace = lubm.namespace;
	
	// force overflow of all data services after the job ends.
	forceOverflow = true;

    /* How long the master will wait in milliseconds to discover the services
     * that you specify for [servicesTemplates] and [clientsTemplate].
     */
    awaitServicesTimeout = 5000;
    
	/* The minimum set of services which must be discovered before the master
	 * can start.
	 */
	servicesTemplates = new ServicesTemplate[] {

		// the metadata service
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IMetadataService.class
				},
				null/*attributes*/),
			null/*filter*/
			),

		// the data services (filter is required to exclude metadata services)
		new ServicesTemplate(
			bigdata.dataServiceCount/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IDataService.class
				},
				null/*attributes*/),
			DataServiceFilter.INSTANCE/*filter*/
			),

		// the load balancer
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[] {
					com.bigdata.service.ILoadBalancerService.class
				},
				null/*attributes*/),
			null/*filter*/
			)
		
	};

    /* Template for matching the services to which the clients will be
     * distributed for execution.  Normally you will specify
     * IClientService as the interface to be discovered.  While it is
     * possible to run tasks on an IDataService or even an
     * IMetadataService since they both implement IRemoteExecutor, it
     * is generally discouraged unless the tasks require explicit
     * access to the local index partitions for their execution.
     */
	clientsTemplate = new ServicesTemplate(
			bigdata.clientServiceCount, // minMatches
			new ServiceTemplate(
				null, //serviceID
				new Class[]{
					com.bigdata.service.IClientService.class
				},
				null // attributes
			    ),
			null // filter
			);

	/*
	 * RDF distributed data loader options.
	 */

    // The KB name.
    namespace = lubm.namespace;
    
    /* @todo This is unused since we are reading file names from a queue and
       the files are written into the outDir specified for the LUBM generator.
       The option will be migrated into a class which is specific to a
       distributed-file loader.
    */
    dataDir = new File(lubm.dataDir);

    // The ontology to load (file or directory)
    ontology = lubm.ontology;

    // #of concurrent data loader threads per client.
    nthreads = 100; // (was 30) Fixed size thread pool.
    //nthreads = Integer.MAX_VALUE; // Unbounded thread pool with synchronous queue.

	/* The capacity of the task queue feeding those clients (ignored when using
	 * an unbounded thread pool).
	 */
	//queueCapacity = 0; // SynchronousQueue 
	queueCapacity = 10; // Bounded queue capacity.
	//queueCapacity = Integer.MAX_VALUE; // Unbounded queue capacity.

    // The StatementBuffer capacity for sync writes (unused for async writes).
    bufferCapacity = 100000;

	// chunk size for async writes (2x perf). when zero, using sync RPC instead.
	asynchronousWrites = true;
	syncRPCForTERM2ID = false; // enable async writes on TERM2ID also.
	asynchronousWriteProducerChunkSize = 10000;
	valuesInitialCapacity = 20000;
	bnodesInitialCapacity = 16;

    // create the KB if not found.
    create = true; // Note: specify [false] here if you are using SplitFinder.

    // when true, deletes each source file once loaded successfully.
    deleteAfter = true;

    // when true, loads data.
    loadData = true;

    // when true, computes closure.
    computeClosure = true;

	/*
	 * When true, requests a compacting merge of the data services in
	 * the federation before computing the closure.
	 */
	forceOverflowBeforeClosure = true;

	/*
	 * LUBM Generator integration options.
	 */

    /* You have a choice of generate only or generate and load using
     * queues to couple the generator to the data loader.
     *
     * Note: If you just generate the data and write it into local
     * files on each host, then you must place a data loader task on
     * each host in order to have the full data set loaded.
     */
    generateOnly = false;

    // The #of universities to generate. 
    univNum = lubm.univNum;
    
    /* Specify where to put the generated data files and the generated
       log files.  The big choice is whether to put the data onto a
       shared volume or to write the data onto a local volume, in
       which case it must be consumed by the same host which generated
       the data. */
    outDir = new File(lubm.dataDir);

    // log directory is always NAS so we can see what is happening.
    logDir = new File(lubm.logDir,jobName);

	/* The #of files that can be queued up by the generator before it
	   will block. */
//	generatorQueueCapacity = 0; // SynchronousQueue
     generatorQueueCapacity = 100; // blocking
	// generatorQueueCapacity = Integer.MAX_VALUE; // unbounded 

    /* More lubm options (not very likely to change).
     */
    
    startIndex = 0;
    
    ontologyURL = "http://www.lehigh.edu/~zhp2/2004/0401/univ-bench.owl";

}

com.bigdata.service.jini.DumpFederation {

	namespace = lubm.namespace;

}

com.bigdata.service.jini.BroadcastSighup {

	pushConfig = false;
	
	restartServices = true;

}

/**
 * Configuration for a component that pre-parses a specified data set
 * in order to find good split points and the allocates a scale-out
 * triple store on the federation using those split points.
 */
com.bigdata.rdf.load.SplitFinder {

    // create iff true, otherwise just report what the splits should be.
    create = true;
    
    // the namespace of the triple store to be created.
    namespace = lubm.namespace;
    
    // #of splits per scale-out index.
    //nsplits = lubm.minDataServices;
    nsplits = 20;
    
    // #of services on which the index partitions will be registered.
    nservices = lubm.minDataServices;

	// data used to generate the split points.
    dataDir = new File("E:/lehigh benchmark/U1");

	// ontology used to generate the split points.
    ontology = new File("E:/lehigh benchmark/univ-bench.owl");

	// ontology to be loaded into the created triple store.    
	postCreateOntology = new File("E:/lehigh benchmark/univ-bench.owl");
	
    /*
     * Override some triple store properties.
     */
    properties = lubm.properties;

}

/**
 * Configuration for a throughput testing utility.
 */
com.bigdata.service.jini.benchmark.ThroughputMaster {

	//forceOverflow=true;

	nclients = 2; // was 50
	jobName = "test_12";
	namespace = "test.throughputTest."+jobName;
	asynchronous = true; //20s vs 120 (5x) FOR 2 clients, 1M ops.
	npartitions = 10; // was 10
	startKeyPartitions = true;
	operationCount = 1000000;
	maxKeysPerOp = 1000;
	incRange = 100;
	zookeeperUpdateInterval = 0; // zero disables.

    /* How long the master will wait in milliseconds to discover the services
     * that you specify
     */
    awaitDataServicesTimeout = 5000;
    
	/* The minimum set of services which must be discovered before the master
	 * can start.
	 */
	serviceTemplates = new ServicesTemplates[] {

		// the metadata service
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IMetadataService
				},
				null/*attributes*/,
				null/*filter*/)
				),

		// the data services (filter is required to exclude metadata services)
		new ServicesTemplate(
			bigdata.dataServiceCount/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[]{
					com.bigdata.service.IDataService},
				null/*attributes*/,
				DataServiceFilter.INSTANCE/*filter*/)
				),

		// the load balancer
		new ServicesTemplate(
			1/*minMatches*/,
			new ServiceTemplate(null/*serviceID*/,
				new Class[] {
					com.bigdata.service.ILoadBalancerService
				},
				null/*attributes*/,
				null/*filter*/)
				)
		
	};

    /* Template for matching the services to which the clients will be
     * distributed for execution.  Normally you will specify
     * IClientService as the interface to be discovered.  While it is
     * possible to run tasks on an IDataService or even an
     * IMetadataService since they both implement IRemoteExecutor, it
     * is generally discouraged unless the tasks require explicit
     * access to the local index partitions for their execution.
     */
	clientServices = new ServicesTemplate(
			bigdata.clientServiceCount, // minMatches
			new ServiceTemplate(
				null, //serviceID
				new Class[]{
					com.bigdata.service.IClientService
				},
				null, // attributes
				null // filter
				));

}
